{
  "db": [
    {
      "meta": {
        "exported_on": 1760907490696,
        "version": "5.130.5"
      },
      "data": {
        "posts": [
          {
            "id": 1,
            "title": "Exposing metrics to Prometheus with Service Monitors",
            "slug": "exposing-metrics-to-prometheus",
            "markdown": "You‚Äôve done the hard part and added instrumentation to your application to gather metrics, now you just need to expose those metrics to Prometheus so you can alert on them and monitor them, easy¬†right?\n\n\n![](/content/images/1et51tvPy1a2psLUB5HR4ow.png)\n*Fry‚Äôs not sure that exposing metrics to Prometheus is¬†easy*\n\n#### Why use a Service¬†Monitor?\n\n\nIf you don‚Äôt have to access to your Prometheus configuration file (if for example you don‚Äôt manage your Prometheus deployment) you can‚Äôt simply add metric endpoints to Prometheus, you will need to create a target that the Prometheus operator can discover and add to the Prometheus configuration: one way to achieve this is via configuring a service monitor to expose your metric endpoints.\n\n\n#### Service Discovery\n\n\n![](/content/images/1KgS_SjbowMLLyo9wlsW7pQ.jpeg)\n*Service monitor metric endpoints service discovery*\n\n**1** Metrics endpoint exposed by a running application in a pod is mapped to a¬†service.\n\n\n**2** Metrics endpoint mapped to a service is exposed by the service to a service¬†monitor.\n\n\n**3** The Prometheus operator discovers the endpoints exposed by the service¬†monitor.\n\n\n**4** The Prometheus operator updates the Prometheus configuration to include the endpoints exposed by the service monitor, Prometheus will now scrape metrics from these endpoints\n\n\n#### How to expose metrics with a Service¬†Monitor\n\n\nCreating a service monitor is as simple as adding a configuration file to the *templates* directory of your projects Helm¬†chart.\n\n\nYour service should look something like the example below: you should expose the port your metrics endpoints are mapped to within your pods (in my case 8080) and give your service a label so that your service monitor can find the¬†service.\n\n\nservice.yaml\n\n```\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: service\n  labels:\n    name: service\nspec:\n  ports:\n    - name: https\n      port: 10443\n      protocol: TCP\n      targetPort: 10443\n    - name: metrics\n      port: 8080\n      protocol: TCP\n      targetPort: 8080\n  selector:\n    application: myApplication\n\n```\n\n\nAs in the example below configure your service monitor with the port your service exposes your metrics on (and the path to those metrics on that port which you will have set whilst implementing the instrumentation to gather these metrics) and set the selector to match the service from the previous¬†step.\n\n\nservice-monitor.yaml\n\n```\napiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\nmetadata:\nname: service-monitor\nlabels:\napplication: myApplication\nspec:\nendpoints:\n- interval: 30s\npath: /prometheus\nport: *metrics*\nselector:\nmatchLabels:\nname: *service*\n```\n\nNow deploy your helm chart and you should see your service monitor (and your new service if you have created one) have been deployed.\n\n```\nhughevans@MacBook-Pro ~ % kubectl get services\nNAME      TYPE       CLUSTER-IP       PORT(S)               AGE\nservice   ClusterIP  ***.**.***.***   10443/TCP,8080/TCP    1m\nhughevans@MacBook-Pro ~ % kubectl get servicemonitors\nNAME                                      AGE\nservice-monitor                           1m\n```\n\n\nWait for the Prometheus Operator to discover the new Service Monitor: once it has done this you will see your new service monitor appear as targets in the Prometheus web¬†UI.\n\n\n#### Troubleshooting\n\n\n**Prometheus has discovered the service monitor but not the metrics endpoints associated with¬†it**\n\n\nMost issues with service monitors can be traced to a misconfiguration of selectors in the service monitor config file: try using kubectl to compare the name label in your service and to the selector in your service monitor to make sure they are set correctly.\n\n\n**Endpoints appear as down when viewed from Prometheus web¬†UI**\n\n\nCheck to ensure that your service is exposing the correct port: you may have exposed the standard HTTP port instead of the metrics port for example. Prometheus will assert that an endpoint is down if it does not receive a valid Prometheus response from it when it attempts to scrape¬†metrics.\n\n\nYou may also see this issue if your metrics port is using HTTPS or requires authenticated requests.\n\n\n#### Further reading\n\n\n[Using service monitors](https://observability.thomasriley.co.uk/prometheus/configuring-prometheus/using-service-monitors/)\n\n[Troubleshooting service monitors](https://stackoverflow.com/questions/52991038/how-to-create-a-servicemonitor-for-prometheus-operator)\n\n[Service monitor target¬†ports](https://github.com/prometheus-operator/prometheus-operator/issues/2515)\n\n[Exposing metrics to Prometheus with Service Monitors](https://medium.com/daemon-engineering/exposing-metrics-to-prometheus-with-service-monitors-326f38b2daf1) was originally published in [daemon-engineering](https://medium.com/daemon-engineering) on Medium.",
            "status": "published",
            "created_at": 1652310000000,
            "published_at": 1652310000000,
            "updated_at": 1652310000000,
            "author_id": 1,
            "canonical_url": "https://medium.com/daemon-engineering/exposing-metrics-to-prometheus-with-service-monitors-326f38b2daf1"
          },
          {
            "id": 2,
            "title": "DALL¬∑E 2 - what happens when machines make art?",
            "slug": "dall-e-2-what-happens-when-mac",
            "markdown": "![](/content/images/1S0r7QUhKx6R7K9jJxX9RRQ.png)\n*3D render of DALL-E-2 making art in an open office on a red brick background, digital¬†art*\n\n#### What is DALL¬∑E¬†2?\n\n\nDALL¬∑E 2 is an AI created by [OpenAI](https://openai.com/about/) which produces original images in response to a text description. DALL¬∑E 2 can also make edits to existing images (which OpenAI describes as *in-painting*) and create different variations based on an existing¬†image.\n\n\nThroughout this article you will find examples of images generated by DALL¬∑E 2 along with the descriptions used to create¬†them.\n\n\n![](/content/images/16MjxnR8JBAuqoZvZnF1idA.png) \n![](/content/images/1OttgW5ELlucROHJMG0-Qkg.png)\n*comfortable armchair in a library neon, pink, blue, geometric, futuristic, ‚Äò80s*\n\nOpenAI recently added an *out-painting* feature, this allows an existing image to be expanded by adding new content around its¬†edges.\n\n\n#### How does DALL¬∑E 2¬†work?\n\n\nUnder the hood, DALL¬∑E 2 is a series of machine learning models which have been trained to perform the operations required to generate a new image, the first of which is [CLIP](https://openai.com/blog/clip/). Developed by OpenAI and able to encode images and descriptions, CLIP can quantify how well matched text and image encodings are in a process called *contrastive learning*. DALL¬∑E 2 uses another model called a *prior* to select a relevant image encoding for a given text encoding of a description, this image encoding is used to create a gist of the image which will be generated.\n\n\n![](/content/images/1OWoXzQlBD3KyTFiIvPPwLA.png)\n![](/content/images/1fQ5-GvFiUhqTvXrVYKGGjw.png)\n![](/content/images/1z-Vl3-0VJlL6zPvYzd04EQ.png)\n![](/content/images/117o9JugflMmHVjhPKwUPpQ.png)\n*salvador dali waving to the camera photo realistic*\n\nThe gist is then passed to a diffusion model. Diffusion models are trained to remove [noise](https://en.wikipedia.org/wiki/Image_noise) from images, but in this case DALL¬∑E 2 uses diffusion to take the gist image and gradually add more detail to it until it is complete, resulting in the generation of an original image corresponding to the entered description.\n\n\nFor a more in-depth explanation of how DALL¬∑E 2 works, please [see this article](http://adityaramesh.com/posts/dalle2/dalle2.html) by [Aditya Ramesh](https://twitter.com/model_mechanic), the creator of DALL¬∑E and one of the co-creators of DALL¬∑E¬†2.\n\n\n#### What are the potential applications for DALL¬∑E¬†2?\n\n\n[OpenAI permits the use of images generated by DALL¬∑E 2 commercially](https://www.technollama.co.uk/dall%C2%B7e-goes-commercial-but-what-about-copyright), which opens up a world of potential business applications for this technology. It is tempting to imagine that DALL¬∑E 2 can be used anywhere an original image is needed but it has several major limitations (see below) which significantly narrows the field of potential applications. Replacing stock image libraries may be a potential application, especially given the lower cost of accessing DALL¬∑E 2 when compared to licensing images and the similarities between querying libraries of stock photos and composing DALL¬∑E 2 descriptions. Generating assets for games also seems like a good candidate. Marketplaces like the [Unity asset store](https://assetstore.unity.com/) currently offer a variety of assets for use in game development and they can range in price dramatically, DALL¬∑E 2 can be used to quickly and cheaply create some types of assets without requiring the application of artistic¬†skill.\n\n\n#### What are the limitations of DALL¬∑E¬†2?\n\n\nDALL¬∑E 2 does not have a good understanding of composition. Consider the following example: when asked to create an image containing a series of different coloured shapes, DALL¬∑E 2 succeeds in creating an image with the correct shapes and colours, however; it fails to assign the correct colours to each shape consistently.\n\n\nThis is as a result of how contrastive learning works: CLIP learns which features are sufficient for matching an image with the correct description from the training data it is provided, unless that data includes counter examples (in this case images with captions describing their shape, colour, and position in a variety of combinations), CLIP disregards information about composition.\n\n\n![](/content/images/1Lc3poRm17aXz1xjmviLkqg.png)\n![](/content/images/1NOm5Orif-bisQ5PbWbel6A.png)\n![](/content/images/1KH31qEckCvu2t4XIcWmKIA.png)\n![](/content/images/1lYUP6Y7-GhYKC8okGhBv0w.png)\n*oil pastel sketch of a red triangle, a blue square, and a yellow circle on¬†canvas*\n\nStruggles with composition also prevent DALL¬∑E 2 from being able to recreate coherent text. Given that DALL¬∑E 2 is an image generation AI, and not a specialised text generation AI such as [GPT-3](https://openai.com/api/) (also created by OpenAI), it is unsurprising that it struggles with text generation. It is clear that DALL¬∑E 2 understands the concept of text: images containing text often have readable lettering and the text itself often reflects information contained in the description, however; DALL¬∑E 2 struggles with rules on grammar and syntax. In generated images, words will usually be in the wrong order, and sometimes text generated will be gibberish, especially if the description doesn‚Äôt specify the exact wording, resulting in some text that doesn‚Äôt feel quite¬†right.\n\n\n![](/content/images/1gKUcvKybfn1xcLVehVpY8A.png)\n![](/content/images/1ebzkbn0S93pZ-ENyHkJ8lw.png)\n![](/content/images/1OfI740MRnRK69AUYd9laZw.png)\n![](/content/images/1JGuhZa4Vyw8z9jixlVvVZw.png)\n*square, polaroid a sign that says hello world photography*\n\nIn an effort to prevent DALL¬∑E 2 being used for harmful purposes OpenAI filters text prompts such that DALL¬∑E 2 will not generate images for prompts containing language which is violent, adult, or political. Whilst it is important to prevent the use of image generation AI for harmful purposes these text filters have been known to be overly restrictive at times, with some users reporting seemingly innocent words or phrases being filtered.\n\n\n<blockquote class=\"twitter-tweet\"><p lang=\"en\" dir=\"ltr\">I can&#39;t force dalle2 to create ü§Ø emoji, because of their content filter, I used up my last credits while trying üòï<a href=\"https://twitter.com/hashtag/dalle2?src=hash&amp;ref_src=twsrc%5Etfw\">#dalle2</a> <a href=\"https://twitter.com/hashtag/dalle?src=hash&amp;ref_src=twsrc%5Etfw\">#dalle</a> <a href=\"https://twitter.com/hashtag/emojis?src=hash&amp;ref_src=twsrc%5Etfw\">#emojis</a> <a href=\"https://twitter.com/hashtag/aiemoji?src=hash&amp;ref_src=twsrc%5Etfw\">#aiemoji</a> <a href=\"https://twitter.com/hashtag/aiemojis?src=hash&amp;ref_src=twsrc%5Etfw\">#aiemojis</a> <a href=\"https://t.co/z0blFHcVr7\">pic.twitter.com/z0blFHcVr7</a></p>&mdash; AIXD (@AIRUNXD) <a href=\"https://twitter.com/AIRUNXD/status/1558308629505359872?ref_src=twsrc%5Etfw\">August 13, 2022</a></blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>\n\nIn addition to filtering text prompts, OpenAI also uses ‚Äúadvanced techniques‚Äù to prevent DALL¬∑E 2 from creating realistic images of real people (including celebrities and public figures). For example, you cannot upload a photo of yourself and modify it with DALL¬∑E 2, at least for¬†now.\n\n\n#### Will DALL¬∑E 2 replace human artists in the near¬†future?\n\n\nDefinitely not. DALL¬∑E 2 has some hard limits that ultimately prevent it from competing with human artists. Other image generation AI such as Craiyon (formerly DALL¬∑E mini), Midjourney, and Dream also show promise, but all such systems suffer from similar limitations to DALL¬∑E 2. Image generation AI are based on machine learning models and so are ultimately reliant on the creativity of the artists, photographers, and other creatives who contribute images to the data used to train these¬†models.\n\n\n![](/content/images/1qzXd69kvqrMzWx4w0qWmtQ.png)\n![](/content/images/1cbp7RaB4802gChJPSY7rpQ.png)\n![](/content/images/1Xqj-eO4Xex2rSDK9Fb3Zmw.png)\n![](/content/images/1SBxRqixb1_8rx_s0LhdcNw.png)\n*digital art of a doctor using a stethoscope, in the style of van gogh, white background*\n\nAI are also reliant on the people who label training data. To function accurately and without bias machine learning models require data that is labeled accurately, consistently, and responsibly. Whilst OpenAI have [attempted to address bias in DALL¬∑E 2](https://openai.com/blog/reducing-bias-and-improving-safety-in-dall-e-2/) the effects of bias are still evident in generated images,¬†for¬†example, descriptions including phrases like ‚Äúdoctor‚Äù typically resulting in images of¬†men.\n\n\n#### What does the future hold for DALL¬∑E¬†2?\n\n\nTo answer this question, it can be helpful to look at an example of an older image generation AI technology. GANs ([Generative Adversarial Networks](https://en.wikipedia.org/wiki/Generative_adversarial_network)) are a class of image generation models trained by using a model that spots AI-generated images to train image generation AI to make better¬†images.\n\n\nGANs can create original images like DALL¬∑E but only of a specific subject. An example implementation of GANs such as [This Person Does Not Exist](https://thispersondoesnotexist.com/) are effective only because it focuses on a particular type of image‚Ää‚Äî‚Ääin this case, faces. Despite the limitations associated with using GANs, they have seen wide use and study; rather than replacing human creativity GANs have become a tool used for creative expression.\n\n\nThere is already a dedicated online community experimenting with DALL¬∑E 2 (which has produced some great [resources on how to get the best results](https://dallery.gallery/the-dalle-2-prompt-book/) when writing descriptions) so it seems likely that DALL¬∑E 2 will, like GANs, become a tool used to support human creativity, not replace¬†it.\n\n\nIt is possible to imagine then that 10 years from now, we may see tools such as DALL¬∑E 2 reach a level of sophistication such that using them is more like working in partnership with another person rather than simply using a¬†tool.\n\n\n![](/content/images/1y2v3JyyIYQiu8WeTHvaysg.png)\n*bright, vibrant, modern AI working with humans to create art 3d¬†render*\n\n[DALL¬∑E 2: what happens when machines make art?](https://medium.com/daemon-engineering/dall-e-2-what-happens-when-machines-make-art-ebd94b3f028b) was originally published in [daemon-engineering](https://medium.com/daemon-engineering) on Medium.",
            "status": "published",
            "created_at": 1662937200000,
            "published_at": 1662937200000,
            "updated_at": 1662937200000,
            "author_id": 1,
            "canonical_url": "https://medium.com/daemon-engineering/dall-e-2-what-happens-when-machines-make-art-ebd94b3f028b"
          },
          {
            "id": 3,
            "title": "Turn a Raspberry Pi into an IoT device with AWS",
            "slug": "turn-a-raspberry-pi-into-an-io",
            "markdown": "#### Cheap and easy IoT with¬†AWS\n\n\n![](/content/images/1IjLjuIeNcSFk_icBgrxapg.png)\n*‚Äúraspberry pi connected to the internet, digital art‚Äù [**DALL¬∑E¬†2**](https://medium.com/daemon-engineering/dall-e-2-what-happens-when-machines-make-art-ebd94b3f028b)*\n\nIn this article I will explain how you can make your own IoT device using a Raspberry Pi and some free AWS services.\n\n\n#### What is¬†IoT?\n\n\nThe Internet of Things (IoT) is the name given to devices which can communicate and/or be controlled over networks such as the internet. Examples include smart home appliances like lights or thermostats.\n\n\n#### Why use¬†IoT?\n\n\nIoT functionality can significantly increase the cost of products because of the extra hardware required and because of the costs associated with running the networks which allow them to communicate. Why then go to the trouble of adding IoT to your product or buying a product with IoT functionality?\n\n\nIoT functionality can provide many useful features:\n\n\n* it can allow devices to be controlled remotely\n* allow systems to be easily operated by a timer or respond to sense¬†data\n* integrate with voice assistants like Alexa, Google, or¬†Siri\n\n\n### What you‚Äôll¬†need\n\n\n* A Raspberry Pi with an internet connection, this will become the brains of your IoT¬†device.\n* An AWS account, you‚Äôll need this to create and manage the AWS services you‚Äôll¬†use.\n\n\n#### 1. Creating the SQS¬†queue\n\n\nThe Simple Queuing Service is an AWS service which allows sending and receiving messages via a web service. We can use an SQS to allow an Alexa skill and a Raspberry Pi to communicate via the internet.\n\n\n[Create an SQS queue from the AWS¬†console](https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-configure-create-queue.html).\n\n\n[Create an IAM role with permission to read and write from the queue](https://docs.amazonaws.cn/en_us/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-basic-examples-of-iam-policies.html) you just created so you can send and receive messages from it programmatically.\n\n\nDownload the CSV with the secrets for your new role and save it somewhere safe for later (in a password manager for example).\n\n\n#### 2. Writing a custom Alexa¬†skill\n\n\nAn Alexa skill is effectively a third party app which you can add to your Amazon account to extend the functionality of your Alexa enabled devices. By creating an Alexa skill we can use Alexa to control our IoT¬†device.\n\n\nGo to the [Alexa developer console](https://developer.amazon.com/alexa/console/ask) and click the create skill button: this will prompt you to add a name for your skill. Once you have entered the name of your skill select the language you want to create your skill for and then select the custom model option. Finally select the Alexa-hosted Python option to host the backend for your new¬†skill.\n\n\nYou will now see the Alexa Developer console which provides you with all the tools you need to build your skill. An Alexa skill has three main components:\n\n\n* an invocation name which is used to open the skill i.e. ‚ÄúWeather¬†App‚Äù\n* an interaction model which consists of intents (things you can ask the skill to do i.e. ‚ÄúTell me what the weather will be like tomorrow)\n* a Lambda function which handles requests made to the¬†skill.\n\n\nYour invocation name is the same as the name of your skill by default, you can change this but due to [some quirks](https://amazon.developer.forums.answerhub.com/questions/30910/can-i-change-the-name-of-the-alexa-skill.html) of the skill publishing and certification process it is often easier to create a new skill with a different name at this stage rather than changing the invocation name.\n\n\nAdd new intents to your interaction model by selecting the ‚ÄúIntents‚Äù option under the interaction model tab. To further customise your intent you may wish to use slots, which are essentially variables in the context of Alexa¬†intents.\n\n\nIf you want to be able to make requests to Alexa which includes data like times, numbers, search queries etc you will need to use a slot to specify where in the intent that data goes and what that data should look like. Either use a pre-existing slot type (in the example intent below the message slot is an AMAZON.SearchQuery slot) or define your own by selecting the ‚ÄúCreate Custom Slot‚Äù option from the slots tab and specifying possible values for the¬†slot.\n\n\nIf you wanted to create a slot that would allow a user to specify whether they want the lights on or off you could create a custom slot called lights with the possible values ‚Äúon‚Äù or ‚Äúoff‚Äù and then include this an intent to control the¬†lights.\n\n\n![](/content/images/1zUefzho2BKveBXusU_SR2A.png)New intents with slots for a short¬†message\n\nOnce you have added your new intent or intents to the model save and build the model. You can now test your new intents by clicking the evaluate model. Once you‚Äôre happy with your new intents you can modify the Lambda function which handles intents by opening the Code¬†tab.\n\n\n![](/content/images/1rOumAoWPxFEs-R1oGrSmQg.png)\n*Testing the new intents added to the interaction model*\n\nAdd the following snippet to the lambda\\_function.py file before the LaunchRequestHandler. Enter the credentials from the role you created earlier and the details of your SQS queue. This snippet will allow you to connect to the SQS queue and push messages to it. Later we will read these messages with the Raspberry Pi.\n\n```\n...\naccess_key = '********************'\naccess_secret = '********************'\nregion = '********************'\nqueue_url = '********************'\nclient = boto3.client('sqs', aws_access_key_id = access_key, aws_secret_access_key = access_secret, region_name = region)\ndef push_message(client, message_body, url):\n    response = client.send_message(QueueUrl = url, MessageBody= message_body )\n...\n```\n\n\nAdd an intent handler for your new intent as below, make sure you also add your intent handler to the skill builder at the end of the¬†file.\n\n\n\n\n```\n...\nclass SendMessageIntentHandler(AbstractRequestHandler):\n    \"\"\"Handler for SendMessage Intent.\"\"\"\n    def can_handle(self, handler_input):\n        # type: (HandlerInput) -> bool\n        return ask_utils.is_intent_name(\"SendMessageIntent\")(handler_input)\ndef handle(self, handler_input):\n        # type: (HandlerInput) -> Response\n        message = get_slot_value(handler_input=handler_input, slot_name=\"message\")\n        push_message(client, message, queue_url)\n        \n        speak_output = \"OK\"\nreturn (\n            handler_input.response_builder\n                .speak(speak_output)\n                # .ask(\"add a reprompt if you want to keep the session open for the user to respond\")\n                .response\n        )\n...\nsb = SkillBuilder()\nsb.add_request_handler(LaunchRequestHandler())\nsb.add_request_handler(SendMessageIntentHandler())\nsb.add_request_handler(HelpIntentHandler())\nsb.add_request_handler(CancelOrStopIntentHandler())\nsb.add_request_handler(FallbackIntentHandler())\nsb.add_request_handler(SessionEndedRequestHandler())\nsb.add_request_handler(IntentReflectorHandler()) # make sure IntentReflectorHandler is last so it doesn't override your custom intent handlers\nsb.add_exception_handler(CatchAllExceptionHandler())\nlambda_handler = sb.lambda_handler()\n```\n\n\nSave and deploy your skill. You can now test your skill by going to the Test tab and invoking your skill followed by one of your new intents. You should be able to see new messages being added to your SQS queue by using the send and receive option in the AWS console and polling your queue for messages.\n\n\n#### 3. Setting up the Raspberry Pi\n\n\nGet your Raspberry Pi loaded up with your OS of choice. Create a new Python script and add the below code snippet. On running this script you should see a message from your SQS queue printed to the Python shell (if there aren‚Äôt any messages in your queue yet you can either use your new Alexa skill or the AWS console to push some messages for testing).\n\n```\nimport boto3\naccess_key = '********************'\naccess_secret = '********************'\nregion = '********************'\nqueue_url = '********************'\nclient = boto3.client('sqs', aws_access_key_id = access_key, aws_secret_access_key = access_secret, region_name = region)\ndef pop_message():\n    responses = client.receive_message(QueueUrl = queue_url, MaxNumberOfMessages = 10)\nif 'Messages' in responses:\n    messages = responses['Messages']\n    client.delete_message(QueueUrl = queue_url, ReceiptHandle = messages[0]['ReceiptHandle'])\n    return messages[0]['Body']\nmessage = pop_message()\nif message:\n    print(message)\n```\n\n\n[Using the systemd linux utility](https://www.raspberrypi-spy.co.uk/2015/10/how-to-autorun-a-python-script-on-boot-using-systemd/) you can run this script when your Raspberry Pi boots up, so all you need to do is connect your Pi to power for it to start reading messages from¬†SQS.\n\n\nYou can then take the messages you read from the queue and use the payload for whatever you want, you could: [control lights using the GPIO pins on the Raspberry Pi](https://www.hackster.io/nathansouthgate/control-raspberry-pi-linux-device-from-alexa-b558ad), play a song from a bluetooth device connected to the Raspberry Pi, or even turn up your heating on your way¬†home.\n\n\n#### Conclusion\n\n\nThe versatile nature of SQS queues means that you can use this pattern for a variety of different applications. You can even use a second queue or modify your existing queue to allow messages to be sent back from the Raspberry Pi to the Alexa skill to output data from sensors or other¬†devices.\n\n\nWhilst this approach isn‚Äôt the most scalable( for large numbers of devices see [AWS IoT](https://aws.amazon.com/iot/)) for small projects this is a quick and easy approach.\n\n[Turn a Raspberry Pi into an IoT device with AWS](https://medium.com/daemon-engineering/turn-a-raspberry-pi-into-an-iot-device-with-aws-c82b03902d7a) was originally published in [daemon-engineering](https://medium.com/daemon-engineering) on Medium.",
            "status": "published",
            "created_at": 1665529200000,
            "published_at": 1665529200000,
            "updated_at": 1665529200000,
            "author_id": 1,
            "canonical_url": "https://medium.com/daemon-engineering/turn-a-raspberry-pi-into-an-iot-device-with-aws-c82b03902d7a"
          },
          {
            "id": 4,
            "title": "Clustered Keycloak SSO Deployment in AWS",
            "slug": "clustered-keycloak-deployment-in-aws",
            "markdown": "[Keycloak](http://www.keycloak.org/) is an open source Identity and Access Management tool with features such as Single-Sign-On (SSO), Identity Brokering and Social Login, User Federation, Client Adapters, an Admin Console, and an Account Management Console.\n\n### Why use Keycloak?\n\nThere are several factors to deciding whether or not to use Keycloak or a SaaS IAM Service like AWS SSO. SaaS IAM services are typically easier to implement, better supported, and do not require manual deployment but Keycloak is free to use, feature rich, and flexible.\n\n### Pre-requisites\n\nThis guide assumes you already have at least one Keycloak instance with a Postgres database configured, if this is the case your _keycloak.conf_ should include a section that looks something like the example below.\n\n```\ndb=postgres\ndb-password=<your db password>\ndb-userame=keycloak\ndb-pool-initial-size=1\ndb-pool-max-size=10\ndb-schema=public\ndb-url-database=keycloak\ndb-url-host=<url of your db>\ndb-url-port=5432\n```\n    \n\nIf you do not yet have your database configured please refer to [the documentation on configuring relational databases for Keycloak](https://www.keycloak.org/server/db).\n\n### Configuring JDBC Ping\n\nIn order for Keycloak instances to cluster they must discover each other and this can be achieved by using JDBC Ping which allows nodes to discover each other via your existing database. JDBC Ping is a convenient discovery method because it does not require the creation of additional AWS resources and is compatible with AWS unlike the default discovery method (multicast) which is not permitted by AWS.\n\nIn order to use JDBC Ping we first need to define a transport stack, this can be achieved by adding the below element to the _infinispan_ tag in your _cache-ispn.xml_ file and replacing the default values (these should match the _db-password_ and _db-url-host_ from your _keycloak.conf_ file).\n\n```\n<jgroups>\n    <stack name=\"jdbc-ping-tcp\" extends=\"tcp\">\n        <JDBC_PING connection_driver=\"org.postgresql.Driver\"\n                    connection_username=\"keycloak\"\n                    connection_password=\"<your database password>\"\n                    connection_url=\"jdbc:postgresql://<url of your database>:5432/keycloak\"\n                    initialize_sql=\"CREATE TABLE IF NOT EXISTS JGROUPSPING (own_addr varchar(200) NOT NULL, cluster_name varchar(200) NOT NULL, ping_data BYTEA, constraint PK_JGROUPSPING PRIMARY KEY (own_addr, cluster_name));\"\n                    info_writer_sleep_time=\"500\"\n                    remove_all_data_on_view_change=\"true\"\n                    stack.combine=\"REPLACE\"\n                    stack.position=\"MPING\" />\n    </stack>\n</jgroups>\n```\n    \n\nWe have now defined a new JGroups stack which will create a table in your database if one doesn‚Äôt already exist which Keycloak instances can use to discover each other, when you start a new Keycloak instance it will write its name as a new record into this table. To use this stack simply amend the _transport_ element as shown below to reference the newly defined stack.\n\n```\n<transport lock-timeout=\"60000\" stack=\"jdbc-ping-tcp\"/>\n```\n\n### Configuring Security Groups\n\nKeycloak uses Infinispan to cache data both locally to the Keycloak instance and for remote caches. Infinispan by default uses port 7800 so we need to configure the Security Group our Keycloak instances are deployed to in order to permit both ingress and egress via port 7800. This can be done in a number of ways such as via the AWS Console, below is an example of configuring ports for Keycloak using Terraform.\n\n```\n## keycloak cluster egress\nresource \"aws_security_group_rule\" \"keycloak_cluster_egress_to_keycloak\" {\n    description              = \"keycloak cluster\"\n    from_port                = 7800\n    protocol                 = \"tcp\"\n    security_group_id        = aws_security_group.keycloak.id\n    source_security_group_id = aws_security_group.keycloak.id\n    to_port                  = 7800\n    type                     = \"egress\"\n}\n\n## keycloak cluster ingress\nresource \"aws_security_group_rule\" \"keycloak_cluster_ingress_to_keycloak\" {\n    description              = \"keycloak cluster\"\n    from_port                = 7800\n    protocol                 = \"tcp\"\n    security_group_id        = aws_security_group.keycloak.id\n    source_security_group_id = aws_security_group.keycloak.id\n    to_port                  = 7800\n    type                     = \"ingress\"\n}\n```   \n\n### Restarting Keycloak\n\nKeycloak does not automatically apply changes made to its configuration so you will need to restart your Keycloak instance/instances for clustering to work. First run the following from the terminal to rebuild your Keycloak instance to register the changes we made to your configuration.\n\n```\n‚ûú/bin/kc.sh build\n```\n\nOnce you have rebuilt Keycloak restart your Keycloak service by running the following (alternatively you can restart your Keycloak instance).\n\n```\nsystemctl restart keycloak\n```\n\nYour Keycloak instances should now be running in a clustered state.\n\n### Testing your Keycloak cluster\n\nTo check that your Keycloak cluster is functioning correctly check your database and see if the _JGROUPSPING_ table both exists and includes the name of all instances currently in the cluster, your table should look something like the below.\n\n    \n|own_addr|cluster_name|ping_data|\n|--------|------------|---------|\n|*****   |ISPN        | *****   |\n|*****   |ISPN        | *****   |\n\nIf you terminate a Keycloak instance or start a new instance you should see the records in this table change.\n\n### Troubleshooting\n\n**Changes made to config files aren‚Äôt applied after building Keycloak**\n\nEnsure that the config files you have changed match those configured in _keycloak.conf_, this guide for example assumes that you have your Infinispan config file set as cache-ispn.xml in your _keycloak.conf_ file.\n```\ncache-config-file: cache-ispn.xml\n```\n\n**Keycloak services don‚Äôt start after changing config files**\n\nCheck the Keycloak logs and ensure your database access details (password and host url) are set correctly: if these values are incorrect the Keycloak service will fail to start.\n\n### Resources\n\n[\nUse of JDBC_PING with Keycloak 17 (Quarkus distro)](https://keycloak.discourse.group/t/use-of-jdbc-ping-with-keycloak-17-quarkus-distro/13571/29)\n\n[Embedding Infinispan caches in Java applications](https://infinispan.org/docs/dev/titles/embedding/embedding.html#jgroups-cloud-discovery-protocols_cluster-transport)\n\n[Keycloak Server caching](https://www.keycloak.org/server/caching)\n\n[Clustered Keycloak SSO Deployment in AWS](https://www.dae.mn/insights/clustered-keycloak-sso-deployment-in-aws) was originally published on the [Daemon Insights blog](https://www.dae.mn/insights)",
            "status": "published",
            "created_at": 1677024000000,
            "published_at": 1677024000000,
            "updated_at": 1677024000000,
            "author_id": 1,
            "canonical_url": "https://www.dae.mn/insights/clustered-keycloak-sso-deployment-in-aws"
          },
          {
            "id": 5,
            "title": "Backup your OpenSearch indices with manual snapshots",
            "slug": "backup-your-opensearch-cluster-with-snapshots",
            "markdown": "You're making a change to your OpenSearch managed service and it's all going great - right up until you make a mistake, destroying your cluster and causing you to lose all your indices. If only you had a snapshot you could restore your cluster from? Too bad you didn't create any.¬†\n\n![Kermit the frog makes a rookie devops error](/content/images/kermit1.png)\n\nTaking OpenSearch snapshots is relatively easy but may require making some configuration changes to your IAM roles. It's definitely worth doing because once you've successfully taken a snapshot you can use it to restore the indices in deleted, destroyed, or corrupted OpenSearch clusters or even create a duplicate cluster with the same data.\n\n### Prerequisites\n\nIn order to manually take snapshots you'll need admin access to your OpenSearch service API either [via curl](https://docs.aws.amazon.com/opensearch-service/latest/developerguide/managedomains-snapshots.html#:~:text=every%20half%20hour.-,Take%20a%20snapshot,-You%20specify%20the) or OpenSearch devtools, in this guide I'll be using the latter method.\n\nBefore taking a snapshot you will need to create a role that will allow your OpenSearch service to write the snapshot to an S3 bucket and grant permission to the OpenSearch service to use that role. The Terraform for your IAM config should look something like the below, for more details see the [AWS documentation](https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_manage-attach-detach.html#add-policies-console).\n\n_IAM role_\n```\nresource aws_iam_role\" \"es_snapshot\" {\n  name                 = \"es-snapshot\"\n  managed_policy_arns  = [aws_iam_policy.es_snapshot.arn]\n  assume_role_policy   = <<EOF\n{\n\"Version\" : \"2012-10-17\",\n\"Statement\" : [{\n    \"Sid\" : \"\",\n    \"Effect\" : \"Allow\",\n    \"Principal\" : {\n    \"Service\" : \"es.amazonaws.com\"\n    },\n    \"Condition\" : {\n    \"StringEquals\" : {\n        \"aws:SourceAccount\" : \"<your aws account id>\"\n    },\n    \"ArnLike\" : {\n        \"aws:SourceArn\" : \"<the arn for your opensearch cluster>\"\n    }\n    },\n    \"Action\" : \"sts:AssumeRole\"\n}]\n\n}\n  EOF\n}\n```\n\nNote the condition in the above terraform statement: this limits access to this role to a specific OpenSearch service with your AWS account, without it any OpenSearch service could assume this role.\n\n_IAM policy_\n```\nresource \"aws_iam_policy\" \"es_snapshot\" {\n  name = \"es-snapshot-policy\"\n  policy = jsonencode({\n    \"Version\" : \"2012-10-17\",\n    \"Statement\" : [{\n      \"Action\" : [\n        \"s3:ListBucket\"\n      ],\n      \"Effect\" : \"Allow\",\n      \"Resource\" : [\n        \"<arn of the s3 bucket you want to store your snapshots in>\"\n      ]\n      },\n      {\n        \"Action\" : [\n          \"s3:GetObject\",\n          \"s3:PutObject\",\n          \"s3:DeleteObject\"\n        ],\n        \"Effect\" : \"Allow\",\n        \"Resource\" : [\n          \"<arn of the s3 bucket you want to store your snapshots in>/*\"\n        ]\n      }\n    ]\n  })\n}\n```\n\n### Register a snapshot repository\n\nIn order to take a snapshot you first need to configure a snapshot repository to store your snapshots. In this guide I'll be covering how to do this using an S3 bucket\n\nFirst, if there isn't one already you will need to register a snapshot repository, you can use the get request below to list any existing repositories (do not use _cs-automated-enc_, it is reserved by OpenSearch for automated snapshots).\n```\nGET _cat/repositories\n```\n\nIf needed, register a new snapshot repository like so (note the use of the role we created in the previous section).\n```\nPUT _snapshot/opensearch-snapshots\n{\n  \"type\": \"s3\",\n  \"settings\": {\n    \"bucket\": \"<your s3 bucket name>\",\n    \"region\": \"eu-west-1\",\n    \"role_arn\": \"<arn of your snapshot role>\",\n    \"server_side_encryption\": true\n  }\n}\n```\n\n### Manually taking a snapshot\n\nCheck for any ongoing snapshots, you cannot take a snapshot if one is already in progress and OpenSearch automatically takes snapshots periodically.\n```\nGET _snapshot/_status\n```\n\nTake a snapshot. Adding the data to the end of the snapshot name is optional, but I'd recommend adding the correct time here so you can easily find the snapshot if you need to restore from it later.\n```\nPUT _snapshot/opensearch-snapshots/snapshot-2023-03-13-1135\n```\n\nCheck snapshot progress with the first get request below and then view it with the second once complete. Use of the ‚Äúpretty‚Äù query is not required but helps to make the output more readable.\n```\nGET _snapshot/_status\nGET _snapshot/opensearch-snapshots/_all?pretty\n```\n\nYou should see your snapshot listed alongside any pre-existing snapshots. Congratulations, you‚Äôre now ready to restore from a snapshot should you ever need to. Don‚Äôt stop here though, I recommend that you continue with the next section to familiarise yourself with the process of restoring from a snapshot - you should also take snapshots regularly to help reduce the risk of data loss.\n\n### Restoring from a snapshot\n\n\nCheck for snapshot repositories, if the cluster was destroyed you will need to re-add the snapshot repository that contains the snapshot you will be restoring from.\n```\nGET _cat/repositories\n```\n\nRe-register your snapshot repository if necessary (if the cluster is destroyed and recreated OpenSearch will not be aware of the pre-existing repositories even though they still exist in your S3 bucket) using the same command you used to register it initially.\n```\nPUT _snapshot/opensearch-snapshots\n{\n  \"type\": \"s3\",\n  \"settings\": {\n    \"bucket\": \"<your s3 bucket name>\",\n    \"region\": \"eu-west-1\",\n    \"role_arn\": \"<arn of your snapshot role>\",\n    \"server_side_encryption\": true\n  }\n}\n```\n\nYou can now view all the available snapshots in your repository.\n```\nGET _snapshot/opensearch-snapshots/_all?pretty\n```\n\nYou should see your each of your snapshots output something like this.\n```\n1| snapshot-2023-03-13-1450 SUCCESS 1678978233 14:50:33 1678978966 15:02:46 12.2m 61 135 0 135\n```\n\nCheck for any existing indexes and delete them if they exist in your snapshot, as restored indices may share the same name as those in the snapshot so in order to restore them it is recommended to delete all of the indices in your cluster prior to recovering from a snapshot (alternatively you can exclude these indices by adding them to the indices parameter and mark them to be ignored with ‚Äú-‚Äù in the next step).\n```\nGET _cat/indices\nDELETE _all\n```\n\nRestore data indexes from your chosen snapshot, note that we're excluding the Opendistro indices here. These are used for storing data about the cluster and are created automatically, we don't need to restore them from the snapshot and if we attempt to we will be unable to restore as they will clash with the snapshot version.\n```\nPOST _snapshot/opensearch-snapshot/snapshot-2023-03-13-1135/_restore\n{\"indices\": \"-.opendistro*\"}\n```\n\nCheck the progress of index recovery with the get request below, this will return a list of all the indices that are being restored along with the progress of restoring each to the cluster. When this call returns null, the recovery process is complete.\n```\nGET _cat/recovery\n```\n\nYou can now check the indices on your cluster using the below get request.\n```\nGET _cat/indices\n```\n\nCongratulations, you‚Äôve successfully restored your cluster from a snapshot. If you‚Äôve been following this guide through from the start you should now be equipped to use snapshots to protect your indices against data loss. If you‚Äôve just used this guide to restore your OpenSearch cluster after a real issue I‚Äôm glad that we could help out, now relax in the knowledge that your data is safe.\n\n¬†![Kermit relaxes after resolving a devops issue](/content/images/kermit2.jpeg)\n\n### Resources\n\n[Manage OpenSearch domain snapshots](https://docs.aws.amazon.com/opensearch-service/latest/developerguide/managedomains-snapshots.html)  \n[Adding IAM Identity Permissions](https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_manage-attach-detach.html#add-policies-console)  \n[Modifying a role trust policy](https://docs.aws.amazon.com/IAM/latest/UserGuide/roles-managingrole-editing-console.html#roles-managingrole_edit-trust-policy)\n\n[Backup your OpenSearch indices with manual snapshots](https://www.dae.mn/insights/backup-your-opensearch-indices-with-manual-snapshots) was originally published on the [Daemon Insights blog](https://www.dae.mn/insights)",
            "status": "published",
            "created_at": 1682377200000,
            "published_at": 1682377200000,
            "updated_at": 1682377200000,
            "author_id": 1,
            "canonical_url": "https://www.dae.mn/insights/backup-your-opensearch-indices-with-manual-snapshots"
          },
          {
            "id": 6,
            "title": "Aerospike Barcelona Data Management Community Meetup",
            "slug": "aerospike-barcelona-data-management-community-meetup",
            "markdown": "I had an amazing time speaking at the Aerospike Barcelona Data Management Community Meetup this week about working with flight radar data in Apache Druid. The team at Criteo were amazing hosts, super welcoming and friendly and the audience were really engaged with great questions after talks wrapped up. I'm looking forward to speaking at another Aerospike event later this year in Copenhagen.\n\nIf you'd like to check out my talk you can watch the recording below.\n\n<div class=\"videoWrapper\"><iframe src=\"https://www.youtube.com/embed/_8jh41yxC2s?si=MlVUZccWsofyFfMz\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" style=\"display:block;\" allowfullscreen></iframe></div>",
            "status": "published",
            "created_at": 1724626800000,
            "published_at": 1724626800000,
            "updated_at": 1724626800000,
            "author_id": 1
          },
          {
            "id": 7,
            "title": "Apache Druid Summit Panel",
            "slug": "apache-druid-summit-panel",
            "markdown": "My last week at Imply was at Druid Summit, it was an absolute pleasure working alongside my former colleagues Reena Leone, Peter Marshall, and Dave Klein running the event and getting to see so many excellent presentations from members of the Druid community. I organized all the panels for the 2024 event and you can see the recording of the Ops and Optimization panel I hosted below.\n\n<div class=\"videoWrapper\"><iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/aF2u46HwEwo?si=tXLC9Mi_015ysQR2\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe></div>",
            "status": "published",
            "created_at": 1726959600000,
            "published_at": 1726959600000,
            "updated_at": 1726959600000,
            "author_id": 1
          },
          {
            "id": 8,
            "title": "Adventures in POSSE",
            "slug": "adventures-in-POSSE",
            "markdown": "<meta property=\"og:image\" content=\"{{ site.url }}/content/images/anne-fehres_luke-conroy_ai4media_hidden-labour-of-internet-browsing_2560x1440.jpg\">\n\n![A collage picturing a chaotic intersection filled with reCAPTCHA items like crosswalks, fire hydrants and traffic lights, representing the unseen labor in data labelling.](/content/images/anne-fehres_luke-conroy_ai4media_hidden-labour-of-internet-browsing_2560x1440.jpg)\n<span><a href=\"https://www.annefehres.com/\">Anne Fehres and Luke Conroy</a> & <a href=\"https://www.luke-conroy.com/\">AI4Media</a> / <a href=\"https://www.betterimagesofai.org\">Better Images of AI</a> / Hidden Labour of Internet Browsing / <a href=\"https://creativecommons.org/licenses/by/4.0/\">Licensed by CC-BY 4.0</a></span>\n\n## Post Own Site Syndicate Everywhere\n\n[POSSE](https://indieweb.org/POSSE) (Post [on your] Own Site Syndicate Everywhere) is a simple concept: when you create digital content like articles, guides, or videos simply upload it to your own website before posting links back to that content on third party sites like Medium or YouTube. One key advantage of this approach is it provides a degree of indepdence from third party platforms as all your content is preserved on your site so the loss of a social media account doesn't mean losing years of work.\n\nI first learned about POSSE in [the excellent article of the same title by Molly White](https://www.citationneeded.news/posse/) and have since adopted the approach for my own work.\n\nAn issue often sited with POSSE is that posting content across multiple channels can be labour intensive either as a result of the work required in manually posting via several third party platforms or maintaining the tooling required to automate the process. There are some ongoing efforts to help simplify the process of cross posting across multiple channels including Ryan Barrett's [Bridgy Fed](https://fed.brid.gy/) project which serves as a bridge between decentralized social networks.\n\nI wanted to build POSSE into my existing [CICD](https://github.com/resources/articles/devops/ci-cd) for my site which I have configured as [GitHub Actions](https://github.com/features/actions) - I decided to (perhaps foolishly) write a few quick scripts to post links to new articles on my Website to my various social media accounts which proved tricker than I expected! In this article I'll talk you through my attempt at automating the common POSSE task of reposting links with a hope that elements of it may be useful to your own projects. All my code is available on my [GitHub](https://github.com/hevansDev/portfolio-website).\n\n## Posting on your own site...\n\nI'm using [Jekyll](https://jekyllrb.com/) a Ruby based static website generator along with the awesome [Contrast theme by Niklas Buschmann](https://niklasbuschmann.github.io/contrast/) for my personal site. I use GitHub Actions workflows to to build the static HTML from my Jekyll project and then copy it across to an Nginx server running on a Raspberry Pi in my Homelab.\n\n![A screenshot of a successfully completed github action for building and deploying a jekyll site](/content/images/gha_screenshot.png)\n*A screenshot of a successfully completed github action for building and deploying a jekyll site*\n\nI like being able to write my posts in both HTML and Markdown because of the flexibility it provides so this approach works well for me - it also makes it easy to edit content already published to my site. I can also test what articles will look like by running Jekyll locally which is really helpful for catching mistakes prior to publishing.\n\n## ...and syndicate everywhere!\n\nOnce I've written a new article I want to link to it from all the other places I have a presence on the internet like Bluesky, LinkedIn, and Mastodon. Frustratingly at time of writing LinkedIn doesn't appear to provide any methods for posting to your own LinkedIn account with their API - instead only allowing you to [post to a company page](https://learn.microsoft.com/en-us/linkedin/marketing/community-management/shares/posts-api?view=li-lms-2024-10&tabs=http) so for now I'm limited to (still two great options!) Bluesky and Mastodon.\n\n### Getting new posts from a PR\n\nBefore I can post any links to articles I need to get new articles from PRs merged to my repo. All the articles on my site are stored in a directory called `_posts` so it's easy enough to get a list of articles by running the handy [`changed-files`](https://github.com/tj-actions/changed-files) action against that directory.\n\n```\n- name: Get changed files\n    id: changed-files\n    uses: tj-actions/changed-files@v45.0.4\n    with:\n    files: |\n        _posts/**\n```\n\nOnce I have a list of all the new articles I can easily iterate over them with a Bash loop. At the moment though this list is of file paths rather than links to the actual articles but this is easy enough to fix by extracting the name of the articles from the file path and appending it to the site url.\n\n```\n- name: Post all new posts\n    if: steps.changed-files.outputs.any_changed == 'true'\n    env:\n    ALL_NEW_POSTS: ${{ steps.changed-files.outputs.added_files }}\n    run: |\n    \n    for file in ${ALL_NEW_POSTS}; do\n        echo $file\n        # Get new post URLs from diff\n        blog_url=\"https://hughevans.dev/${file:18:-3}\"\n```\n\nNow that I can get a url for each new article it's relatively simple to just POST that url in the text field of a post to the social media platform of my choice.\n\n### Mastodon\n\nPosting a link to Mastodon and getting a nice embedded card is really easy as Mastodon pulls out all the [OpenGraph](https://ogp.me/) information and renders it automatically for you into a nice [preview card](https://box464.com/posts/mastodon-preview-cards/).\n\nAll you need to POST via the Mastodon API is your user token which can be found under Preferences > Development ([see the Mastodon docs](https://docs.joinmastodon.org/client/intro/) for more information).\n\nWith my list of new articles I can easily use the below POST request via curl to create a post on Mastodon with a nice preview card for my articles.\n\n```\n# Post to Mastodon\ncurl -X POST -d \"{\\\"status\\\":\\\"$blog_url\\\", \\\n\\\"media_ids\\\":null,\\\"poll\\\":null}\" \\\n-H \"Content-Type: application/json\" \\\n-H \"Authorization: Bearer ${{secrets.MASTODON_USER_TOKEN}}\" \\\n\"https://hachyderm.io/api/v1/statuses\"\n```\n\nTa-dah!\n\n![A post on mastodon with a link to a blog about me speaking at the Barcelona Aerospike Meetup](/content/images/mastodon-post.png)\n*A post on mastodon with a link to a blog about me speaking at the Barcelona Aerospike Meetup*\n\n### Bluesky\n\nI found posting an embed card to Bluesky much trickier than posting via the Mastodon API (as anyone unfortunate enough to be following me on Bluesky whilst I was writing this article probably noticed!)\n\nI found [these examples](https://gist.github.com/pojntfx/72403066a96593c1ba8fd5df2b531f2d) shared by [Felicitas Pojtinger](https://gist.github.com/pojntfx) really helpful but my main stumbling block was that I needed to manually include the OpenGraph information as fields in the body of my post requests as [Bluesky won't detect this automatically yet](https://github.com/bluesky-social/social-app/issues/1672).\n\nBefore posting via the API you need to create a new app password under Settings > Privacy and Security > App Passwords. I learned that posting to Bluesky isn't as simple as just making a POST request with my app password, as a result of running on the decentralized [AT Protocol](https://atproto.com/), posting via the API required that I find the [Decentralized Identifier (or DID)](https://atproto.com/specs/did) for my handle and with that get a session API key - I did both of these with the pair of curl commands in the Bash snippet below.\n\n```\n# Post to Bluesky\nexport APP_PASSWORD='${{secrets.BLUESKY_APP_PASSWORD}}'\n\nHANDLE='hevansdev.bsky.social'\nDID_URL=\"https://bsky.social/xrpc/com.atproto.identity.resolveHandle\"\nexport DID=$(curl -G \\\n    --data-urlencode \"handle=$HANDLE\" \\\n    \"$DID_URL\" | jq -r .did)\n\n# Get API key with the app password\nAPI_KEY_URL='https://bsky.social/xrpc/com.atproto.server.createSession'\nPOST_DATA=\"{ \\\"identifier\\\": \\\"${DID}\\\", \\\"password\\\": \\\"${APP_PASSWORD}\\\" }\"\nexport API_KEY=$(curl -X POST \\\n    -H 'Content-Type: application/json' \\\n    -d \"$POST_DATA\" \\\n    \"$API_KEY_URL\" | jq -r .accessJwt)\n```\n\nOnce I had an API key posting text to Bluesky was simple, however: I wanted to create a preview card with an embedded image and an article title. I couldn't find any neat way to do this directly in the Bluesky API - as a work around I used the snippet below to pull the Open Graph data from the blog post automatically so I can pass it in the Bluesky POST body.\n\n```\n# Get page og image\nog_img_url=$(curl -L $blog_url | grep 'og.image' | grep -oE \"(http|https)://[a-zA-Z0-9./?=_%:-]*\")\ncurl -O $og_img_url\n\n# Get page title\npage_title=$(curl $blog_url -so - | grep -o \"<title>[^<]*\" | tail -c+8)\n```\n\nSo with all that done I should be ready to POST the link right? No such luck. The embed image first needs to be uploaded as a blob and the blob link and size recorded for use in the POST.\n\n```\n# Upload embed image blob\nblob=$(curl -X POST \\\n    -H \"Authorization: Bearer ${API_KEY}\" \\\n    -H 'Content-Type: image/jpeg' \\\n    --data-binary @$(ls *.jpg) \\\n    \"https://bsky.social/xrpc/com.atproto.repo.uploadBlob\")\n\n\nblob_size=$(echo $blob | jq  '.blob.size')\n\nblob_link=$(echo $blob | jq  '.blob.ref.\"$link\"')\n```\n\nFinally, I can post the link - I can POST an empty text post to Bluesky which includes an embed with a link to my new post, the post title, link to the upload image blob, and the size of that blob.\n\n```\n# Site embed\nPOST_FEED_URL='https://bsky.social/xrpc/com.atproto.repo.createRecord'\nPOST_RECORD=\"{ \\\"collection\\\": \\\"app.bsky.feed.post\\\",\n\\\"repo\\\": \\\"${DID}\\\",\n\\\"record\\\": {\n\\\"text\\\": \\\"\\\",\n\\\"\\$type\\\": \\\"app.bsky.feed.post\\\",\n\\\"createdAt\\\": \\\"$(date -u +\"%Y-%m-%dT%H:%M:%SZ\")\\\",\n\\\"embed\\\": {\n    \\\"\\$type\\\": \\\"app.bsky.embed.external\\\",\n        \\\"external\\\": {\n            \\\"uri\\\": \\\"$blog_url\\\",\n            \\\"title\\\": \\\"$page_title\\\",\n            \\\"description\\\": \\\"\\\",\n            \\\"thumb\\\": {\n                \\\"\\$type\\\": \\\"blob\\\",\n                \\\"ref\\\": {\n                    \\\"\\$link\\\": $blob_link\n                },\n                \\\"mimeType\\\": \\\"image/jpeg\\\",\n                \\\"size\\\": $blob_size } \n            }\n        }\n    } \n}\"\ncurl -X POST \\\n    -H \"Authorization: Bearer ${API_KEY}\" \\\n    -H 'Content-Type: application/json' \\\n    -d \"$POST_RECORD\" \\\n    \"$POST_FEED_URL\" | jq -r\n```\n\nThat POST gets us this final result.\n\n![A post on Bluesky with a link to a blog about me speaking at the Barcelona Aerospike Meetup](/content/images/bluesky-post.png)\n*A post on Bluesky with a link to a blog about me speaking at the Barcelona Aerospike Meetup*\n\n## Final thoughts\n\nI had a lot of fun playing around with automating posting for POSSE and learning a little bit about the nuances of the Bluesky API and AT Proto. In future I'd like to expand on this project with integrations for other channels like Medium - I think I'll most likely explore using existing tooling rather than building my own because of how unwieldy my implementation has ended up being.",
            "status": "published",
            "created_at": 1731283200000,
            "published_at": 1731283200000,
            "updated_at": 1731283200000,
            "author_id": 1
          },
          {
            "id": 9,
            "title": "Everything I Made in 2024",
            "slug": "everything-i-made-in-2024",
            "markdown": "<meta property=\"og:image\" content=\"{{ site.url }}/content/images/san-fran1.jpeg\">\n\n![A photo of me in front of the Golden Gate Bridge during a visit to San Francisco for Apache Druid Summit](/content/images/san-fran1.jpeg)\n*A photo of me in front of the Golden Gate Bridge during a visit to San Francisco for Apache Druid Summit*\n\n**I made a lot of things this year**\nincluding mistakes - I started the year out of a job having been let go from a Senior Platform Engineer Role because I'd pursued a Senior role before I was ready. I'm now in a better place with work but it's been a challenging year and one thing that's helped me stay grounded and keep moving forward is making.\n\nLife for me has always been divided into time spent working on one project or another, a cardboard city encompassed much of my childhood bedroom, and as a teen I spent the summer tinkering with 3D printers and Raspberry Pis. The process of coming up with an idea for a project, building, and iterating until I have an end product that I'm (mostly) happy with never fails to hold my attention and recharge my creative batteries. \n\nIn this blog I'd like to revisit some of the projects I worked on this year and share some of what I learned from each one.\n\n## Tool Storage from Reused Grocery Crates\n\n![Two sets of drawers side by side in a garage, the drawers are made from grocery crates]({{ site.baseurl  }}/content/images/tool-storage-2024-11-21.jpeg)\n\nThis was a super scrappy project: I've been running a tool library out of my garage for the past couple of years and was rapidly running out of space for donated tools so I decided to cobble something together from some grocery crates I picked up on Olio and some parts of an old fitted kitchen.\n\nI'm still using these drawers to store things so they work but they needed a bit of tweaking with shims as the drawers initially kept falling through the runners and getting stuck - should have triple checked my measurements before taking a circular saw to my offcuts of kitchen counter!\n\n## ASCII Art Photo Booth for EMF Camp\n\n![Me stood next to the ASCII booth]({{ site.baseurl  }}/content/images/ascii-booth-2024-11-21.jpeg)\n![The ASCII booth, a plastic enclosure with a clear front holding a receipt printer and a camera]({{ site.baseurl  }}/content/images/ascii-booth-2-2024-11-21.jpeg)\n\nThis year for EMF camp I built an exhibit that would take your picture and print it out as ASCII art on receipt paper. This has been done lots of times before but I fancied trying my hand at it because I thought it would be a fun project. \n\nI got this working at home but once I actually got to EMF I couldn't figure out how to light the subjects of the photos in the environment properly to get nice ASCII images which was a shame - next time around better lightning, a backdrop, and potentially a more flexible algorithm for generating ASCII art would make for better results.\n\n\n## ASCII Art Workshop for EMF CAMP\n\n![Me delivering a workshop at EMF]({{ site.baseurl  }}/content/images/emf-workshop.jpeg)\n![Students at my ASCII art workshop]({{ site.baseurl  }}/content/images/emf-workshop-2.jpeg)\n\nLuckily what did work better at EMF was my workshop, also on ASCII art, which I delivered on the last day of the festival. Although there were some AV issues (the screen in the workshop tent broke halfway through the workshop) the practice I'd put in before hand and the resources I put together meant that almost every student was able to have their own working image-to-ASCII Python app by the end of the session and some students even built their own cool versions including with animation.\n\n## Bonus Round: Things made at EMF\n\n![An EMF 2022 t-shirt which I had reprinted into a 2024 t-shirt at EMF]({{ site.baseurl  }}/content/images/emf-t-shirt-2024-11-21.jpeg)\n![All the hexpansions I collected at EMF this year, a duck, a rainbow, a goose, a 3d printed seven segment display, and a reading light]({{ site.baseurl  }}/content/images/emf-hexpansions.png)\n![A keyring I made at the blacksmithing workshop]({{ site.baseurl  }}/content/images/emf-keyring.png)\n![My badge from EMF 2024 which is a microcomputer with display and ports for hexpansions]({{ site.baseurl  }}/content/images/emf-badge.png)\n\n## Bike Hanger from Scrap Wood\n\n![A bike hanging on a wall from a bike hanger made of scrap wood]({{ site.baseurl  }}/content/images/bike-hanger-2024-11-21.jpeg)\n![Two bike hangers on a work bench made of scrap wood]({{ site.baseurl  }}/content/images/bike-hanger-2-2024-11-21.jpeg)\n\nThis was a super quick project: I was frustrated trying to work around my bike in the garage so I quickly put two brackets together from offcuts to hang my bike on the wall and get it out of the way. Drilling holes in the brick for the hardware has hard going but I'm super happy to have my floor space back and the bike hasn't fallen off the wall yet: 10/10.\n\n## Pallet Wood Coffee Table\n\n![A small wooden table made of pallet wood with a candle in front of a small blue outdoor sofa on a patio at night]({{ site.baseurl  }}/content/images/coffee-table-2024-11-21.jpeg)\n\nMy partner and I got a free sofa which we put outside on our little patio area and I decided we needed a table to go with it. I had a small pallet my partner had snagged for me on her way home (I'm a very lucky man) and I used that and a couple of old scaffolding boards to make this little coffee table. I've built outdoor furniture from pallet wood in the past and it went horribly because I failed to account for movement of the wood in the elements so this time I used outdoor rated hardware and glue and left gaps for the expansion of the wood. \n\nI wish I'd been more careful putting in the final touches because whilst squaring up the top I accidentally cut into the one of the legs which was very frustrating and not easy to fix - a little more patience in those crucial last few steps would have gone a long way.\n\n## Portable ADSB Receiver\n\n![A small pelican case with a raspberry pi and an antenna held up in front of a passing aircraft](/content/images/flight-radar-2024-11-21.jpeg)\n![A small pelican case with a raspberry pi and an antenna on a wall in a park](/content/images/flight-radar-2-2024-11-21.jpeg)\n\nI was lucky to have a opportunity to work as a Developer Advocate this year. I built a demo which used portable ADSB receivers like the one pictured to collect data about the position of aircraft and funnel it into Apache Druid before visualizing it in Grafana.\n\nI learned a lot building this demo and whilst I definitely have a lot of room for improvement on my public speaking and on communicating ideas in this format overall I was really happy with how this demo came out and the response it got from the community.\n\n## Signs for a Birthday Party\n\n![A brown UK landmark sign for a birthday party](/content/images/landmark-sign-2024-11-21.jpeg)\n![A white arrow sign with fairy lights round the edge](/content/images/arrow-sign-2024-11-26.png)\n\nMy partner and her sister had a joint birthday party this year styled after a mini music festival so I made some signs to fit the theme.\n\nFrom a distance they really look the part although my technique for the lettering (printing a stencil and covering it with masking tape before cutting out the letters) wasn't great and the letters needed a lot of clean up. I think next time around I'll try looking into how to properly make big custom stencils like this to avoid paint bleed out.\n\n## Restored Expanding Table\n\n![Me removing the old varnish from a table top](/content/images/table-before-2024-11-21.jpeg)\n![The table top with the old varnish removed](/content/images/table-after-2024-11-21.jpeg)\n\nI rescued a table that was headed for landfill for some friends that needed a larger table so they could host games of Dungeons and Dragons. It needed a lot of work as the top was falling apart and stained with paint.\n\nI made the mistake of using the wrong kind of varnish when I resealed the top which left a finish that was sticky for weeks whilst it was drying, next time I think I'd be more careful in my selection of finish!\n\n## Antweight Combat Robot\n\n![A GIF of a small, circular, two-wheeled robot spinning in circles](/content/images/robot-gif.gif)\n\nI'm a big fan of Angus Deveson's (of [Maker's Muse](www.makersmuse.com)) robotics projects so thought I'd have a go at building my own Antweight robot based on the kit from [Bristol Bot Builders](https://bristolbotbuilders.com/) and my own 3D printed chassis.\n\nI made a couple of annoying mistakes like accidentally shorting out the ESC by not properly insulating the contacts on the motors and breaking off a contact on a motor while soldering which had me sending off for replacement parts. I still haven't quite figured out how to get the handling dialed in because it's very sensitive but otherwise I'm really happy with my robot and learned a lot in the process of putting it together.\n\n## London Transport Museum Late Poster\n\n![A block printed poster with two bikes against the London Skyline above a tube train in a tunnel](/content/images/transport-poster.jpeg)\n\nI made this poster at a London Transport Museum late. I really enjoyed the exhibition on poster design and it was fun messing around with block printing. Great vibes, great company, great drinks from the bar. 10/10\n\n## Raspberry Pi Digital Fireplace\n\n![A video of a fireplace playing on an old monitor](/content/images/digital-fireplace.gif)\n\nOn a festive whim I scavenged a Raspberry Pi from another project and set up this digital fireplace for Christmas.\n\nI made things more difficult for myself than I needed to by using this older monitor (no HDMI meant no nice fullscreen video without a lot of wrangling) and adding in audio over BlueTooth which meant I needed to restart the speaker periodically because the Pi seemed to lose pairing after a day or so but overall I was really happy with the result - with the overhead lights off and the crackling of fire over the speakers you could almost mistake it for the real thing. \n\n---\n\n**I made it though 2024**\nand making helped me do that. I'm grateful for the opportunities I had to make, to collaborate with others, and the people in my life who support me and lend a friendly ear when I want to talk about how I got an old receipt printer talking to a Raspberry Pi. \n\nI'm looking forward to working on more projects in 2025.",
            "status": "published",
            "created_at": 1733875200000,
            "published_at": 1733875200000,
            "updated_at": 1733875200000,
            "author_id": 1
          },
          {
            "id": 10,
            "title": "How to use AI to write documentation that actually works",
            "slug": "how-to-use-AI-to-write-documentation-that-actually-works",
            "markdown": "Writing documentation is an oft maligned part of the software development process. It takes time away from solving technical problems and can feel like an unending task., new changes and versions can make your newly updated quick-start or API reference rapidly out of date. It comes as no surprise then that there are now many tools offering to take the pain out of this process by automating some or all of the work required to produce documentation with AI.\n\nI've seen the importance of good docs first hand. Good documentation helps teams to communicate a vision for a project both internally and with external stakeholders which in turn can lead to dodging costly misunderstandings, and can make the actual source code of the project better by keeping the whole team aligned on what they are building. In contrast bad documentation could see you wasting hours providing support to get new devs onboarded to a project. Worse still, in the case of competing products, it could see people opt to use different products entirely, even if they are less feature rich, simply because it's easier to understand how to use them. \n\nGiven the importance of good documentation, is it actually possible to use AI tools to make the process of producing documentation easier without degrading the overall quality? In this blog I will explore using Swimm, an AI documentation tool, to document one of my personal projects to try to answer this question.\n\n\n# What is Swimm?\n\n![Swimm website landing page](/content/images/swimm.png)\n\nSwimm is billed as an AI agent powered \"knowledge management tool for code\" launched by Swimm Tech in October 2019. In practice, Swimm supports two main use cases: automatically generating documentation using an index of your project source code and providing explanations to developers to help them understand complex or unfamiliar code. I chose to try out writing docs with Swimm because of its ability to generate recommendations for documentation updates based on source code whilst it's being written. After a week of re-recording instructional videos due to some product features changing, that seemed like a very appealing idea!\n\n# Generating documentation with Swimm\n\n<center><iframe width=\"100%\" height=\"500\" src=\"https://www.youtube.com/embed/dZaa7CdbI08?si=MqvNGajSK2Q1zACp\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe></center>\n\n<center>Video of my first attempt at using Swimm to generate documentation</center>\n\nI chose my [ASCII art photo booth project](https://hughevans.dev/everything-i-made-in-2024/#:~:text=ASCII%20Art%20Photo%20Booth%20for%20EMF%20Camp) that I built out for [EMF Camp 2024](https://www.emfcamp.org/) to test out the Swimm automatic documentation feature because I know the project code well, it's a very simple project consisting of one file, and I already have a basic README for the project to benchmark any AI generated docs against.\n\nTo get started I [created an account](https://app.swimm.io/register) via the Swimm website and connected it to my GitHub account, I then downloaded the [Swimm VSCode plugin](https://marketplace.visualstudio.com/items?itemName=Swimm.swimm) to my locally checked out project. I was then able to create a new markdown file with documentation for the `main.py` file in my project simply by using the generate option in the Swimm VSCode plugin and providing a title and prompt. Here's the prompt I provided:\n\n> How to deploy your own ASCII booth with this project\n\n\n> Explain how to setup this project by running the code on a raspberry pi with a webcam and thermal printer attached. You should wherever possible link to other documentation about how to acomplish more complex steps like setting up a raspberr ypi for the first time, selecting a thermal printer, creating a mastodon account, and so on. You should format the documentation as a set of step by step instructions documenting each part of the process\n\n\nLooking at the generated file the results were pretty mixed. In some areas like the introduction Swimm did a great job of understanding the purpose of the code and context of the project, giving a nice break down of what the documentation is about and outlining the steps required to get the project working.\n\n![Introduction of a piece of documentation produced by Swimm](/content/images/swimm1.png)\n\nThings start to break down a little in the next section, whilst Swimm clearly makes an attempt at following the instruction in the prompt to \"link to other documentation about how to accomplish more complex steps\" the second step here is vaguer than I would like, and the last step fails to explain how to correctly install the dependencies. \n\n![Instructions on how to set up project as generated by Swimm](/content/images/swimm2.png)\n\nSwimm missed the necessary command to install dependencies in the last step. This is likely because we did not index the entire project for [Retrieval Augmented Generation (RAG)](https://blogs.nvidia.com/blog/what-is-retrieval-augmented-generation/) or pass in the rest of the project as context. Currently, we only have the option to provide one file, so the agent behind Swimm is completely unaware of our handily available requirements file.\n\nThe rest of the documentation produced is a high level view of each function in the `main.py` file that Swimm appears to be primarily designed to produce. This could be useful - particularly as it can quickly be regenerated as docs are updated, but I feel this kind of documentation is out of place in the setup guide I instructed the agent to produce.\n\n![Function level explanation produced by Swimm](/content/images/swimm3.png)\n\nFinally, I tried out the killer feature I was really excited for: automatic suggestion generation in response to code changes. Unfortunately, this turned out to not be as automatic as I had first hoped: I added a new function to `main.py` but had to select the _Add to existing doc_ option on the new function to get Swimm to produce a suggestion. When it did, it added it to the end of the Markdown file instead of in the proper location, not exactly what I'd been hoping for but still a very quick way of fixing gaps in the documentation.\n\n![Docs for new function generate by Swimm](/content/images/swimm4.png)\n\n\n# Does Swimm really make it easier to write good documentation?\n\nI really like some aspects of the Swimm agent and think it could be useful both to generate drafts of docs and to help make suggestions for updates and improvements to existing code, however Swimm alone does not generate good documentation. The documentation produced by Swimm did not contain all of the expected information a user would need to get started from a setup guide - particularly for a project like this one with both hardware and software components. Adding to the case against Swimm and bad news if you like to work with separate docs and source code repos: the Swimm VSCode plugin doesn't appear to be able to cope with a VSCode project that contains multiple `.git` directories.\n\nOverall I think Swimm shows a lot of promise, its live tracking of changes to source code combined with a slick VSCode integration makes for a more polished experience than other competing tools, but I don't think I'll be incorporating it into my day-to-day work just yet. I'm sure that with more time I could get better results from Swimm, but ultimately like many tools I've found that the work to get desired results is not significantly less than simply writing the desired documentation, and that's before you factor in the $28 cost for the table stakes of AI code generation and CI CD integration.\n\nIt's clear though that AI code documentation tools like Swimm can generate at the very least basic documentation, with the potential to do far more in future as the technology behind them improves, and the products themselves mature. There may come a time when such tools become a prominent feature of the docs writing process, but for me that time isn't just yet.",
            "status": "published",
            "created_at": 1738540800000,
            "published_at": 1738540800000,
            "updated_at": 1738540800000,
            "author_id": 1
          },
          {
            "id": 11,
            "title": "Meetup is terrible now - Codebar Festival Fringe",
            "slug": "meetup-is-terrible-now-codebar-festival-fringe",
            "markdown": "On Saturday I had the pleasure of speaking at the [codebar](https://codebar.io/) Festival Fringe at Newspeak House about my thoughts on the decline of Meetup and approaches for tackling platform decline in the community organising space and elsewhere.\n\nThe community came together to raise hundreds for codebar and it was a privilege to get to speak alongside such fantastic speakers with lightning talks from Willow S. on obsolesce proofing your smart home, Kourosh Simpkins on watching the implications of watching (deep-faked) politicians play Minecraft, and Keegan R. with what has to be the single most entertaining talk on box packing ever (I'm still a little in awe).\n\nThank you to Leo Riviera and Melissa Tranfield for hosting and organising, I'm sure you did Kimberley Cook proud!\n\n<div class=\"videoWrapper\"><iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/qkP29HDekdQ?si=duxAUvqg2lB7qIXF\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe></div>",
            "status": "published",
            "created_at": 1741564800000,
            "published_at": 1741564800000,
            "updated_at": 1741564800000,
            "author_id": 1
          },
          {
            "id": 12,
            "title": "Laptop Sticker Collection",
            "slug": "stickers",
            "markdown": "My friend [Leo](https://cowsay.io/) built this cool web app for annotating his laptop stickers to record where he got them. This tickled the sticker-goblin and developer advocate parts of my brain, so I nagged Leo until he shared the source code with me. He did so graciously on the condition I contribute to it. \n\nTry clicking on the stickers below to see where I got them.\n\n<div>\n  <iframe src=\"/projects/stickers/index.html\" width=\"100%\" height=\"500px\"></iframe>\n</div>\n*Go fullscreen [here](/projects/stickers/index.html).*\n\nYou can find the source code for this project in the [GitHub repo](https://github.com/leoriviera/stickers) (hassling Leo optional). You can also try creating your own sticker gallery [here](/projects/stickers/generate/index.html) on my site if you don't fancy getting stuck into the code just yet.\n\n---\n\nIn exchange for this delicious code I added a couple of features: a simple button to open the annotation generate tab from the main page and an option to replace your image with a new one when you get more stickers.\n\nI used Claude pretty heavily because I don't pretend for one minute to know about front-end development (I'm a data guy after all) so had to check the changes Claude suggested pretty closely and do some tweaking to get the new code to match the style of Leo's code.\n\nI also worked on adding some instructions to the README to help people create their own sticker galleries and to contribute to the project.\n\nMy next challenge was to create my own sticker gallery and upload it here. This was fairly simple but required some minor changes to the stickers project code and to my [Jekyll](https://jekyllrb.com/) configuration.\n\nI updated the `next.config.ts` file with the below config to change the export config to include the relative path to the location on my site I would be putting the export static code and to configure the static site to be exported to the `out` directory with `output: 'export'`.\n\n```typescript\nconst nextConfig: NextConfig = {\n    output: 'export',\n    trailingSlash: true,\n    images: {\n      unoptimized: true,\n    },\n    assetPrefix: '/projects/stickers',\n    // Add basePath for deployment in a subdirectory\n    basePath: '/projects/stickers', // Adjust this to where you'll host it in your Jekyll site\n};\n```\n\nBefore building, I used the project to build my own sticker gallery and took the exported `annotation.json` file and replaced the default one in the stickers project.\n\n![A GIF of annotating stickers with Leo's app](/content/images/new-annotation.gif)\n*Annotating stickers*\n\nI then built the project in the usual way with `npm run build`. I copied the contents of the `out` file into my Jekyll project and updated the `_config.yml` file with the below so that my Jekyll build would pick up the Next.js project files properly.\n\n```yaml\ninclude:\n  - _next\n  - \"**/_next/**\"\n```\n\nFinally, I could add my stickers gallery to this post with an iframe like so:\n\n```html\n<iframe src=\"/projects/stickers/index.html\" width=\"100%\" height=\"500px\"></iframe>\n```\n\n---\n\nBuilding my sticker gallery was super quick and fun thanks to this awesome tool from Leo. It was fun to contribute features and to help make the project more accessible for others to contribute too. I hope this blog inspires you to try making your own sticker gallery and to maybe get involved in trying your hand at contributing to this project.\n\nThis project is one of the first from [notanother.pizza](https://notanother.pizza) a community of community organisers and developer advocacy folks mostly based in London, you can check out our other projects [here](https://github.com/notanotherpizza).",
            "status": "published",
            "created_at": 1742860800000,
            "published_at": 1742860800000,
            "updated_at": 1742860800000,
            "author_id": 1
          },
          {
            "id": 13,
            "title": "Does AI actually make you more productive? Speed running my job with Cursor.",
            "slug": "speedrunning-my-job-with-cursor",
            "markdown": "![Speed running my job with cursor thumbnail](/content/images/cursor_thumbnail.png)\n\nLast week at [TurinTech](https://www.turintech.ai/) we had a workshop on agentic AI with an unusual challenge: complete a work task without writing any code manually; the goal was to see if we could boost our productivity using these tools. I'm fairly skeptical of using agentic tools at work - my concern was that I would spend as much (or even more) time vetting AI generated work than I would have doing them manually - even without considering that, could I really be productive with my hands tied behind my back?. I decided to try speedrunning a common task in my day to day work to see if I could really improve my productivity.\n\n## The Speedrun Setup\n\n### Cursor%\n\n* Use [Cursor](https://www.cursor.com/) exclusively to add an optimization example to our team's GitHub repo \n\n* Add a fully documented example to our optimization repo using only AI-generated code\n\n* No manual typing of code allowed, only prompting and editing AI suggestions\n\n### Any%\n\n* Add a fully documented example to our optimization repo manually using my normal approach\n\nI specifically chose the task of adding an example to this repo as it is both a task I have to do frequently and a task I thought it would be simple for an agentic tool to complete as it just involves copying data from one place to another and reformatting it as a basic report. It was my hope that the simplicity of the sample task would help avoid [an automation rabbit hole](https://xkcd.com/1319/) and give the agent the best possible chance to be genuinely useful.\n\nUsually once I complete an optmisation or someone shares a PR with an optimisation with I manually copy the template directory, write a short summary of the project, copy the data across, and where appropriate add a demo which you can use to show the difference in peformance before and after the optimisation. For my speedrun I would see if I could complete the same task as a tool assisted speedrun using only Cursor without interacting with anything other than the terminal and chat.\n\nThe specific example I was documenting in both cases was a DataStax Langflow Optimisation by Jordan Frazier ([see the PR here](https://github.com/langflow-ai/langflow/pull/7248)) who used our tool [Artemis](https://www.turintech.ai/artemis) to improve the performance of the Langflow Python library.\n\n\n<center>\n  <iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/PTAhBZ56238?si=zgdVzjayJzw8itYZ\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe>\n</center>\n<center>Writing examples manually takes 11 minutes and involves constant context-switching</center>\n\n<center>\n  <iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/YmoHR-LTTbg?si=bYEWRseweGPkdD_c\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe>\n</center>\n<center>Completing the same task in under 9 minutes without typing code</center>\n\n## My Experience with Cursor\n\nFor the challenge I would have to provide Cursor with the relevant information as context and then prompt it to create the example directory, this would have been easier if I had used [MCP](https://modelcontextprotocol.io/introduction), but I wasn't familiar with that approach yet, so instead I just copied the body of the PR into the chat. Cursor did an excellent job of parsing and formatting the data from the PR, completing them with only a handful of prompts. I was particularly happy with how easy cursor made it reformat data as Markdown tables, a task that can often be tedious.\n\n## Run Stats\n\nOnce I was done speed running the creation of this example both with and without Cursor I found that using Cursor enabled me to complete the task 20.59% faster. Initially I was surprised that it still took 79.41% of my usual time but reviewing the footage it became clear that once the time saved in the manual tasks likes updating tables (including a neat \"skip\" of removing the need for a separate table creation step entirely) and writing an overview was accounted for most of the remaining time spent adding the example was spent waiting for processes like zipping files, cloning repos, and adding LFS files which always take a fixed time. Even a modest 20% time saving though can be extremely valuable: the average salary for Solution Engineers in the UK is ¬£57,500/year, this means that if each task is completed 2 minutes and 19 seconds faster that represents approximately ¬£1.07 saved in my time per task. At ¬£15.40 monthly, Cursor Pro pays for itself after adding just 15 new examples.\n\n## When Cursor Gets Stuck: The AI Loop Run Killer\n\nIt's worth mentioning that during my first attempt at this run I tried to get Cursor to make a script to demo the optimisation, even after a few hours of trying, it was clear that it simply could not do this. Cursor's attempts at producing a demo script ranged between irrelevant to outright broken and even with a lot of feedback it was unable to produce anything usable, instead it kept getting stuck in loops of making and undoing the same changes. It's possible with a better approach I could have got better results here but either way I was forced to skip this step in both attempts in order to complete the challenge. \n\n## Lessons for Documentation Tasks\n\nThe appeal of AI-written documentation is significant, as I have previously written about before in [here](/how-to-use-AI-to-write-documentation-that-actually-works/) when I wrote about [Swimm](https://swimm.io/). Documentation tasks particularly benefit from AI assistance, as demonstrated in the videos, because they often involve repetitive formatting and data organization that AI can handle efficiently. In short automating the boring stuff frees you up to work on the interesting, challenging, or (whisper it) fun parts of a project that AI can't do.\n\nOne key takeaway from this run is the importance of having good examples for Cursor to draw on as context for inference.  By using the existing contents of a repo AI tools like Cursor could, hypothetically, help maintain consistency across a collaborative repository in this way, reducing discrepancies and ensuring a uniform style across documentation. This requires providing a lot of documentation as context though which can lead to issues like [loss of fluency](https://huggingface.co/blog/tomaarsen/attention-sinks#limitations-for-chat-assistant-llms:~:text=Loss%20of%20Fluency,assistant%3A%20assistant%3A) and running into context window limits. \n\nI mitigated these issues by providing a few high-quality, well-structured examples as context which ensures that Cursor can generate accurate and useful outputs - in this project I provided the template that contributors use to add examples to this project which helped guide Cursor to produce a response in the correct format. \n\n## The Productivity Verdict\n\nTo recap using Cursor did in fact give me the ability to complete 26% more examples in the same timeframe and potentially helped me write more consistent documentation across examples. Whilst I still can't replace all manual steps of this particular task with Cursor and still had to spend time thoroughly checking the output it did save me time, so I will likely use Cursor to help with adding future examples. Looking at the numbers, Cursor Pro clearly pays for itself after adding just 15 examples - making it a worthwhile investment purely from a productivity standpoint.\n\nWould you try the \"no manual coding\" challenge after seeing these results? The best speedrun routes are found as a result of sharing techniques. I'm curious if anyone else has found 'skips' or optimizations I missed in their own AI-assisted workflows.\n\n---\n\nDid you know you can use Artemis Intelligence to keep your documentation up to date? Find out more on the [TurinTech AI Developer page](https://www.turintech.ai/developer).",
            "status": "published",
            "created_at": 1743462000000,
            "published_at": 1743462000000,
            "updated_at": 1743462000000,
            "author_id": 1
          },
          {
            "id": 14,
            "title": "What do community organisers need? Not another pizza.",
            "slug": "not-another-pizza",
            "markdown": "![stack of pizza boxes](/content/images/pizza_stack.jpg)\n\n## Part 1 : If I never see another pizza again, it'll be too soon.\n\nThe year is 2022, I am a know-nothing twenty-something cloud consultant. [ChatGPT](https://openai.com/index/chatgpt/) has just been launched and the company I work for has created a brand-new practice with the business to focus on AI, and a colleague approaches me to see if I would be interested in helping to host the AI meetup the company has just adopted.\n\nNow three years later I'm still a know-nothing twenty-something, but now with a little more experience under my belt. I've organised 22 [AI and Deep Learning for Enterprise](https://aidle.uk) events, been a committee member for three conferences, and had a hand in helping to host and organise several other tech focused events in the capital. Participation in these communities has led me to all kinds of interesting places like speaking at events, learning new skills, running AV, and doing video editing, it‚Äôs even got me a job or two. Community organising has also left me more burnt out than I've ever been in my life.\n\nOrganising and hosting community events (particularly solo as I have done on occasion) is exhausting work. If you are putting on an event once a month, which for a while is what we were doing at AIDLE, the work is essentially unending. As soon as we finish hosting one event we immediately move on to planning the next (if we hadn't started already). Hosting can also be very physically demanding: moving furniture, running to grab stacks of beer and soft drinks, and lugging an AV kit on the tube on top of the late nights can leave you physically exhausted - particularly whilst also having to put on a friendly face and look after attendees and speakers.\n\nNeedless to say, I would not still be involved in organising events without a lot of support. Support from other organisers who've given advice, promoted and collaborated on events, from friends who have come along to help out running AV and taking pictures, and from sponsors and hosts who provide funding and venues. Co-organisers are perhaps the greatest source of support, as the difference in the feeling of doing something alone versus being part of a team cannot be overstated. Something I wish I had starting out was a way to find other organisers to collaborate with and share resources, I've met some great organisers over the last three years, but it took a long time.\n\n---\n\n![a swag table at a Grafana x Confluent event](/content/images/swag-table.jpeg)\n\n## Part 2: It was never about the Pizza (sorry)\n\nAfter the last three years of being in technical communities both as an organiser and as a regular member, I've eaten a LOT of pizza. I used to really like Pizza, but now not so much, and I know I'm not alone in this. [Not Another Pizza](https://notanother.pizza) is a fun little name for a group for community organisers, but (and you'll have to forgive me for this) the pizza isn't pizza, it's an analogy.\n\nPizza is a stand in for all the bare minimum requirements of hosting an event: the food yes, but also venues, AV, community event platforms ([on which I have _thoughts_](/meetup-is-terrible-now-codebar-festival-fringe/)), and so on. Of course community organisers need these things, but often as an organiser you'll find yourself reaching for something more.\n\nAs an organiser I've benefited greatly from meeting collaborators, being given advice on tracking community growth and retention, tools to help automate away the drudgery of managing guest lists, resources on managing codes of conduct and code of conduct violations, and maybe a sympathetic ear from someone from time to time. Think soul food, not cold pizza.\n\nSome of the biggest challenges I ran into early on in organising were burnout, managing relationships with sponsors, and finding speakers.\n\nI helped beat my organiser burn out by both cutting down on the frequency of events I was organising and attending, but also by avoiding solo efforts where possible. It's not about shifting the work on to other people but about working together as part of a team - helping other people tackle their obstacles as much as they help you tackle yours. For Developer advocates this may be a familiar feeling, being the glue between different people helps make you a very useful colleague but also means people are more willing to help you on when you need help. \n\nFinding speakers was made easier by emulating the best practices of other groups and organisers, for example: opening a call for papers so that speakers could apply to speak at AIDLE rather than having to constantly go out chasing them. This was a lesson I only learned because I was attending other peoples events, asking for advice, and shifting my perspective by becoming a speaker myself.\n\nSo, the vision for Not Another Pizza is a support network for dev advocates, community managers, and event organisers that can help provide some of these things. There are some great communities and resources out there already doing this like the [Developer Marketing Alliance](https://dev-mar-com.slack.com), but I've found that resources and collaboration opportunities can be very dependent on geography, so I wanted to set something up more focused on organising technical communities in London and the UK in general.\n\nThe London community landscape is highly concentrated around the City, with many smaller communities relying on access to free or subsidised event spaces rather than paying through the nose for renting event spaces. It feels like we are also rapidly approaching \"peak meetup\" with several events on the same topic on any given night, with 4 different AI and data meetups on the same night as the last AIDLE meetup alone. I don't see the large number of organisers in London as a problem though - we as organisers don't need to compete for limited resources and venues if instead organisers and groups collaborate to pool resources, shouldering the work of organising events together. London then is the perfect environment for building this kind of meta-community of community organisers.\n\nMy hope is that Not Another Pizza can foster genuine connections between organisers and serve to facilitate meaningful collaboration beyond being just another networking event.\n\n---\n\n![apache druid summit 2024](/content/images/apache-druid-summit.jpeg)\n\n## Part 3: Something better than another pizza\n\nIt's very early days yet, but some friends and I are working on building out our community by inviting other dev rel professionals and organisers to the [Discord community](https://discord.notanother.pizza) and building out the website. We've also been building out a catalogue of projects on the site that members of the group have been working on, including tools made by community members designed to help make managing communities easier.\n\nOne project we've recently started working on is [bridge](https://github.com/notanotherpizza/bridge), an API written in Python which will eventually enable the publishing and updating of events across multiple platforms simultaneously. I've previously spoken about how I think applying the [POSSE](https://indieweb.org/POSSE) (Post Own Site Syndicate Everywhere) content management strategy to community events could potentially help tackle the degradation of platforms like Meetup, and I hope bridge will help enable more organisers to experiment with that.\n\nGoing forward I'd love for us to create spaces for authentic collaboration by enabling organisers to share success stories of collaborations born through Not Another Pizza, providing match-making for mentorship opportunities, hosting a library of useful resources and content, and to become a really great place to get help with community organising. Perhaps in future we could even have some in person events, who knows. For now our focus will be on growing the community and continuing development on projects like bridge. I think we'll know if this community has been successful if we can see concrete examples of members meeting collaborators through Not Another Pizza and going on to run events.\n\nIf Not Another Pizza sounds like a useful resource you can join the discord community over at [discord.notanother.pizza](https://discord.notanother.pizza) to meet other organisers who can help your community thrive. Don't worry if you aren't based in London either, there are members in the group all over the world and some community organiser experiences really do feel universal at times, like wanting nothing less than another slice of cold pizza.",
            "status": "published",
            "created_at": 1744671600000,
            "published_at": 1744671600000,
            "updated_at": 1744671600000,
            "author_id": 1
          },
          {
            "id": 15,
            "title": "From Radio Waves to Kafka Topics - Building a Real-Time Aircraft Data Pipeline",
            "slug": "flight-radar-demo",
            "markdown": "![Flight radar talk photo collage](/content/images/flight-radar-banner.png)\n\nIf you want to showcase real-time data architectures you need a data source that's live, high-volume, varied, and messy enough to showcase real-world challenges. This is an issue I've run into several times over the last year whilst giving talks about real-time analytics using Kafka, Druid, ClickHouse, and Grafana in various combinations. You could use a data generator like [ShadowTraffic](https://shadowtraffic.io/) but when trying to bring the sometimes dry topic of data engineering to life nothing beats real data. So when I'm building demos I've consistently turned to the same compelling dataset: ADS-B aircraft transmissions.\n\nI was introduced to ADS-B (Automatic Dependent Surveillance‚ÄìBroadcast) by my former colleague at Imply [Hellmar Becker](https://blog.hellmar-becker.de/), and is one of the technologies aircraft use to relay data including their position, heading, and speed to air traffic controllers and to other aircraft. This creates a continuous stream of real-time data that's publicly accessible and rich with analytical possibilities. The dataset perfectly illustrates the complexities of streaming analytics‚Äîit arrives at high velocity, contains mixed data types, requires deduplication and enrichment, and benefits from both real-time alerting and historical analysis.\n\nWhat makes ADS-B particularly valuable for demonstrations is its combination of technical complexity and intuitive appeal. Everyone understands aircraft movement, making it easy to visualize concepts like windowing, aggregation, and anomaly detection. Yet underneath this accessibility lies genuine engineering challenges: handling bursty traffic patterns, dealing with incomplete or duplicate messages, and correlating position data with aircraft metadata.\n\nIn this article, I'll walk through building a complete ADS-B ingestion pipeline‚Äîfrom setting up a simple antenna to producing clean, structured data to Kafka topics ready for real-time analysis. By the end, you'll have both the technical foundation and a rich dataset to explore your own streaming analytics architectures.\n\n---\n\n## Understanding ADS-B Data\n\n![Flight radar 24 gif](/content/images/flight-radar.gif)\n\nADS-B transmissions use a standardized message format called SBS (BaseStation format), which arrives as comma-separated text lines. Each message contains different types of aircraft information, for example:\n\n**Position Messages (MSG,3)**: Location, altitude, and identification data\n```\nMSG,3,1,1,40756A,1,2025/06/01,17:42:30.733,2025/06/01,17:42:30.776,,35000,,,40.1234,-74.5678,,,0,0,0,0\n```\n\n**Velocity Messages (MSG,4)**: Speed, heading, and vertical rate\n```\nMSG,4,1,1,40756A,1,2025/06/01,17:42:31.233,2025/06/01,17:42:31.276,,,450,275,,,256,,,,,0\n```\n\nADS-B data has a high data velocity with anywhere from 100 to 2000 messages a second produced by a receiver depending on location. There are some problems with ADS-B data that present a barrier to real time analytics with this data: the data contains duplicate messages because the same aircraft can be tracked by multiple receivers (as many as 20-30% of messages will be duplicates), the are missing fields because not all messages contain complete information, and traffic varies by time of day and geographic location.\n\nThis real-world messiness makes ADS-B data perfect for demonstrating streaming analytics challenges like de-duplication, windowing, and real-time aggregation.\n\n---\n\n## Hardware Setup and Data Collection\n\nYou can be receiving live ADS-B data for around ¬£95 (or less, if you already have some of these parts or can pick them up second hand) and have it running in 15 minutes. Here's my exact setup that's been reliably collecting ADS-B data for months.\n\n### Hardware shopping list\n\n- [Raspberry Pi 3](https://amzn.to/43Fst45) (¬£35) - or any similarly capable computer, you can pick Raspberry Pi 3s now for as little as ¬£18 second hand\n- [NooElec NESDR Mini USB RTL-SDR](https://amzn.to/4kwR9mk) (¬£35) - An easy to use and readily available ADS-B antenna that can be connected to your Pi via USB. The included antenna gets you 50-100km range, which is perfect for getting started.\n- [Micro SD Card](https://amzn.to/43C0nXh) (¬£10) - These get cheaper all the time, here I've linked to a 128GB card that's relatively good value but really anything over 16GB will be fine as long as you can write to it properly, for more information on picking an appropriate SD card see [this great article by Chris Dzombak](https://www.dzombak.com/blog/2023/12/choosing-the-right-sd-card-for-your-pi/).\n- [Micro USB Power Supply](https://amzn.to/43C0nXh) (¬£15) - make sure to pick a reliable power supply that consistently delivers 5V/2.5A, I've linked to the official power supply here but almost any half decent Micro USB power supply will do.\n\n### Setup\n\n1) Install a supported OS on your Pi, I'm using a lite version (without a UI) of the official Debian Bookworm build, for details on how to do this follow the steps in the [guide on the Raspberry Pi website](https://www.raspberrypi.com/software/).\n\n2) Install Docker on your Pi and add your user to the docker group to run docker without sudo. **Important**: Log out and back in for group changes to take effect.\n\n```bash\ncurl -sSL https://get.docker.com | sh\nsudo usermod -aG docker pi\n# Log out and back in for group changes to take effect\n```\n\n3) Create a new Docker compose called `docker-compose.yml` and define an ultrafeeder services as below. Note: this is a very basic ultrafeeder configuration, you may wish to consult the [setup guide in the ADS-B Ultrafeeder repo](https://github.com/sdr-enthusiasts/docker-adsb-ultrafeeder?tab=readme-ov-file#minimalist-setup) for a more in depth guide to setting up this part.\n\n```yaml\nservices:\n  ultrafeeder:\n    image: ghcr.io/sdr-enthusiasts/docker-adsb-ultrafeeder\n    container_name: ultrafeeder\n    restart: unless-stopped\n    device_cgroup_rules:\n      - \"c 189:* rwm\"\n    ports:\n      - 8080:80     # Web interface\n      - 30003:30003 # SBS output (for Kafka)\n    environment:\n      - READSB_DEVICE_TYPE=rtlsdr\n      - READSB_RTLSDR_DEVICE=00000001  # Usually 00000001\n      - READSB_LAT=51.4074             # Your antenna latitude\n      - READSB_LON=-0.1278             # Your antenna longitude\n      - READSB_ALT=52                  # The altitude of your antenna\n    volumes:\n      - /dev/bus/usb:/dev/bus/usb\n```\n\n4) Deploy your ultrafeeder services:\n\n```bash\ndocker-compose up -d\n```\n\n5) **Optional: Add FlightRadar24 Integration**\n\n![Flight radar 24](/content/images/flight-radar.jpeg)\n\nAdding FR24 gives you two immediate benefits: a professional flight tracking interface and confirmation that your data quality meets commercial standards. Plus, contributing data gets you free access to FR24's premium features. Register [via the flight radar site](https://www.flightradar24.com/share-your-data) to get your sharing key, you should then be able to find your key in your Account Settings under \"My data sharing\".\n\nAdd the flight radar feed service to your docker compose to start sending data to FR24.\n\n```yaml\n# Add to existing services\n  fr24feed:\n    image: ghcr.io/sdr-enthusiasts/docker-flightradar24:latest\n    container_name: fr24feed\n    restart: always\n    ports:\n      - 8754:8754\n    environment:\n      - BEASTHOST=ultrafeeder\n      - FR24KEY={your flight radar 24 key}\n    dns_search: . # prevents rare connection issues related to a bug in docker and fr24feed\n```\n\nRedeploy with:\n\n```bash\ndocker-compose up -d\n```\n\nOnce setup your station should appear on their coverage map within 10-15 minutes.\n\n6) **Validate ADS-B Data Reception**\n\nTest that you are receiving ADS-B data correctly:\n\n```bash\nnc localhost 30003\n```\n\nYou should see continuous messages like:\n```\nMSG,8,1,1,40756A,1,2025/06/01,17:42:30.733,2025/06/01,17:42:30.776,,,,,,,,,,,,0\nMSG,3,1,1,40756A,1,2025/06/01,17:42:33.003,2025/06/01,17:42:33.015,,35000,,,40.1234,-74.5678,,,0,0,0,0\nMSG,4,1,1,40756A,1,2025/06/01,17:42:35.120,2025/06/01,17:42:35.156,,,450,275,,,256,,,,,0\n```\n\nIf your antenna has a good view of the sky you can expect around 100-2000 messages/second (depending on your location) with CPU usage sitting comfortably under 20% on a Pi 3.\n\n### Quick Troubleshooting\n\n**No aircraft?** Check your antenna USB connection:\n\n```bash\nlsusb | grep RTL\n```\n\nYou should see something like:\n```\nBus 001 Device 033: ID 0bda:2838 Realtek Semiconductor Corp. RTL2838 DVB-T\n```\n\nIf not, your antenna may not be connected correctly. Verify your antenna connection is secure or try different USB port (preferably USB 2.0+) and try restarting ultrafeeder:\n\n```bash\ndocker-compose down && docker-compose up -d\n```\n\n**Tracking very few aircraft?** Try placing your antenna higher and away from electronics, for best results try and get an unobstructed view of the sky.\n\n---\n\n## Kafka Integration\n\nNow that we have ADS-B data streaming on port 30003 let's produce it to Kafka to allow us to work with it as an event stream. We'll add Kafka to our Docker stack and build a producer that can handle thousands of aircraft updates per second.\n\n\n### Deploy Kafka\n\n1) First, let's extend your existing docker-compose.yml with Kafka services. Deploying Kafka alongside ultrafeeder on your Pi helps to keep networking simple but if you want to produce data from multiple receivers you may find it more practical to deploy Kafka elsewhere like on a secondary Pi or in a managed Kafka cluster in the Cloud ([check out this awesome guide by Robin Moffat on different configurations of Kafka and its associated producers and consumers](https://www.confluent.io/blog/kafka-client-cannot-connect-to-broker-on-aws-on-docker-etc)). Add the following to your services section:\n\n```yaml\n# Add to existing services\n  broker:\n    image: apache/kafka:latest\n    hostname: broker\n    container_name: broker\n    ports:\n      - 9092:9092     # Expose external SASL port\n    volumes:\n      - ./kafka-config/kafka_server_jaas.conf:/etc/kafka/kafka_server_jaas.conf\n    environment:\n      KAFKA_BROKER_ID: 1\n      # Map listener names to security protocols\n      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:SASL_PLAINTEXT,EXTERNAL:SASL_PLAINTEXT,CONTROLLER:PLAINTEXT\n      # Define what addresses to advertise to clients\n      KAFKA_ADVERTISED_LISTENERS: INTERNAL://broker:29092,EXTERNAL://localhost:9092\n      # Basic Kafka configs\n      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n      # KRaft mode settings\n      KAFKA_PROCESS_ROLES: broker,controller\n      KAFKA_NODE_ID: 1\n      KAFKA_CONTROLLER_QUORUM_VOTERS: 1@broker:29093\n      # Define interfaces and ports to listen on\n      KAFKA_LISTENERS: INTERNAL://0.0.0.0:29092,EXTERNAL://0.0.0.0:9092,CONTROLLER://broker:29093\n      # Internal communication between brokers (if you had multiple)\n      KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL\n      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER\n      KAFKA_LOG_DIRS: /tmp/kraft-combined-logs\n      CLUSTER_ID: MkU3OEVBNTcwNTJENDM2Qk\n      # SASL configuration\n      KAFKA_SASL_ENABLED_MECHANISMS: PLAIN\n      KAFKA_SASL_MECHANISM_INTER_BROKER_PROTOCOL: PLAIN\n      KAFKA_ALLOW_EVERYONE_IF_NO_ACL_FOUND: \"true\"\n      KAFKA_OPTS: \"-Djava.security.auth.login.config=/etc/kafka/kafka_server_jaas.conf\"\n```\n\nThe above configuration supports producers within the same docker network and consumers outside of the network to allow for flexibility adding services later on. \n\n```\n  # Map listener names to security protocols\n  KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:SASL_PLAINTEXT,EXTERNAL:SASL_PLAINTEXT,CONTROLLER:PLAINTEXT\n  # Define what addresses to advertise to clients\n  KAFKA_ADVERTISED_LISTENERS: INTERNAL://broker:29092,EXTERNAL://localhost:9092\n```\n\nAll the default are set to keep the Kafka broker as small as possible with no data replication.\n\n```\n  KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n  KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n  KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n  KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n```\n\nWe will also enable some basic auth. More on that in the next step.\n\n```\n  KAFKA_SASL_ENABLED_MECHANISMS: PLAIN\n  KAFKA_SASL_MECHANISM_INTER_BROKER_PROTOCOL: PLAIN\n  KAFKA_ALLOW_EVERYONE_IF_NO_ACL_FOUND: \"true\"\n  KAFKA_OPTS: \"-Djava.security.auth.login.config=/etc/kafka/kafka_server_jaas.conf\"\n```\n\n2) Before we deploy our Kafka cluster let's set up our basic authentication. Whilst the ADS-B data we are producing is publicly available some basic auth will help protect your topic from 3rd parties modifying your data should you choose to make your broker publicly accessible over the network, for example: if you want to support multiple receivers on your home network producing to your topic. Create the directory structure:\n\n```bash\nmkdir -p kafka-config\n```\n\n3) Create `kafka-config/kafka_server_jaas.conf` and configure a username and password (enter your own values instead of the defaults below!) both for server access and for client access:\n\n```\nKafkaServer {\n   org.apache.kafka.common.security.plain.PlainLoginModule required\n   username=\"admin\"\n   password=\"admin-secret\"\n   user_admin=\"admin-secret\"\n   user_producer=\"producer-secret\"\n   user_consumer=\"consumer-secret\";\n};\n\nKafkaClient {\n   org.apache.kafka.common.security.plain.PlainLoginModule required\n   username=\"admin\"\n   password=\"admin-secret\";\n};\n```\n\n4) Redeploy with:\n\n```bash\ndocker-compose up -d\n```\n\n### Creating the ADS-B Producer\n\nNext we'll build a producer that listens to the ADS-B data on port 30003 and produces it to a topic in our Kafka broker.\n\n![ADS-B data gif](/content/images/adsb-data.gif)\n\n1) Create the producer directory:\n\n```bash\nmkdir -p adsb-connector && cd adsb-connector\n```\n\n2) Download the connector script. This script will handle writing your ADS-B data to the `adsb-raw` topic in Kafka as well as some quality of life features like ensuring the topic exists before producing, handling logging, and parsing ADS-B data:\n\n```bash\ncurl -o connector.py https://raw.githubusercontent.com/hevansDev/olap-demo/main/receiver/adsb-connector/connector.py\n```\n\nBefore moving on, take a look at this section in particular, here we're handling building the actual structure of the Kafka message. If you wanted to write each of the fields in the ADS-B data to separate headers instead of as the raw message you could use something like `parts = message.strip().split(',')` to parse the CSV and then assign each part to a header like `'header': parts[index]`.\n\n```python\ndef send_message(producer, message):\n    \"\"\"Send a message to Kafka with proper error handling\"\"\"\n    \n    msg_data = {\n        'timestamp': datetime.datetime.now().isoformat(),\n        'raw_message': message\n    }\n```\n\n3) Create .env file in the adsb-connector directory to store your producer username and password and Kafka broker details (again update the placeholder values with your username and password from the previous section):\n\n```bash\ncat > .env << EOF\nSBS_HOST=ultrafeeder\nSBS_PORT=30003\nKAFKA_BROKER=broker:29092\nKAFKA_PRODUCER_USERNAME=producer\nKAFKA_PRODUCER_PASSWORD=producer-secret\nTOPIC_NAME=adsb-raw\nEOF\n```\n\n4) Return to your main directory:\n\n```bash\ncd ..\n```\n\n5) Add the producer service to your docker-compose.yml:\n\n```yaml\n# Add to existing services\n  adsb-kafka-connector:\n    image: python:3.9\n    container_name: adsb-kafka-connector\n    restart: unless-stopped\n    depends_on:\n      - broker\n      - ultrafeeder\n    volumes:\n      - ./adsb-connector:/app\n    working_dir: /app\n    environment:\n      - KAFKA_BROKER=broker:29092\n      - KAFKA_PRODUCER_USERNAME=producer\n      - KAFKA_PRODUCER_PASSWORD=producer-secret\n      - TOPIC_NAME=adsb-raw\n    command: sh -c \"apt-get update && apt-get install -y librdkafka-dev && pip install confluent-kafka python-dotenv && python connector.py\"\n```\n\n6) Deploy your new producer and check all the services are running:\n\n```bash\ndocker-compose up -d\ndocker ps\n```\n\nExpected output:\n```\nCONTAINER ID   IMAGE                                          COMMAND                  CREATED         STATUS         PORTS                                                  NAMES\na1b2c3d4e5f6   ghcr.io/sdr-enthusiasts/docker-adsb-ultrafeeder   \"/init\"                  2 minutes ago   Up 2 minutes   0.0.0.0:8080->80/tcp, 0.0.0.0:30003->30003/tcp       ultrafeeder\nb2c3d4e5f6a7   ghcr.io/sdr-enthusiasts/docker-flightradar24      \"/init\"                  2 minutes ago   Up 2 minutes   0.0.0.0:8754->8754/tcp                                fr24feed\nc3d4e5f6a7b8   apache/kafka:latest                                \"/etc/confluent/dock‚Ä¶\"   1 minute ago    Up 1 minute    0.0.0.0:9092->9092/tcp                                broker\nd4e5f6a7b8c9   python:3.9                                         \"sh -c 'apt-get upd‚Ä¶\"   1 minute ago    Up 1 minute                                                           adsb-kafka-connector\n```\n\n7) **Verify Kafka Integration**\n\nConnect to the broker container and verify the setup:\n\n```bash\ndocker exec -it broker bash\n```\n\nCreate consumer config with credentials:\n\n```bash\ncat > consumer.properties << EOF\nsecurity.protocol=SASL_PLAINTEXT\nsasl.mechanism=PLAIN\nsasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username=\"consumer\" password=\"consumer-secret\";\nEOF\n```\n\nTest consuming messages:\n\n```bash\nkafka-console-consumer \\\n  --bootstrap-server localhost:29092 \\\n  --topic adsb-raw \\\n  --from-beginning \\\n  --consumer.config /etc/kafka/consumer.properties\n```\n\nYou should see JSON messages like:\n```json\n{\"timestamp\": \"2025-06-01T17:42:30.733Z\", \"raw_message\": \"MSG,3,1,1,40756A,1,2025/06/01,17:42:30.733,2025/06/01,17:42:30.776,,35000,,,40.1234,-74.5678,,,0,0,0,0\"}\n{\"timestamp\": \"2025-06-01T17:42:31.233Z\", \"raw_message\": \"MSG,4,1,1,40756A,1,2025/06/01,17:42:31.233,2025/06/01,17:42:31.276,,,450,275,,,256,,,,,0\"}\n```\n\nPress `Ctrl+C` to exit the consumer and `exit` to leave the broker container.\n\n### Quick Troubleshooting\n\nNot seeing any messages in Kafka? Triple check the values of your secrets both in your `.env` file and in `consumer.properties`, these must match for either the initial production or the test consumer to work.\n\n#### No aircraft detected?\n\nCheck your antenna USB connection:\n\n```bash\nlsusb | grep RTL\n```\n\nExpected output:\n\n```bash\nBus 001 Device 033: ID 0bda:2838 Realtek Semiconductor Corp. RTL2838 DVB-T\n```\n\nIf the device isn't showing up, verify the USB connection is secure, try a different USB port (preferably USB 2.0+), and restart ultrafeeder with:\n\n```bash\ndocker-compose down && docker-compose up -d\n```\n\nDevice detected but ultrafeeder shows \"no supported devices found\"?\nKernel 6.12+ has known issues with RTL-SDR USB support. Conflicting DVB-TV drivers claim the device before RTL-SDR drivers can access it.\nSolution: Blacklist the DVB drivers\nCreate a blacklist configuration file:\n\n```bash\nsudo nano /etc/modprobe.d/blacklist-rtl.conf\n```\n\nAdd these lines:\n\n```bash\nblacklist dvb_usb_rtl28xxu\nblacklist rtl2832\nblacklist rtl2830\n```\n\nSave the file (Ctrl+X, then Y, then Enter) and reboot the Pi:\n\n```bash\nsudo reboot\n```\n\nVerify the fix and test the RTL-SDR directly with:\n\n```bash\nrtl_test\n```\n\nExpected output:\n\n```bash\nFound 1 device(s):\n  0:  Realtek, RTL2838UHIDIR, SN: 00000001\n\nUsing device 0: Generic RTL2832U OEM\nFound Rafael Micro R820T tuner\n```\n\nPress Ctrl+C to stop the test.\nCheck the ultrafeeder logs:\n\n```bash\ndocker logs --tail 20 ultrafeeder\n```\n\nThey should contain:\n\n```bash\nrtlsdr: using device #0: Generic RTL2832U OEM (Realtek, RTL2838UHIDIR, SN 00000001)\nFound Rafael Micro R820T tuner\n```\n\n---\n\n## Conclusion and Next Steps\n\nYour pipeline now processes ADS-B data through several stages: ultrafeeder receives radio signals and outputs ADS-B data, our adsb-kafka-connector service parses messages into structured JSON, and the Kafka broker stores and distributes the structured aircraft data.\n\nEach message in Kafka now looks like:\n\n```json\n{\n  \"timestamp\": \"2025-06-01T17:42:30.733\",\n  \"raw_message\": \"MSG,3,1,1,40756A,1,...\"\n}\n```\n\nIn building this pipeline you should now have learned the basics of deploying Apache Kafka and producing data to it that I hope will be useful to you in your future projects, if you build anything with this data or with this project please let me know! I'd love to see what you get up to.\n\nFor the full code used to build this project check out the [project on my GitHub](https://github.com/hevansDev/olap-demo/tree/main/receiver). It is worth noting that whilst this project is a good place to get started with Kafka it is not yet production ready, further refinement is needed to make this project properly secure (like removing plaintext secrets in favour of proper secrets management).\n\n![Sample real time architecture for analytics](/content/images/flight-radar-architecture.jpg)\n\nThis structured ADS-B data is now ready for real-time analytics, I plan on covering how you can do this with ClickHouse and Grafana in a future article. I've spoken before about analysing ADS-B with Apache Druid and Grafana at the Aerospike Barcelona Data Management Community Meetup and [you can find a recording of my talk here](/aerospike-barcelona-data-management-community-meetup/).\n\n## Further Reading\n\n[The 1090 Megahertz Riddle (second edition)](https://mode-s.org/1090mhz/content/ads-b/1-basics.html) A Guide to Decoding Mode S and ADS-B Signals by [Junzi Sun](junzis.com)\n\n[My Python/Java/Spring/Go/Whatever Client Won‚Äôt Connect to My Apache Kafka Cluster in Docker/AWS/My Brother‚Äôs Laptop. Please Help!](https://www.confluent.io/blog/kafka-client-cannot-connect-to-broker-on-aws-on-docker-etc) A really helpful guide to troubleshooting Kafka connectivity issues by [Robin Moffatt](https://www.confluent.io/blog/author/robin-moffatt/)",
            "status": "published",
            "created_at": 1748473200000,
            "published_at": 1748473200000,
            "updated_at": 1748473200000,
            "author_id": 1
          },
          {
            "id": 16,
            "title": "Getting Started with Diskless Kafka - A Beginner's Guide",
            "slug": "beginners-guide-diskless-apache-kafka-kip-1150",
            "markdown": "*Diskless topics are proposed in KIP-1150, which is currently under community review. The examples in this article use \"Inkless\", Aiven's implementation of KIP-1150 that lets you run it in production.*\n\n**I joined Aiven as a Developer Advocate in May, shortly after the [Kafka Improvement Proposal](https://cwiki.apache.org/confluence/display/kafka/kafka+improvement+proposals) [KIP-1150: Diskless Topics](https://cwiki.apache.org/confluence/display/KAFKA/KIP-1150%3A+Diskless+Topics) was announced,** which reduces the total cost of ownership of Kafka by up to 80%\\! It was very exciting to join [Aiven](https://aiven.io) just as the streaming team were making this major contribution to open source but I wanted to take my time to understand the KIP before sharing my thoughts.\n\nIn this article I‚Äôll share my first impressions of Diskless Kafka, walk you through a simple example you can use to experiment with Diskless, and highlight some of the great resources that are out there for learning about the topic. **First though, what actually is *Diskless Kafka*?**\n\n## What is Classic Kafka?\n\nTo understand Diskless Kafka, you first need to understand how Apache Kafka¬Æ works today. Kafka data is stored and replicated across multiple broker servers using local disks. A designated leader broker handles all writes to a given partition, while follower brokers maintain copies of the data. To ensure high availability Kafka clusters are often deployed with cross-zone replication, where data is duplicated across different cloud availability zones, but this creates a significant cost problem. Up to 80% of Kafka's total cost of ownership comes from expensive cross-zone network traffic, with cloud providers like AWS charging per GB for data transfer between zones.\n\n![a diagram showing the flow of messages in a clasic kafka cloud deployment with the charges at az boundaries marked](/content/images/classic-kafka-diagram.png)\n\n## What is Diskless Kafka?\n\nDiskless Kafka fundamentally reimagines this architecture by delegating replication directly to object storage services like Amazon S3, eliminating the need for cross-zone disk replication entirely. Instead of storing user data on local broker disks, diskless topics write data straight to object storage, adopting a leaderless design where any broker can handle any partition. This is as opposed to tiered storage, which still relies on local disk replication for recent data before moving older segments to object storage.\n\n![a diagram showing the flow of messages in diskless kafka avoiding cross az charges by using object storage](/content/images/diskless-kafka-diagram.png)\n\nThe trade-off of Diskless is that reads and writes from object storage are slower than those from local disk, to mitigate this KIP-1150 has been engineered such that you can run both traditional low-latency topics (sub-100ms) and cost-optimized diskless topics (200-400ms) in the same cluster, allowing you to choose the right performance profile for each workload. KIP-1150 maintains all existing Kafka APIs and client compatibility. Many use cases tolerate higher latency that Diskless topics enable such as logging and are thus a natural fit, but some use cases like high frequency trading or gaming are latency critical.\n\n![a chart comparing the latency of classic kafka to diskless kafka](/content/images/diskless-latency-sketch-chart.png)\n\nAnother snag with \"Diskless\" Kafka is that the name is somewhat of a misnomer. While ‚ÄúDiskless‚Äù implies complete elimination of disk usage, brokers still require local disks for Kafka metadata, batch coordination, temporary operations like compaction, and optional caching. The term \"Diskless\" specifically refers to topic data storage for Diskless topics \\- user messages and logs that traditionally consume the vast majority of disk space and I/O resources. Therefore it‚Äôs more accurate to describe the changes in KIP-1150 as adding *Diskless Topics* within classic Kafka than creating a new ‚Äú*Diskless Kafka‚Äù*.\n\n***TL;DR: Naming things is hard. Speaking of naming things \\-*** \n\n## What is *Inkless Kafka*?\n\nInkless is the name the team behind KIP-1150 gave to the temporary GitHub repository that contains the implementation KIP-1150, so you can use Diskless Kafka before it is merged into the Apache Kafka main branch. You can find the [Inkless repo here](https://github.com/aiven/inkless).\n\n![a screenshot of the aiven inkless repo](/content/images/aiven-inkless.png)\n\n### Run Diskless Kafka Locally with Inkless and MinIO\n\nWhen I first got hands on with Diskless I wanted to experiment with running it locally to see what made it tick. In order to run Inkless locally we also require object storage, I decided to use [MinIO](https://min.io/), a performant object store that you can deploy locally with a docker container. You can try running Diskless Kafka yourself by following the steps below:\n\n- First, ensure you have Docker installed locally ([see the Docker documentation for detailed instructions on how to do this](https://docs.docker.com/get-started/get-docker/)).  \n- Clone [my GitHub repo](https://github.com/Aiven-Labs/diskless-docker-quickstart) with the example Docker Compose and navigate into the local directory.\n\n```shell\ngit clone https://github.com/Aiven-Labs/diskless-docker-quickstart.git\ncd diskless-docker-quickstart\n```\n\n- Start the Kafka and MinIO services by running the docker compose and check the status of the newly created containers with the following:\n\n```shell\ndocker compose up -d\ndocker compose ps\n```\n\n- You should see a MinIO service created and a temporary service which creates a bucket in MinIo called `kafka-diskless-data` which our Diskless broker will use to store our Kafka data. Our Kafka configuration remains pretty much the same as a bog standard Kafka deployment apart from the inclusion of the following environment variables which specify that the broker should use S3 for the storage backend as well as the details required to access our bucket. Interestingly the S3 specification requires a region so even though the bucket is on our local machine we still need to set the region to `us-east-1` as we‚Äôre using the S3 spec to write to our local bucket in this example.\n\n```shell\n     # Inkless Storage Configuration\n      - KAFKA_INKLESS_STORAGE_BACKEND_CLASS=io.aiven.inkless.storage_backend.s3.S3Storage\n      - KAFKA_INKLESS_STORAGE_S3_PATH_STYLE_ACCESS_ENABLED=true\n      - KAFKA_INKLESS_STORAGE_S3_BUCKET_NAME=kafka-diskless-data\n      - KAFKA_INKLESS_STORAGE_S3_REGION=us-east-1\n      - KAFKA_INKLESS_STORAGE_S3_ENDPOINT_URL=http://minio:9000\n      - KAFKA_INKLESS_STORAGE_AWS_ACCESS_KEY_ID=minioadmin\n      - KAFKA_INKLESS_STORAGE_AWS_SECRET_ACCESS_KEY=minioadmin\n```\n\n\n- Create a new diskless topic with the following:\n\n```shell\ndocker compose exec kafka /opt/kafka/bin/kafka-topics.sh \\\n--create --topic test-diskless \\\n--bootstrap-server localhost:9092 \\\n--config inkless.enable=true\n```\n\n- You can now produce and consume from your Diskless topic using the standard Kafka CLI tools. As I mentioned previously this is because whilst KIP-1150 adds new functionality under the hood the Kafka API remains unchanged so we don‚Äôt have to change any of our ways of producing and consuming from Kafka. \n\n```shell\necho \"This should go to MinIO\" | docker compose exec -T kafka \\\n/opt/kafka/bin/kafka-console-producer.sh \\\n--topic test-diskless \\\n--bootstrap-server localhost:9092\n\ndocker compose exec kafka /opt/kafka/bin/kafka-console-consumer.sh \\\n  --topic test-diskless --bootstrap-server localhost:9092 --from-beginning\n```\n\n- It's worth noting that other companies (like the first implementation from WarpStream) have created their own 'Diskless' streaming solutions but these are completely separate platforms that replace Kafka entirely. What makes KIP-1150 exciting is that it extends the existing Apache Kafka codebase by adding Diskless topics as a new option, rather than forcing you to migrate to a different streaming platform. This means you can gradually adopt Diskless topics for specific workloads while keeping your existing Kafka infrastructure and expertise.\n\n- You should now be able to see the data that are written to your MinIO bucket by visiting [http://localhost:9001](http://localhost:9001) (username and password are both `minioadmin` in this example). \n\n![A screenshot of partions written to local object storage from Kafka in the MinIO webconsole](/content/images/local-object-storage-segments-screenshot.png)\n\t\n- You may notice that in this example one message becomes one object in the bucket which is an inefficient way of writing to storage, at higher volumes Diskless Kafka employs batching to more efficiently write messages as objects.\n\nYou can find more Docker based demos on the Inkless repo [here](https://github.com/aiven/inkless/tree/main/docker/examples/docker-compose-files) that demonstrate how to use the Diskless clusters and allow you to make comparisons between regular and Diskless topics.\n\n### Troubleshooting\n\nCheck the Kafka container status and logs with:\n\n```shell\ndocker compose ps\ndocker compose logs kafka\n```\n\nIf you see reference to the control plane being provisioned in memory it‚Äôs likely your environment variables configuring your object storage are not being picked up \\- double check your variable names are correct and that your docker compose is formatted correctly.\n\n##  What does Diskless mean for Kafka?\n\nIt‚Äôs exciting to see the new use cases the KIP enables and it‚Äôs clear that Diskless will benefit a variety of Kafka users. I‚Äôm already using Diskless Kafka in updated versions of my flight data demo. ([which you can check out here](https://hughevans.dev/flight-radar-demo/)), to backup data from my receivers to S3, saving my data from SD card failure on my Raspberry Pis.\n\nI'm still exploring the space, and in future and I look forward to exploring both potential cost savings and the new use cases Diskless enables now. For example: could it be possible to leave data in Kafka to enable operational use cases without having to ingest data into a database first?For analytical use cases in future there are also exciting possibilities \\- would it someday be possible to write data directly from Kafka into an object store like S3 in a format like Apache Iceberg?\n\nDiskless Kafka represents a fundamental shift in how we think about streaming data storage. By moving replication to object storage, it promises to slash total cost of ownership while enabling new use cases that were previously too expensive to consider. The slight latency trade-off (200-400ms vs sub-100ms) is a small price to pay for most workloads outside of high-frequency trading or gaming. If you're running Kafka in the cloud and dealing with high cross-zone replication costs, Diskless topics could reduce your streaming infrastructure costs significantly.\n\nReady to help shape the future of Kafka? Join the conversation on the dev@kafka.apache.org mailing list and add your voice to the KIP-1150 discussion. Every \"+1\" vote and thoughtful feedback helps determine whether this innovation becomes part of Apache Kafka's core. The streaming future is being written now, make sure you're part of the conversation\\!\n\n![Aiven crab's lining up to vote](/content/images/get-out-the-crab-vote.png)\n\nLearn more about Diskless Kafka with these awesome resources:\n\n* [Diskless - The Cloud-Native Evolution of Kafka](https://www.geeknarrator.com/blog/diskless-warpstream-confluentfreight) by [Kaivalya Apte](https://x.com/thegeeknarrator)\n* [The Hitchhiker‚Äôs guide to Diskless Kafka](https://aiven.io/blog/guide-diskless-apache-kafka-kip-1150) by Filip Yonov & Josep Prat\n* [Diskless Kafka: 80% Leaner, 100% Open](https://aiven.io/blog/diskless-apache-kafka-kip-1150) by Filip Yonov\n* Read  [KIP-1150](https://cwiki.apache.org/confluence/display/KAFKA/KIP-1150%3A+Diskless+Topics) itself!\n\n---\n\n**Want to explore Diskless Kafka in production?**\n\nJoin us for \"Get Kafka-Nated Episode One\" where we'll dive into Apache Kafka's evolution from LinkedIn's internal tool to modern streaming infrastructure. We'll cover major architectural shifts including KIP-500, KIP-1150 (Diskless Topics), enterprise adoption patterns, and what's driving the next phase of streaming with industry experts who've been there since the beginning.\n\nPlus every attendee gets a free coffee voucher\\! [Click here to register.](https://www.linkedin.com/events/getkafka-nated-episode1-apachek7338917180241047555/theater/)\n\n*Brought to you by Aiven, KIP-1150 contributors.*",
            "status": "published",
            "created_at": 1750287600000,
            "published_at": 1750287600000,
            "updated_at": 1750287600000,
            "author_id": 1
          },
          {
            "id": 17,
            "title": "Get Kafka-Nated Podcast (Episode 1) Apache Kafka¬Æ's Evolution - 14 Yrs of Streaming",
            "slug": "get-kafka-nated-ep-1",
            "markdown": "Check out the first episode of Get Kafka - Nated! Filip Yonov and I had a great chat exploring everything from Kafka's journey from on-prem to the cloud, this years major Kafka improvement proposals, to what we're excited about for the future of Kafka.\n\nTune in next time for a conversation with Josep Prat about life as a Kafka contributor.\n\n<div class=\"videoWrapper\"><iframe src=\"https://www.youtube.com/embed/XZMfyEuPvqg?si=rlrC1AP9msp1YY97\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" style=\"display:block;\" allowfullscreen></iframe></div>\n\n[Original release](https://www.linkedin.com/events/getkafka-nated-episode1-apachek7338917180241047555/theater/)",
            "status": "published",
            "created_at": 1750892400000,
            "published_at": 1750892400000,
            "updated_at": 1750892400000,
            "author_id": 1
          },
          {
            "id": 18,
            "title": "Smart Bird Feeder Part 1 - How can I weigh things with a Raspberry Pi? - Using a HX711 ADC and load cell with a Raspberry Pi",
            "slug": "load-cell-raspberry-pi",
            "markdown": "![An image of robins eating bird seed off a wii fit balance board with the caption meanwhile in suburban south london](/content/images/bird_seo.jpg)\n\nThere is a very large robin that often visits the bird feeder on my office window. It's clear this robin is much heavier than other robins because when he lands the impact makes a loud *thwack* sound. I decided to see if I could build a simple setup to figure out exactly how heavy this robin is and in predictable fashion got carried away - this will be the first article in a three part series exploring: building a smart bird feeder than can weigh visiting birds, using AI to identify birds automatically, and bringing it all together with Kafka and Iceberg. \n\nIn order to get the weight of birds on my bird feeder I would need to add a load cell to the feeder platform. Whenever I'm building something like this I tend to start with a Raspberry Pi as that's what I'm most familiar with, there's a lot of great guides online on how to use Arduinos and other micro controllers with load cells and amplifiers but there isn't a huge amount out there on Raspberry Pis other than [this great tutorial from Tutorials for Raspberry Pi](https://tutorials-raspberrypi.com/digital-raspberry-pi-scale-weight-sensor-hx711) from several years ago. I was able to get a working setup with a cheap 5kg rated load cell and HX711 ADC as explained in the tutorial but I encountered few snags along the way so I thought in addition to documenting my bird feeder project I would write and updated version of the Tutorials for Raspberry Pi guide to help anyone else looking to work with load cells and the Raspberry Pi.\n\nThe below guide will talk you through step by step everything you need to do to weigh an object up to 5kg in weight with a Raspberry Pi including selecting components, assembly, and calibration.\n\n---\n\n## What is an HX711?\n\nFirst though, why do we need an HX711 at all? Load cells convert forces applied to them into analog electrical signals via strain gauges (resistors that change their resistance when bent or stretched) that we can use to measure weight but these signals are both analog and too small to be detected by the Raspberry Pis GPIO (General Purpose Input Output) pins. The HX711 is an ADC (Analog to Digital Convertor) which takes the weak analog signal from the load cell and outputs a digital signal (as a 24bit integer) the Raspberry Pi can read.\n\n![HX711 converts analog signals to digital signals](/content/images/hx711-signal.png)\n\n---\n\n## Hardware Setup\n\nSetting up your HX711 will require some soldering, don't worry if you've not done soldering before this is a particularly simple soldering job (even I could do it!) If you follow the method I used you'll need to cut and drill the some parts to install your load cell - if you'd rather not do this you can buy a load cell and HX711 kit with these parts pre-made, for example [this kit with laser cut wooden sheets with mounting holes](https://amzn.to/4kqA6BJ). If you already have a soldering iron all the parts for this project new should set you back no more than ¬£85 but you could save a fair bit if you pick up the Raspberry Pi second hand (or already have one laying around) and scavenge your bolts and rigid sheets rather than buying them new.\n\n### Hardware shopping list\n\n- [Raspberry Pi 3](https://amzn.to/43Fst45) (¬£35) - or any similarly capable computer with soldered headers for GPIO, you can pick Raspberry Pi 3s now for as little as ¬£10 second hand\n- [Load cells and HX711s](https://amzn.to/44PCFbR) (¬£8) - a pair of load cells and HX711s, these are cheap as chips and it's nice to have a spare in case you get over enthusiastic tightening bolts or testing later on.\n- [Bolts and Washers](https://amzn.to/4lDKHtU) (¬£8) - the load cell I picked had 4 holes for mounting, two threaded for M2 and two threaded for M3 bolts - this selection of bolts should have everything you need including washers which are useful for making sure the strain gauges don't get pinched. I scavenged the bolts and washers I needed from some other projects.\n- [Micro SD Card](https://amzn.to/43C0nXh) (¬£10) - These get cheaper all the time, here I've linked to a 128GB card that's relatively good value but really anything over 16GB will be fine as long as you can write to it properly, for more information on picking an appropriate SD card see [this great article by Chris Dzombak](https://www.dzombak.com/blog/2023/12/choosing-the-right-sd-card-for-your-pi/).\n- [Micro USB Power Supply](https://amzn.to/43C0nXh) (¬£15) - make sure to pick a reliable power supply that consistently delivers 5V/2.5A, I've linked to the official power supply here but almost any half decent Micro USB power supply will do.\n- [Acrylic](https://amzn.to/3TrxKaJ) / [Plywood-wood](https://amzn.to/4nBnRoE) sheets approximately 100x200mm (¬£6)- these will be used to mount your load cell and hold whatever it is you're weighing so anything cheap and rigid will do, I used some old plexiglass offcuts but I suspect even sturdy cardboard would work fine.\n- [Female to female Dupont cables](https://amzn.to/4nx52mJ) (¬£4) - you only need 4 of these to connect your HX711 to the headers on your Raspberry Pi but its worth buying them in a big pack like the one linked as it's much cheaper than only buying 4 and dupont cables are always handy for projects like this.\n- Optional: [Precision weights](https://amzn.to/3TsgiTv) (¬£12) - Can be used to accurately calibrate your load cell but any object that you know the exact weight of will do, a reasonably accurate kitchen scale can come in handy here.\n\n### Tools\n\nEssential:\n- Soldering iron\n- Solder\n- Scissors or snips\n\nHandy for shaping your rigid sheets and making mounting holes:\n- Dremel\n- Coping saw\n- File or sanding paper\n- Drill\n\n### Setup\n\n1) Cut your sheets to size and drill two holes in each sheet to attach the load cell and bolt the load cell into place. Your sheets should look something like the diagram below with the holes for mounting the top sheet roughly centered and the hole for mounting the base towards the edge:\n\n![A rough schematic of what your rigid sheets should look like with mounting holes](/content/images/rigid-sheets.png)\n\nThe positioning of the holes is important! We want one end of the load cell to be centered roughly on the middle sheet so the arrow on the end is oriented correctly.\n\n2) Bolt the load cell sandwiched between both rigid sheets as in the diagram below. You may need to add some washers between the load cell and the rigid sheets to stop the strain gauges in the white blob around the middle from getting pinched when weight is added to the top sheet - only the \nmounting surfaces of the load cell should make contact with the rigid sheets.\n\n![Diagram showing how the scale should be assembled with the load cell sandwiched between both rigid plates, the end with the arrow in the center pointed down, and washers between the load cell and rigid sheets](/content/images/assemble-scale.png)\n\nIf everything is assembled correctly each of the rigid sheets should be parallel to the load cell, if things are askew or the rigid sheets are resting on the epoxy in the middle of the load cell which covers the strain gauges try adding more washers between the load cell and the rigid sheet to free things up.\n\n3) Solder the leads from the load cell to the correct pads on the HX711 as follows: Red to E+, Black to E-, Green to A-, and White to A+ (the pins labeled B+/B- remain empty).\n\n![A schematic showing a red wire connected to E+, a black wire connected to E-, a green wire to A_, and a white wire to A+ on the terminal of a HX711](/content/images/solder-hx711.png)\n\n4) Cut off a 4 pin long strip of the included headers, press them short end first into the holes in the board marked GND, DT, SCK, and VCC and solder them from the reverse of the board. This can be fiddly! I usually use a big blob of Blu Tack to hold my headers in place when soldering them but anything that can hold the headers square (i.e. a second pair of hands!) can be really helpful here.\n\n![A diagram showing how to correctly orient the headers with the black part and long pins on the top of the board and the short pins poking through the holes](/content/images/solder-headers-hx711.png)\n\n5) Tear off a strip of four female to female dupont wires, (keeping the four stuck together can help keep your wiring tidy but it can help to tease the ends apart a bit to make it easier to plug them into your headers) and use them to connect the headers on the HX711 to the headers on your Raspberry Pi as follows: VCC to Raspberry Pi Pin 2 (5V),GND to Raspberry Pi Pin 6 (GND), DT to Raspberry Pi Pin 29 (GPIO 5), and SCK to Raspberry Pi Pin 31 (GPIO 6). The pin out of your Raspberry Pi may vary slightly depending on model, for reference check out this awesome resource over on [pinout.xyz](https://pinout.xyz/).\n\n![A diagram showing how to correctly connect hx711 to the raspberry pi with dupont connectors](/content/images/connect-hx711-to-pi.png)\n\n5) Flash your SD card and setup your Raspberry Pi. For instructions on how to do this properly check out [this guide on the Raspberry Pi website](https://www.raspberrypi.com/documentation/computers/getting-started.html).\n\n6) Get the library we need to control the HX711 with Python and navigate into the directory:\n\n``` bash\ngit clone https://github.com/tatobari/hx711py\ncd hx711py\n```\n\n7) Finally, we're ready to calibrate the load cell. Create a script called `calibration.py` with the following code and run it:\n\n```\nimport time\nimport RPi.GPIO as GPIO\nfrom hx711 import HX711\n\n# Setup HX711\nhx = HX711(5, 6)\nhx.set_reading_format(\"MSB\", \"MSB\")\nhx.set_reference_unit(1)\nhx.reset()\nhx.tare()\n\n# Configuration\nnum_samples = 15\n\nprint(f\"Place known weight on scale and enter it's weight in grams:\",end=\"\")\nknown_weight = int(input())\n\n# Collect samples\nprint(\"Collecting samples...\")\nsamples = []\nfor i in range(num_samples):\n    reading = hx.get_weight(1)\n    samples.append(reading)\n    print(f\"{i+1}: {reading}\")\n    time.sleep(0.2)\n\n# Remove outliers (simple method: remove top and bottom 20%)\nsamples.sort()\nclean_samples = samples[3:-3]  # Remove 3 highest and 3 lowest\n\n# Calculate reference unit\naverage = sum(clean_samples) / len(clean_samples)\nreference_unit = average / known_weight\n\nprint(f\"\\nAverage reading: {average:.1f}\")\nprint(f\"Reference unit: {reference_unit:.2f}\")\nprint(f\"\\nAdd this to your script:\")\nprint(f\"hx.set_reference_unit({reference_unit:.2f})\")\n\nGPIO.cleanup()\n```\n\nWhen prompted add one of your calibration weight or your known weight to the top of your scale and enter the weight in grams in the script and hit enter:\n\n```\nPlace known weight on scale and enter it's weight in grams:50\n```\n\nKeep a note of the reference unit, calculated as `referenceUnit = longValueWithOffset / known_weight` where longValueWithOffset is the 24bit integer reading from the HX711 minus the tare value.\n\n```\nAverage reading: 20873.4\nReference unit: 417.47\n\nAdd this to your script:\nhx.set_reference_unit(417.47)\n```\n\n8) Remove your test weight from the scale and create a new script with the code below called `scale.py` (update the reference unit with the value from the step above).\n\n```\nimport time\nimport RPi.GPIO as GPIO\nfrom hx711 import HX711\n\n# Setup HX711\nhx = HX711(5, 6)\nhx.set_reading_format(\"MSB\", \"MSB\")\nhx.set_reference_unit(417.47)  # Use your calculated reference unit here\nhx.reset()\nhx.tare()\n\nprint(\"Scale ready! Place items to weigh...\")\nprint(\"Press Ctrl+C to exit\")\n\ntry:\n    while True:\n        weight = hx.get_weight(3)  # Average of 3 readings\n        print(f\"Weight: {weight:.1f}g\")\n        \n        hx.power_down()\n        hx.power_up()\n        time.sleep(0.5)\n        \nexcept KeyboardInterrupt:\n    print(\"\\nExiting...\")\n    GPIO.cleanup()\n```\n\nRun the script and add the test weight again, you should see it's weight accurately reported in grams.\n\n\n### Quick Troubleshooting\n\nIf you are getting incorrect or inconsistent values:\n\n- Try rerunning the calibration script and increase `num_samples` to 25-30 for better averaging.\n\n- Your setup must be on a solid, level surface: if the bolts on the base plate are preventing it from sitting level try putting something underneath to level things out.\n\n- There is a known issue with byte ordering across Python versions, try changing `hx.set_reading_format(\"MSB\", \"MSB\")` to `hx.set_reading_format(\"LSB\", \"MSB\")` and see if your readings stabilize.\n\nIf you aren't getting any readings at all or are only getting negative readings after taring:\n\n- Check that the connections are solid.\n\n- Check the load cell wires should connect to the HX711 as follows: Red to E+, Black to E-, Green to A-, and White to A+ (the pins labeled B+/B- remain empty).\n\n- Check the HX711 wires should connect to the Raspberry Pi as follows: VCC to Raspberry Pi Pin 2 (5V),GND to Raspberry Pi Pin 6 (GND), DT to Raspberry Pi Pin 29 (GPIO 5), and SCK to Raspberry Pi Pin 31 (GPIO 6).\n\n- Make sure you are using a power supply that is providing adequate power and not causing under voltage issues.\n\n---\n\n You may already see how our code from the previous section could be extended for a variety of different projects, for example, weighing our heavy robin! Instead of bolting my load cell between two rigid sheets I attached it to the bottom of the seed tray in my [bird feeder (which has a handy removable tray which makes adding the load cell a breeze)](https://amzn.to/4lKy14S) and to a rigid sheet fitted into the frame of the bird feeder so that I can weigh birds when they land.\n\nLet's create a final script called `weigh_bird.py` with the following code:\n\n```\nimport time\nfrom datetime import datetime\nimport sys\nimport RPi.GPIO as GPIO\nfrom hx711 import HX711\n\nbird_present = False\n\nBIRD_THRESHOLD = 5 # Lightest british songbird Goldcrest 5g\n\ndef cleanAndExit():\n    print(\"Cleaning...\")\n        \n    print(\"Bye!\")\n    sys.exit()\n\nhx = HX711(5, 6)\n\n\nhx.set_reading_format(\"MSB\", \"MSB\")\n\nreferenceUnit = 416.71\nhx.set_reference_unit(referenceUnit)\n\nhx.reset()\n\nhx.tare()\n\nprint(\"Tare done! Waiting for birds...\")\n\ndef bird_landed(weight):\n    \"\"\"Called when a bird lands on the feeder\"\"\"\n    print(f\"üê¶ Bird landed at {current_time.isoformat()}! Weight: {weight:.2f}g\")\n\ndef bird_left():\n    \"\"\"Called when a bird leaves the feeder\"\"\"\n    print(f\"ü¶Ö Bird left!\")\n    time.sleep(2)\n    print(\"Tare done! Waiting for birds...\")\n    hx.tare()\n\nwhile True:\n    try:\n        current_weight = hx.get_weight(5)\n        current_time = datetime.now()\n\n        if not bird_present and current_weight > BIRD_THRESHOLD:\n            bird_present=True\n            bird_landed(current_weight)\n        \n        elif bird_present and current_weight < BIRD_THRESHOLD:\n            bird_present=False\n            bird_left()\n\n        hx.power_down()\n        hx.power_up()\n        time.sleep(0.1)\n\n    except (KeyboardInterrupt, SystemExit):\n        cleanAndExit()\n\n```\n\nRun the above code and now every time a bird lands on the feeder (or you add a small weight) the script outputs its weight like so:\n\n```\nhugh@bird:~/bird-kafka-demo/bird_weights $ python3 bird_weights.py \nTare done! Waiting for birds...\nüê¶ Bird landed at 2025-07-07T22:55:03.468192! Weight: 9.08g\nü¶Ö Bird left!\nTare done! Waiting for birds...\nüê¶ Bird landed at 2025-07-07T22:55:28.768343! Weight: 59.71g\nü¶Ö Bird left!\nTare done! Waiting for birds...\nüê¶ Bird landed at 2025-07-07T22:55:35.926844! Weight: 52.20g\nü¶Ö Bird left!\nTare done! Waiting for birds...\n```\n\nAmong some other new features we've added like a variable to keep track of if a bird is on the feeder or not to avoid reporting the weight of the same bird multiple times also note that we now tare the scale shortly after the bird takes off. Adding a scale to a bird feeder presents a unique challenge, accounting for bird seed! We re-tare the scale after the bird takes off to avoid eaten bird seed causing the weight of subsequent birds to be under reported.\n\n## Conclusion and Next Steps\n\nYou should now have a working Raspberry Pi powered scale which can be used to accurately weigh things and hopefully some ideas about how you can use it to build your own projects. If you build something using this guide please let me know, I'd love to see what you come up with! If I were to build this project over I would try and reinforce the delicate cables connecting the load cell to the HX711, I tore these a couple of times and had to resolder them, I think maybe wrapping them in heat shrink or putting a blob of hot glue over the solder joints could help with this.\n\nWhilst this project goes someway to helping me figure out just how heavy my visiting robin is it doesn't actually give me any information about what kind of birds are visiting: just how much they weigh. If the robin visits whilst I'm away I won't know if it was him or just some other slovenly bird. To help solve this problem in future I'd like to explore using a webcam and an image classifier to identify the species of bird visiting my feeder.\n\n## Further Reading\n\n- [The Tutorials for Raspberry Pi guide this guide is based on](https://tutorials-raspberrypi.com/digital-raspberry-pi-scale-weight-sensor-hx711)\n- [HX7711 Analogue to Digital Convertor Data Sheet](https://cdn.sparkfun.com/datasheets/Sensors/ForceFlex/hx711_english.pdf)\n- [The Python implementation for HX711 I used in this project](https://github.com/tatobari/hx711py)",
            "status": "published",
            "created_at": 1751842800000,
            "published_at": 1751842800000,
            "updated_at": 1751842800000,
            "author_id": 1
          },
          {
            "id": 19,
            "title": "Get Kafka-Nated (Episode  2) Life of a Kafka contributor with Josep Prat",
            "slug": "get-kafka-nated-ep-2",
            "markdown": "Check out the second episode of Get Kafka - Nated! I had a great time chatting with Josep Prat about his experiences as a Kafka Contributor and Apache Kafka PMC member. We covered everything from first open source contributions to how to properly manage a major open source project.\n\n[Tune in next time](https://www.linkedin.com/events/getkafka-natedepisode3-gregharr7351967184987779074/theater/) for a conversation with Greg Harris about KIP-1150 which introduces Diskless topics to Apache Kafka.\n\n<div class=\"videoWrapper\"><iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/P8ajaQsXAyQ?si=rEq1hv1RQadUgxT1\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe></div>\n\n[Original release](https://www.linkedin.com/events/getkafka-natedepisode2-joseppra7344316406101925888/theater/)",
            "status": "published",
            "created_at": 1752620400000,
            "published_at": 1752620400000,
            "updated_at": 1752620400000,
            "author_id": 1
          },
          {
            "id": 20,
            "title": "Get Kafka-Nated (Episode  3) Greg Harris on Contributing to KIP 1150 diskless",
            "slug": "get-kafka-nated-ep-3",
            "markdown": "Check out the latest episode of Get Kafka-Nated! I had a fantastic conversation with Greg Harris about his work on KIP-1150 and life as an open source software engineer.\n\nWe dove deep into the technical architecture, explored the challenges of implementing this game-changing feature, and discussed what Diskless topics mean for the future of real-time data streaming.\n\n<div class=\"videoWrapper\"><iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/HjWuAsvhVcA?si=pPAJOH11pzuSz1DM\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe></div>\n\n[Original release](https://www.linkedin.com/events/getkafka-natedepisode3-gregharr7351967184987779074/comments/)\n\nYou can find all the past episodes of Get Kafka-Nated as well as Kafka news and technical deep dives over at [getkafkanated.substack.com](https://getkafkanated.substack.com/)",
            "status": "published",
            "created_at": 1753657200000,
            "published_at": 1753657200000,
            "updated_at": 1753657200000,
            "author_id": 1
          },
          {
            "id": 21,
            "title": "Smart Bird Feeder Part 2 - How can I automatically identify bird species from an image? - Using Tensorflow and a webcam to spot birds",
            "slug": "bird-species-image-classifier",
            "markdown": "![An image of robins eating bird seed with the text this robin weighs 26.3g](/content/images/this-robin-weighs-26.3g.png)\n\nIn [the previous entry in this series](/load-cell-raspberry-pi) I built a smart bird feeder that could weigh birds with the goal of figuring out how heavy a particularly portly looking robin was. This only got my part of the way to my goal of once and for all answering the question: is this an abnormally huge robin?\n\nThe next step is to collect pictures of birds that visit my bird feeder and automatically label them with the species to check to see if the image is of a Robin or not, this will let me track just the weights of Robins so I can easily spot any abnormally heavy birds.\n\nThe below guide will talk you through step by step everything you need to do to take a picture of a bird using a cheap webcam and a Raspberry Pi and then using an image classifier model to identify the bird species. \n\n---\n\n## What is an image classifier model?\n\nWhy do we need an image classifier model at all? Our bird feeder can now weigh visiting birds, but weight alone doesn't tell us the species: a 60g bird could be an enormous robin or a tiny pigeon. An image classifier model can analyze a photo from our webcam and automatically identify the bird species so we can track weights by species.\n\nThe model works by analyzing the mathematical patterns in the image data that distinguish one bird species from another. Rather than training our own model (which would require thousands of labeled bird photos), we'll use a pre-trained model that already knows how to several British bird and non-bird species including:\n\n* squirrel\n* crow\n* wren\n* pigeon\n* cat\n* house sparrow\n* magpie\n* blackbird\n* dunnock\n* chaffinch\n* song thrush\n* robin\n\n---\n\n## Hardware Setup\n\nIf you tried setting up your own bird feeder from the first part of this series you'll have everything you need already apart from the camera, if not you can get everything you need from the list below.\n\n### Hardware shopping list\n\n- [Raspberry Pi 3](https://amzn.to/43Fst45) (¬£35) - or any similarly capable computer with soldered headers for GPIO, you can pick Raspberry Pi 3s now for as little as ¬£10 second hand. *Note: Raspberry Pi 3s won't be able to run the classifier model without appropriate cooling, I chose to run the model on my laptop instead but you should consider alternatives to the Pi 3 if you want to run the model on device).\n- [Micro SD Card](https://amzn.to/43C0nXh) (¬£10) - These get cheaper all the time, here I've linked to a 128GB card that's relatively good value but really anything over 16GB will be fine as long as you can write to it properly, for more information on picking an appropriate SD card see [this great article by Chris Dzombak](https://www.dzombak.com/blog/2023/12/choosing-the-right-sd-card-for-your-pi/).\n- [Webcam](https://amzn.to/44Z1MJx) (¬£40) - Relatively cheap 1080p USB Webcam, I've linked to the one I used because it's nice to mount but I already had this one lying around and I suspect you can find one going spare at work or elsewhere if you're on a budget.\n- [Suction cups](https://amzn.to/4l6OFuJ) (¬£7) - These make it really easy to stick your webcam to a window for simple mounting and adjustment. The threaded bolt part fits into the standard ISO insert in the webcam above.\n- [Micro USB Power Supply](https://amzn.to/43C0nXh) (¬£15) - make sure to pick a reliable power supply that consistently delivers 5V/2.5A, I've linked to the official power supply here but almost any half decent Micro USB power supply will do.\n- Optional: [window mounted bird feeder](https://amzn.to/4lKy14S) (¬£30) - This is the bird feeder I used, having it mount on a window makes it much easier to get clear pictures by sticking your webcam on the other side of the glass.\n\n## Setup\n\n1) Flash your SD card and setup your Raspberry Pi. For instructions on how to do this properly check out [this guide on the Raspberry Pi website](https://www.raspberrypi.com/documentation/computers/getting-started.html). Connect your webcam to a USB port on your Raspberry Pi.\n\n![Diagram showing webcam connected to USB port on a raspberry pi](/content/images/plug-in-webcam.png)\n\n2) Screw one of the suction cups into the threaded insert in your webcam - this will make it easy to position and adjust your webcam in your window.\n\n![Diagram showing a suction sub with a thread bolt being inserted into a threaded brass insert in the base of a webcam](/content/images/insert-suction-cup.png)\n\n3) Stick your webcam somewhere with a good view of your bird feeder, the closer the lens is to the glass the less glare you'll have in your images. Camera positioning is crucial for accurate bird identification:\n\n- Position the camera as close as you can to bird feeder. If the camera is too far away details can become unclear for classification.\n- Natural daylight works best. Avoid positioning the camera where it captures direct sunlight or creates harsh shadows on the feeder. North-facing windows often provide the most consistent lighting throughout the day.\n- A simple, uncluttered background helps the model focus on the bird. If your view is busy with garden furniture or complex foliage, consider adding a plain backdrop behind your feeder.\n- Most webcams have fixed focus, so test your setup by taking a few photos of objects placed where birds typically perch. Adjust the camera distance until birds in the center of the frame appear sharp.\n- Position the camera to capture birds from the side rather than head-on or from behind - side profiles show the most distinguishing features like breast markings, wing patterns, and body shape.\n\nRemember that the model was trained on a variety of lighting conditions and angles, so don't worry about getting perfect shots every time - even a blurry Robin in motion can classify correctly!\n\n![A diagram showing a view of a window from the outside with a webcam stuck facing a bird feeder](/content/images/view-of-a-bird-feeder.png)\n\n4) Now that we've got a nice little bird photo-booth set up we can start taking some pictures (if you're following along from part 1 you can update your code to take a photo when a bird is detected [see my source code on GitHub for reference](https://github.com/hevansDev/bird-kafka-demo/blob/main/bird_weights/bird_weights.py)), lets install [OpenCV](https://opencv.org/) for capturing and processing pictures from the webcam.\n\n```bash\npython3 -m pip install opencv-python-headless==4.8.1.78\n```\n\n5) Create a new script called `take_picture.py` with the following Python code:\n\n```python\nimport os\nimport time\nfrom datetime import datetime\nimport sys\nimport cv2\n\ndef take_photo():\n    \"\"\"Take a photo when a bird lands\"\"\"\n    cap = cv2.VideoCapture(0)\n    if cap.isOpened():\n        # Let camera adjust\n        for i in range(5):\n            ret, frame = cap.read()\n        ret, frame = cap.read()\n        if ret:\n            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n            filename = f\"bird_{timestamp}.jpg\"\n            cv2.imwrite(\"./images/\"+filename, frame)\n            print(f\"üì∏ Photo: {filename}\")\n        cap.release()\n\ntake_photo()\n```\n\nThis script will take a picture and save it to the `images` directory, lets create that dir now and test our script out.\n\n```bash\nmkdir images\npython take_picture.py\n```\n\nYou should end up with a picture like the example below in the `images` dir (for those following on from part 1 your images will also include the weight measured when the photo was taken).\n\n![A picture of a robin on a bird feeder](/content/images/bird_20250801_120750_19.5g.jpg)\n*images/bird_20250801_120750.jpg*\n\n6) Now that we have an image of a bird we can use a classifier model to predict the species of the bird in the image. \n\n![A warning triangle with a broken raspberry pi in it](/content/images/dead-pi.png)\n\n**It is unlikely that your Raspberry Pi will be able to run the model due to how computationally intensive it can be to run - I suggest copying your `images` dir from the previous step to your laptop or more powerful computer! I tried running the model on my Raspberry Pi on a hot day and it got so hot it was permanently damaged, by default the Pi has no active cooling unlike your PC or laptop so this can be surprisingly easy to do.**\n\nFor this we'll use the pre-trained uk garden birds model from [secretbatcave](https://github.com/secretbatcave/Uk-Bird-Classifier). Download the saved model (the `.pb` stands for [ProtoBuff](https://www.tensorflow.org/guide/saved_model#the_savedmodel_format_on_disk) format) and the classes with:\n\n```bash\nmkdir models\ncurl -o models/ukGardenModel.pb https://raw.githubusercontent.com/secretbatcave/Uk-Bird-Classifier/master/models/ukGardenModel.pb\ncurl -o models/ukGardenModel_labels.txt https://raw.githubusercontent.com/secretbatcave/Uk-Bird-Classifier/master/models/ukGardenModel_labels.txt\n```\n\n7) Install tensorflow and its dependencies. [Tensorflow](https://www.tensorflow.org/learn) is a software library for machine learning that was used to produce the model we're working with here, we'll use it now to run the model to make a bird species prediction.\n\n```bash\npip install tensorflow \"numpy<2\"  protobuf==5.28.3\n```\n\n8) Create a new Python script called `identify_bird.py` with the following Python code:\n\n```python\nimport os\nimport sys\nimport numpy as np\nimport tensorflow.compat.v1 as tf\ntf.disable_v2_behavior()\n\n# Suppress warnings\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\ntf.disable_v2_behavior()\n\n# Load model\nwith tf.io.gfile.GFile('models/ukGardenModel.pb', 'rb') as f:\n    graph_def = tf.GraphDef()\n    graph_def.ParseFromString(f.read())\n    tf.import_graph_def(graph_def, name='')\n\n# Load labels\nwith open('models/ukGardenModel_labels.txt', 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Read image\nimage_path = sys.argv[1]\nwith open(image_path, 'rb') as f:\n    image_data = f.read()\n\n# Run inference\nwith tf.Session() as sess:\n    predictions = sess.run('final_result:0', {'DecodeJpeg/contents:0': image_data})\n    bird_class = labels[np.argmax(predictions)]\n    print(bird_class)\n```\n\nNote the use of `tensorflow.compat.v1`: this is an older model (from 7+ years ago) so we're using the version 1 compatibility module rather than `tensorflow` to ensure everything works correctly (this is also why we're using the `\"numpy<2\"` and `protobuf==5.28.3` downgrades). There are better models out there but this one is lightweight, free to use, and does not require API access.\n\nLets try making a prediction with one of your photos to see if everything is working correctly:\n\n```bash\npython identify_bird.py images/bird_20250801_120750.jpg\n```\n\nYou should see a result like:\n\n```\nWARNING:tensorflow:From /Users/hugh/test/.venv/lib/python3.13/site-packages/tensorflow/python/compat/v2_compat.py:98: disable_resource_variables (from tensorflow.python.ops.resource_variables_toggle) is deprecated and will be removed in a future version.\nInstructions for updating:\nnon-resource variables are not supported in the long term\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1754516598.102893 5536073 mlir_graph_optimization_pass.cc:437] MLIR V1 optimization pass is not enabled\nrobin\n```\n\nYou should see a predicted bird species on the last line of the output.\n\n### Quick Troubleshooting\n\n* `No module named 'cv2'` or other OpenCV errors: Make sure you installed the headless version with `python3 -m pip install opencv-python-headless==4.8.1.78`. The regular opencv-python package can cause issues on headless Raspberry Pi setups.\n\n* Camera not found / `cannot open camera` error: Check your camera is properly connected with `lsusb`, you should see your webcam listed.\n\n```bash\nhugh@bird:~/bird-kafka-demo $ lsusb\nBus 001 Device 004: ID 328f:003f EMEET HD Webcam eMeet C960\nBus 001 Device 003: ID 0424:ec00 Microchip Technology, Inc. (formerly SMSC) SMSC9512/9514 Fast Ethernet Adapter\nBus 001 Device 002: ID 0424:9514 Microchip Technology, Inc. (formerly SMSC) SMC9514 Hub\nBus 001 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub\n```\n\nTry unplugging and reconnecting the USB cable or trying a different USB port. Some cameras need a moment to initialize after being plugged in. Check with the manufacturers website to see if your webcam requires any specific drivers to work with the Pi.\n\n* Photos are too dark or too bright: Most USB webcams auto-adjust exposure, but this can take a few seconds. The script already captures 5 frames before taking the final photo to allow for this adjustment, but you might need to increase this number for your specific camera.\n\n```python\n        for i in range(5): # Try increasing this value\n            ret, frame = cap.read()\n```\n\n* TensorFlow installation errors: The model requires specific versions. If you get import errors, try: `pip uninstall tensorflow numpy protobuf` followed by `pip install tensorflow \"numpy<2\"  protobuf==5.28.3`.\n\n* Incorrect classifications: The model works best with clear side-view shots of birds. Very small birds in the distance, birds partially obscured by feeder parts, or photos with multiple birds may give poor results. Try repositioning your camera for clearer shots. Double check the list of classes in `ukGardenModel_labels.txt` to see if the bird species in your model is represented there at all, this model is great at spotting robins but it wasn't trained on images of Blue Tits so might label them incorrectly as a cat or a crow.\n\n---\n\n## Conclusion and Next Steps\n\nYou now have two separate systems: one that detects and photographs birds (and weighs birds if you're following on from part 1), and another that identifies species. These systems can't run on the same hardware though because of the performance limitations of the Raspberry Pi and right now our workflow requires transferring the bird photos to our laptop periodically to run species identification. With this setup I now have some pictures of heavy robins but without storing and analyzing lots of examples of images of birds with species and weight labels I still can't answer my original question of: is this robin abnormally heavy?\n\nIn the third and final entry in this bird feeder series I'll use Kafka and Iceberg to bridge the gap between my laptop and the bird feeder, analyze all my collected data, and once and for all figure out just how heavy this Robin is.\n\n\n## Further Reading\n\n- YouTube video [Use AI on a RASPBERRY PI to IDENTIFY BIRDS](https://www.youtube.com/watch?v=pFEhSCYy2LA)\n- [Train your own image classifier with TensorFlow](https://www.tensorflow.org/tutorials/images/classification)",
            "status": "published",
            "created_at": 1754175600000,
            "published_at": 1754175600000,
            "updated_at": 1754175600000,
            "author_id": 1
          },
          {
            "id": 22,
            "title": "Get Kafka-Nated (Episode  4) Alex Merced Co Author of the Iceberg Definitive Guide",
            "slug": "get-kafka-nated-ep-4",
            "markdown": "Check out the latest episode of Get Kafka-Nated! I had a fantastic conversation with Alex Merced, Head of DevRel at Dremio and co-author of \"Apache Iceberg: The Definitive Guide,\" about why Iceberg is becoming essential for streaming data architectures.\n\nWe explored how Apache Iceberg is blurring the lines between streaming and analytics, discussed the exciting new \"Iceberg Topics\" development that makes Kafka topics directly queryable as tables, and dove into real-world patterns for integrating Kafka with modern data lakehouses.\n\nAlex shared insights from his work with enterprises adopting lakehouse architectures, explained how schema evolution works in streaming scenarios, and gave practical advice for teams looking to add Iceberg to their Kafka-based data pipelines.\n\n<div class=\"videoWrapper\"><iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/nJ9uV-DZjPQ?si=FWh1T5GcT9PL1Q6e\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe></div>\n\n[Original release](https://www.linkedin.com/events/getkafka-natedep4-alexmerced-co7356997308900597760/theater/)\n\nYou can find all the past episodes of Get Kafka-Nated as well as Kafka news and technical deep dives over at [getkafkanated.substack.com](https://getkafkanated.substack.com/)",
            "status": "published",
            "created_at": 1755212400000,
            "published_at": 1755212400000,
            "updated_at": 1755212400000,
            "author_id": 1
          },
          {
            "id": 23,
            "title": "Get Kafka-Nated (Episode  5) Uber Staff Engineer and Kafka Tiered Storage Contributor Satish Duggana",
            "slug": "get-kafka-nated-ep-5",
            "markdown": "Check out the latest episode of Get Kafka-Nated! I had a fantastic conversation with Satish D from Uber, who was a key contributor to KIP 405 that brought tiered storage to Kafka.\n\nWe explored what tiered storage actually is, the specific storage challenges at Uber that led to KIP 405, and the biggest technical hurdles in implementing such a fundamental change to Kafka. Satish shared insights on working with the Apache community process including why it can take so long to get major KIPs into open-source Kafka.\n\nI really getting to interview another member of the community who was directly involved in architecting one of Kafka's most significant features. Check out the recording below. \n\n<div class=\"videoWrapper\"><iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/KflHyS4WLh4?si=Pc6WDugavKZ54XhJ\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe></div>\n\n[Original release](https://www.linkedin.com/events/getkafka-natedep5-satishduggana7358442645544652801/theater/)\n\nYou can find all the past episodes of Get Kafka-Nated as well as Kafka news and technical deep dives over at [getkafkanated.substack.com](https://getkafkanated.substack.com/)",
            "status": "published",
            "created_at": 1755644400000,
            "published_at": 1755644400000,
            "updated_at": 1755644400000,
            "author_id": 1
          },
          {
            "id": 24,
            "title": "Get Kafka-Nated (Episode 6) Kafka Meets Flink  with Ben Gamble",
            "slug": "get-kafka-nated-ep-6",
            "markdown": "Check out the latest episode of Get Kafka-Nated! This one was a real treat - I had a great time chatting with my friend and field CTO at Ververica Ben about all things Flink and Kafka. I think we could probably have talked all day about this but you can check out the 30 minute version below!\n\n<div class=\"videoWrapper\"><iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/-8qikNKs7n8?si=c3q95lzPLRw1Ien0\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe></div>\n\n[Original release](https://www.linkedin.com/events/getkafkanated-ep6-kafkameetsfli7365781071902294016/theater/)\n\nYou can find all the past episodes of Get Kafka-Nated as well as Kafka news and technical deep dives over at [getkafkanated.substack.com](https://getkafkanated.substack.com/)",
            "status": "published",
            "created_at": 1756854000000,
            "published_at": 1756854000000,
            "updated_at": 1756854000000,
            "author_id": 1
          },
          {
            "id": 25,
            "title": "My PyCon UK 2025 Highlights",
            "slug": "py-con-uk-2025",
            "markdown": "![PyCon UK 2025](/content/images/pyconuk.png)\n\nI had a great time at my first PyCon UK over the weekend: the community was really friendly and I feel like I missed as many good talks as I saw which feels the mark of a good conference to me. \n\n## **Day One: Highlights üêç**\n\nI started the day with a nice wander around Manchester before the conference and a great breakfast at Brunchos before heading over to the Contact Theatre for the welcome session.\n\n![](/content/images/pycon25_day1.jpg)\n\nThere was [an awesome keynote](https://youtu.be/gDvwRpl9erE?si=4hGbubiNqFSUl8GF) from [**Hynek Schlawack**](https://hynek.me/) about Python's Super Power followed by [**David Seddon**](https://seddonym.me)'s [great talk](https://youtu.be/3N8qs0qWZoY?si=ZBRIKfm1ZkTgz-Yf) on why other collection types are often better than Python lists.\n\n[**Hannah Hazi**](https://www.linkedin.com/in/hannah-hazi/)'s talk about recovering from Long Covid as a programmer was full of useful information about the ongoing impact of Covid and a first hand account of surviving long covid.\n\nI had a lot of fun at [**David Asboth**](https://davidasboth.com/index.html)'s [talk about using exec to put Python in your Python](https://youtu.be/sSSHHFVCMYg?si=nXtwz-fCbxnNT3Lk) so you can code while you code (all the best Python features have big red warning boxes in the docs).\n\n[**CJ Shearwood**](https://bsky.app/profile/cj.shearwood.games)'s brilliant talk [\"I'm a Luddite, Why Aren't You\"](https://youtu.be/HXdSGF0cB5U?si=tDJBpv4CyBxYwquX) was an insightful and moving history on the intersection of technology and labour organising with lessons as relevant today as they were in 1800\\.\n\nWe had some awesome [lightning talks](https://youtu.be/CouUftzuQVQ?si=Fcx2LaGMFiJDboHh) from [**Eli Holderness**](https://www.linkedin.com/in/eli-holderness-4890b886), [**Lydia Cordell**](https://www.linkedin.com/in/lydia-cordell/), [**Anthony Harrison**](https://www.linkedin.com/in/anthonypharrison/), [**Jyoti Bhogal**](https://www.linkedin.com/in/jyoti-bhogal/), [**Perla Godinez Castillo**](https://www.linkedin.com/in/perlagcastillo/), **Alex Willmer**, and [**Sheena O'Connell**](https://www.linkedin.com/in/sheena-oconnell-7a4991b9/). I took the opportunity to speak a bit about [**notanother.pizza**](https://notanother.pizza/) and community organising in another \"Meetup is terrible now\" lightning talk.\n\n<div class=\"videoWrapper\"><iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/CmCvNasHwZo?si=dUMYhjumOZO6TMeo\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe></div>\n\nMy favourite talk of the day was a lightning talk from [**Daniele Procida**](https://www.linkedin.com/in/danieleprocida/), the unassumingly titled **\"What is your favourite film?\"**. It was as delightful as it was thought provoking \\- [I won't spoil it, watch the recording if you get the chance](https://youtu.be/CouUftzuQVQ?si=LaxPF3ib0IiJTpLi&t=645)\\!\n\n## **Day Two: Highlights üêç**\n\nIn the morning I had the pleasure of facilitating a session on making ASCII Art with Python for the young coders track alongside some awesome volunteers including [**Ekaterina Savenya**](https://www.linkedin.com/in/ekaterina-savenya/) and **Nese Dincer**. The volunteers who facilitate the young coders track are beyond awesome.\n\nIt was [really interesting](https://youtu.be/xlse7KJLUus?si=yNmlxgnherS6r2sZ) hearing from [**Kristian Glass**](https://www.linkedin.com/in/kristian-glass-b2552b1/) about the work of the [**The UK Python Association**](https://uk.python.org/) and how instrumental they are in supporting the Python community (and not just in the UK\\!)\n\n[The tale of PEP 765](https://youtu.be/vrVXgeD2fts?si=v4bG3fHWhB5WFroP) as told by [**Irit Katriel**](https://www.linkedin.com/in/irit-katriel-6b501a/) was a great story of how to find and fix a problem in open-source even when not everyone agrees that the problem should be fixed.\n\n[**Aivars Kalvans**](https://www.linkedin.com/in/aivarsk/) talk on solving a Python mystery took me back to my time in the DevOps trenches and had some great pointers for troubleshooting Python applications in production.\n\n![](/content/images/pycon25_day2.jpg)\n\nNaturally my favourite talk of the day was the presentation from the young coders sharing their learnings from the day \\- to say these kids were awesome is an understatement, I do public speaking for a living and I don't think I'm half as confident in front of a conference audience as these young people\\!\n\nThere were [some more brilliant lightning talks](https://youtu.be/pQRNVCJf-LQ?si=KjD3JOn0xMuTgAHH) covering everything from pencil preferences, to building a crow army, to getting started self-hosting, to exciting news from open-source.\n\nAlso there was a badge maker ü§©\n\n## **Day Three: Highlights üêç**\n\nThe morning keynote []\"Playing the long game\"](https://youtu.be/b0GqRDfumR8?si=Qo6FE8ql9yJVc4s9) by [**Sheena O'Connell**](https://www.linkedin.com/in/sheena-oconnell-7a4991b9/) was an insightful session about development in a world of Language Models with some great advice for developers regardless of experience.\n\n[**Deb Nicholson**](https://www.linkedin.com/in/badhessian/)'s [talk about coping with your project becoming popular](https://youtu.be/sxT2UikIyeQ?si=6rVJGfQ7Is00DbEm) was full of extremely good advice for both software projects and any community facing projects writ large. I particularly liked the point around letting people know what you need help with and politely pushing back on companies that think you work for them. If you're in any kind of community organising role this talk is essential reading\\!\n\n[**John Carney**](https://www.linkedin.com/in/john-carney-706b7062/) and I helped facilitate a hallway track conversation with some community organising folks with some really useful insights into volunteer experience, keeping your institutional knowledge somewhere other than in your head, and avoiding (or not avoiding as it happens) burn out.\n\nI'm super excited for t-strings to come to Python now after [**Dr. Philip Jones**](https://www.linkedin.com/in/dr-philip-jones-4b0a1879/) shared [his work on using them to dynamically build SQL queries in Python](https://youtu.be/naQyq9kd2_w?si=Rv6L4InB3dbqbLRy). Then of course there was the awesome talk from my wonderful colleague at [**Aiven**](https://aiven.io/) [**Tibs**](https://www.linkedin.com/in/tony-ibbs/) about [building an app using CLIP, PostgreSQL¬Æ and pgvector](https://youtu.be/p3JThptSYos?si=ogaHAhGBcp32b_Nh). Always fun nerd sniping Tibs with a question in the Q and A\\!\n\n![](/content/images/pycon25_day3.jpg)\n\nThere was [another round of awesome lightning talks](https://youtu.be/pQRNVCJf-LQ?si=qfT5YCi-8XfzZ_6P) including from the amazing [**Dawn Gibson Wages**](https://www.linkedin.com/in/dawnwages/) with a hero's journey of Python development.\n\n## **Day Four: Sprints and Contributions üêç**\n\n![](/content/images/pycon25_day4.jpg)\n\nI had a great time participating in the [**BeeWare**](https://beeware.org/) sprints and earning my challenge coin with some tiny docs changes. It was also great helping [**Ekaterina Savenya**](https://www.linkedin.com/in/ekaterina-savenya/) and [**Dan Taylor**](https://www.linkedin.com/in/dan-taylor0/) make some contributions as well, although I feel I might have been more than a hinderance than a help when it came to updating Town Crier üòÖ A huge thanks to [**Russell Keith-Magee**](https://www.linkedin.com/in/freakboy3742/) for running the session and generally being an exemplary maintainer of such a cool project\\!\n\n---\n\nA huge thank you to the [**The UK Python Association**](https://uk.python.org/) and the organisers and volunteers that put on **PyCon UK** this year. I had a wonderful time and it was such a treat to spend the weekend with so many interesting people, already looking forward to the next one\\!\n\n*All recordings are now available at the [PyCon UK YouTube channel](https://www.youtube.com/@PyconUKSoc)*",
            "status": "published",
            "created_at": 1758668400000,
            "published_at": 1758668400000,
            "updated_at": 1758668400000,
            "author_id": 1
          },
          {
            "id": 26,
            "title": "Get Kafka-Nated (Episode 7) Redpanda vs Kafka with Tristan Stevens",
            "slug": "get-kafka-nated-ep-7",
            "markdown": "Check out the latest episode of Get Kafka-Nated! I get a great conversation with Tristan about the merits of Kafka vs Kafka Compatible solutions. You can watch the recording below.\n\n<div class=\"videoWrapper\"><iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/gAtEmpBKtG0?si=Qg96Ca__UqE6tlGL\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe></div>\n\n[Original release](https://www.linkedin.com/events/getkafka-natedep7-redpandavskaf7373740658102083584/theater/)\n\nYou can find all the past episodes of Get Kafka-Nated as well as Kafka news and technical deep dives over at [getkafkanated.substack.com](https://getkafkanated.substack.com/)",
            "status": "published",
            "created_at": 1758754800000,
            "published_at": 1758754800000,
            "updated_at": 1758754800000,
            "author_id": 1
          },
          {
            "id": 27,
            "title": "Is Meetup Terrible or is *my* meetup terrible?",
            "slug": "is-meetup-terrible-or-is-my-meetup-terrible",
            "markdown": "I‚Äôve spent a lot of time and energy this year complaining about Meetup ([at the CodeBar Festival Fringe](https://hughevans.dev/meetup-is-terrible-now-codebar-festival-fringe/), [at PyCon UK](https://hughevans.dev/py-con-uk-2025/#:~:text=I%20took%20the%20opportunity%20to%20speak%20a%20bit%20about%20notanother.pizza%20and%20community%20organising%20in%20another%20%E2%80%9CMeetup%20is%20terrible%20now%E2%80%9D%20lightning%20talk.), and [on my blog](https://hughevans.dev/not-another-pizza/)). Meetup is becoming more expensive, less reliable, and generally ‚Äò[enshittified](https://en.wikipedia.org/wiki/Enshittification)‚Äô which came to a head at the beginning of this year when [AI Signals, a community](http://aisignals.org.uk) I organise through Meetup saw a marked drop in attendance.\n\nThe thesis of my ‚ÄúMeetup is Terrible Now‚Äù talk was that reducing reliance on Meetup could help with declining attendance and the burden of organising events \\- rather than just speculating about this though I actually followed my own advice to see if it would work in practice.\n\n![](/content/images/before-signals-attendance.png)\n\nMeetup as a platform provides network effects that drive attendance, tooling for managing RSVPs and community membership, and has good SEO for discoverability via search (in part because of their capturing of the ‚ÄúMeetup‚Äù namespace). Any solution to my problem must still have these things to be a meaningful Meetup alternative.\n\nSome potential solutions I proposed were:\n\n* Moving platforms \\- use another community platform like Luma, OddCircles etc instead of Meetup  \n* [POSSE \\- Post Own Site Syndicate Everywhere](https://indieweb.org/POSSE), maintain a central self-hosted community platform and syndicate event invites across 3rd party community platforms  \n* Collaboration \\- Work with other organisers to share resources and cross promote events (potentially via [notanother.pizza](http://notanother.pizza), more on this later)\n\nEach has their own merits and issues but my hope was that in combination these three solutions might be enough to break from dependence on Meetup.\n\nWhat I didn‚Äôt cover in my talk was metrics of the success of these solutions which are an important part of any initiative like this, in the end I tracked the following metrics before and after embarking on this experiment:\n\n* Event attendance  \n* Net new organisers  \n* % Of event RSVPs from Meetup vs non-Meetup sources\n\n## Putting it into practice\n\nWorking with my friend and former colleague Steve Morris, we rebranded AI Signals from AIDLE (AI and Deep Learning for Enterprise) and launched our own website. We were hugely lucky to have support from Steve ([you can read more about the strategy and brand work we did with Steve over on his blog here](https://steve-morris.medium.com/strategy-is-what-you-do-next-e39d83be3f04)) and as you‚Äôll see this was one of the most impactful actions in this initiative, but I ran out of steam before successfully implementing POSSE for AI Signals. Starting my new role at Aiven meant I no longer had time for larger infrastructure efforts so setting up POSSE was put on hold.\n\nWe set up [a Luma calendar](https://luma.com/ai-signals?k=c&period=past) and started cross promoting events from Meetup there. We also embedded Luma as the registration system for events in our website with a view to encourage our audience to move to Luma from Meetup. \n\nAdditionally we worked with other technical communities in London like [Women in Data](https://www.womenindata.org/) to co-host and cross promote events. \n\n## The result: What worked\n\nThe rebrand was enormously successful \\- at the first event after the rebrand we had a turnout of 55 people, more than double the attendance of the previous event at which we only had 20 people.   \n![](/content/images/after-signals-attendance.png)\n\nThis increase in attendance has been lasting \\- we now see an average of 53 attendees at each event post rebrand. We also had several new organisers join, growing our team from 3 to 10 people. Anecdotally, some cited the new brand as sparking their interest in getting involved. We also saw an increase in submissions to our call for papers and three new sponsors engaging with us.\n\n![](/content/images/ai-signals-growing-team.png)\n\nPromoting events in parallel on Luma contributed to some net new RSVPs in this time, it‚Äôs hard to say by exactly how much though as we don‚Äôt register attendees on the door so we don‚Äôt know exactly how many people attend a given event from Meetup vs Luma. With that said though since we started promoting events on Luma 32% of our total RSVPs have been from Luma.\n\n![](/content/images/luma-vs-meetup-post-signals.png)\n\n[Our event in collaboration with Women in Data](https://aisignals.org.uk/events/spotlight-on-agentic-ai-women-in-data) had an above average attendance compared to other post rebrand events. I don‚Äôt have good data on referrals from cross promotion from other communities but we have seen a lot of new faces since we started collaboration with other groups. We also worked with the Civo team to promote their Civo navigate event and in exchange they helped us out with some free venue use.\n\n## The result: What didn‚Äôt\n\nBoth our audience and audience growth remain concentrated on Meetup. We now have 4,084 members there versus only 236 on Luma. I had hoped to pull the plug on our Meetup page at the end of this year but as it stands I won‚Äôt be able to do that \\- the network effects there remain too strong to leave on the table for now.\n\nWhilst the new brand and our website was extremely impactful, building a website with SquareSpace added additional costs in the form of a ¬£204 a year Squarespace subscription and ¬£16 a year for domain registration. Squarespace is a powerful tool but many of the features that would help us in our efforts to move off Meetup such as managing mailing lists and sending mass emails are paywalled. In the long run I‚Äôd like us to migrate off Squarespace to avoid trading one bad platform for another.  \nWe didn‚Äôt have as many opportunities to collaborate and for cross promotion with other communities as I would have liked. We started providing a slot in our event for other communities to promote their community announcements but there was virtually zero take up for this after the first time we did this.\n\n## Conclusion\n\nMeetup is still Terrible: expensive, unreliable, and plagued with bad user experience. Still though there was a lot we could do better to make our Meetup a success and avoid terrible events with very low attendance and engagement.  \nWe haven‚Äôt yet been able to execute a full move off of Meetup so it's unclear whether moving off Meetup entirely would have been beneficial \\- although the continued growth of our community on the platform would suggest otherwise. Adding Luma as a platform didn‚Äôt hurt but did contribute to workload for our organisers with replicated effort for each additional platform. The rebrand drove new engagement although without directly surveying our volunteers or attendees it‚Äôs hard to conclusively link the spike in growth with the new brand.\n\nMy approach to measuring metrics of success made it hard to decouple the result of each specific action but overall the result has been positive. Rebranding and parallel promotion of events to Luma definitely helped revive our community but did not measurably reduce our reliance on Meetup. Collaboration was great for bringing some new energy into our community but the impact of our collaboration efforts was hard to measure.\n\nYes, Meetup is terrible, but by doing things differently our community got back to growing and hosting great AI talks for our community.\n\n## Next Steps\n\nI‚Äôm working with the awesome team at PyData London in an effort to learn how to coordinate our growing team of volunteers at AI Signals. I've learned a lot from volunteering about building teams around trust with PyData and may write up some of this work in future. \n\nAt Signals we‚Äôre figuring out how to make the most of our regained momentum, considering setting up an LLC to allow us to transact, building new partnerships with sponsors, and scaling the number of events we‚Äôre able to deliver.\n\n---\n\nIf you‚Äôre a community organiser I hope this article had some useful insights into our experiences trying to move away from Meetup. I‚Äôd love to hear from you about your experiences as an organiser, please join the community over at [notanother.pizza](http://notanother.pizza) and join the conversation.",
            "status": "published",
            "created_at": 1758841200000,
            "published_at": 1758841200000,
            "updated_at": 1758841200000,
            "author_id": 1
          },
          {
            "id": 28,
            "title": "Getting Started with Iceberg Topics - A Beginner's Guide",
            "slug": "2025-09-5-beginners-guide-iceberg-topics",
            "markdown": "*Understand how Kafka integrates with Apache Iceberg‚Ñ¢ and experiment locally with Docker and Spark*\n\n**The streaming data landscape is evolving rapidly**, and one of the most exciting developments is the integration between Apache Kafka and Apache Iceberg. While Kafka excels at real-time data streaming, organizations often struggle with the complexity of moving streaming data into analytical systems. Iceberg Topics for Apache Kafka promises to bridge this gap by enabling direct integration between Kafka streams and Iceberg tables, creating a seamless path from real-time ingestion to analytical workloads.\n\nIn this article, I'll share what Iceberg Topics are, walk you through a hands-on example you can run locally, and explore the potential this integration holds for modern data architectures. But first, let's understand what we're working with.\n\n## What is Apache Iceberg?\n\nApache Iceberg is an open table format designed for huge analytic datasets. Unlike traditional data formats, Iceberg provides features that make it ideal for data lakes and analytical workloads. It's become increasingly popular because it solves many of the pain points associated with managing large-scale analytical data, including:\n\n* ACID transactions: Ensuring data consistency across concurrent operations\n* Schema evolution: Adding, dropping, or renaming columns without breaking existing queries\n* Time travel: Querying historical versions of your data\n* Hidden partitioning: Automatic partition management without exposing partition details to users\n\n## What are Iceberg Topics?\n\nIceberg Topics represent a powerful integration between Kafka's streaming capabilities and Iceberg's analytical features. Instead of requiring complex ETL pipelines to move data from Kafka into analytical systems, Iceberg Topics allow Kafka to write data directly into Iceberg table format in object storage like S3 - all zero copy without unnecessary data replication across brokers, sink connectors, and sinks.\n\nBefore:\n\n![](/content/images/before_iceberg_topics.png)\n\nThis integration leverages Kafka's Remote Storage Manager (RSM) plugin architecture to seamlessly transform streaming data into Iceberg tables. When you create a topic with Iceberg integration enabled, Kafka automatically:\n\n1. **Streams data** through standard Kafka topics as usual\n2. **Transforms messages** into Iceberg table format using schema registry integration\n3. **Writes data** directly to object storage as Iceberg tables\n4. **Enables seamless querying** through Spark, Trino, or other Iceberg-compatible engines once segments are written to the Iceberg table\n\nAfter:\n\n![](/content/images/iceberg_topics_after.png)\n\nThe beauty of this approach is that it maintains full Kafka API compatibility while adding analytical capabilities. Your existing producers and consumers continue to work unchanged, but now your streaming data is simultaneously available for real-time processing and analytical queries.\n\n## The Benefits of Iceberg Topics\n\nTraditional architectures require separate systems for streaming and analytics, creating operational complexity and data duplication. With Iceberg Topics, you get:\n\n**Simplified Architecture:** Eliminate complex ETL pipelines between streaming and analytical systems. Data flows directly from Kafka into queryable Iceberg tables.\n\n**Unified Data Model:** Use the same schema for both streaming and analytical workloads, reducing inconsistencies and maintenance overhead.\n\n**Real-time Analytics:** Query streaming data without waiting for batch processes to complete.\n\n**Cost Efficiency:** Reduce infrastructure costs by eliminating duplicate storage and processing systems.\n\n**Operational Simplicity:** Manage one system instead of coordinating between streaming platforms and data lakes.\n\n**Note:** Iceberg Topics integration is still evolving in the Kafka ecosystem. The example in this article demonstrates the concept using Aiven's Remote Storage Manager plugin, which provides Iceberg integration capabilities for experimentation and development.\n\n## Run Iceberg Topics Locally with Docker\n\nTo understand how Iceberg Topics work, let's set up a complete local environment with Kafka, MinIO (for S3-compatible storage), Apache Iceberg REST catalog, and Spark for querying. This setup will let you see the entire data flow from Kafka streams to Iceberg tables.\n\n### Prerequisites\n\nBefore getting started, ensure you have the following installed:\n\n* **JDK version 17 or newer** - Required for building the plugin\n* **Docker** - For running the containerized services\n* **Make** - For building the plugin code\n\n## Setting Up the Environment\n\nFirst, you'll need to clone the Iceberg demo\n\n```bash\ngit clone https://github.com/Aiven-Open/tiered-storage-for-apache-kafka.git\n```\n\nBuild the Remote Storage Manager plugin that handles the Iceberg integration:\n\n```bash\ncd demo/iceberg\nmake plugin\n```\n\nThis command compiles the necessary components that enable Kafka to write data directly to Iceberg format.\n\nNext, start all the required services using Docker Compose:\n\n```bash\ndocker compose -f docker-compose.yml up -d\n```\n\nThis command starts several interconnected services:\n\n* **Kafka Broker:** Configured with Remote Storage Manager for Iceberg integration\n* **Karapace Schema Registry:** Manages Avro schemas for structured data\n* **MinIO:** Provides S3-compatible object storage for Iceberg tables\n* **Iceberg REST Catalog:** Manages Iceberg table metadata\n* **Spark:** Enables querying of Iceberg tables through notebooks\n\nWait for all containers to start completely. You can monitor the startup process by watching the Docker logs.\n\n## Creating and Populating Iceberg Topics\n\nOnce your environment is running, create a topic and populate it with sample data:\n\n```bash\nclients/gradlew run -p clients\n```\n\nThis demo script performs several important operations:\n\n1. **Creates the** `people` **topic** with Iceberg integration enabled\n2. **Generates sample Avro records** representing person data\n3. **Produces messages** to the Kafka topic using standard Kafka APIs\n4. **Triggers automatic conversion** of streaming data to Iceberg format\n\nThe magic happens behind the scenes - while your application produces and consumes data using standard Kafka APIs, the Remote Storage Manager plugin automatically converts the streaming data into Iceberg table format and stores it in MinIO.\n\n## Exploring Your Data\n\nAfter the demo runs and Kafka uploads segments to remote storage, you can explore your data in multiple ways:\n\n**Query with Spark:** Visit the Spark notebook at `http://localhost:8888/notebooks/notebooks/Demo.ipynb` to run SQL queries against your Iceberg tables. You'll be able to perform analytical queries on the streaming data using familiar SQL syntax.\n\n![](/content/images/iceberg_topics_sql.png)\n\n**Inspect Storage:** Browse the MinIO interface at `http://localhost:9001/browser/warehouse` to see the actual Iceberg table files and metadata stored in object storage.\n\n## What Makes This Powerful\n\nThis local setup demonstrates several key capabilities:\n\n**Immediate Querying:** As soon as data is produced to Kafka, it becomes available for analytical queries through Spark - no batch processing delays.\n\n**Storage Efficiency:** Iceberg's columnar format and compression provide efficient storage for analytical workloads while maintaining streaming performance.\n\n**ACID Compliance:** Your streaming data benefits from Iceberg's ACID transaction support, ensuring consistency even with high-throughput streams.\n\n## Troubleshooting Common Issues\n\nIf you encounter problems during setup:\n\n**Build Issues:** Ensure you have JDK 17+ installed and that your `JAVA_HOME` is set correctly before running `make plugin`.\n\n**Container Startup:** Check Docker logs with `docker compose logs [service-name]` to identify startup issues. Services have dependencies, so ensure Kafka is healthy before other services start.\n\n**Schema Registry Connection:** If you see schema-related errors, verify that Karapace is running and accessible at `http://localhost:8081`.\n\n**Storage Access:** MinIO credentials are `admin/password` by default. If you see S3 access errors, check the MinIO service status and credentials.\n\n**Plugin Version Mismatch:** If you see `ClassNotFoundException: io.aiven.kafka.tieredstorage.RemoteStorageManager`, the Makefile version doesn't match your build output. Check what version was built:\n\n```bash\nls -la ../../core/build/distributions/\n```\n\nIf you see a `SNAPSHOT.tgz` with a different version instead of `core-0.0.1-SNAPSHOT.tgz`, update the Makefile to match the version from the command above, for example:\n\n```bash\nsed -i '' 's/0\\.0\\.1-SNAPSHOT/1.1.0-SNAPSHOT/g' Makefile\nmake plugin\n```\n\n## What Do Iceberg Topics Mean for Kafka?\n\nThe integration between Kafka and Iceberg represents a fundamental shift toward unified streaming and analytical architectures. Instead of maintaining separate systems for real-time and analytical workloads, organizations can now use Kafka as a single platform that serves both use cases.\n\n**For Stream Processing Teams:** Continue using familiar Kafka APIs while automatically generating analytical datasets for data science and business intelligence teams.\n\n**For Data Engineering Teams:** Eliminate complex ETL pipelines and reduce the operational overhead of maintaining separate streaming and analytical systems.\n\n**For Analytics Teams:** Access streaming data immediately for real-time analytics without waiting for batch processes or dealing with data freshness issues.\n\n**For Organizations:** Reduce total cost of ownership by consolidating infrastructure and eliminating data duplication across systems.\n\n## Ready to Explore Further?\n\nThe local example in this article provides a foundation for understanding Iceberg Topics, but the real value comes from experimenting with your own data and use cases. Consider how eliminating the boundary between streaming and analytical systems could simplify your data architecture and enable new capabilities.\n\nThe streaming analytics landscape is evolving rapidly, and integrations like Iceberg Topics are leading the way toward more unified, efficient, and capable data platforms. Whether you're processing IoT sensor data, financial transactions, or user activity streams, the ability to seamlessly bridge real-time and analytical workloads opens up exciting possibilities for your data-driven applications.\n\n---\n\n## Learn More\n\nExplore these resources to deepen your understanding of Kafka and Iceberg integration:\n\n[Apache Iceberg Documentation](https://iceberg.apache.org/)\n\n[Kafka Remote Storage Manager](https://kafka.apache.org/documentation/#remote_storage)\n\n[Aiven's Kafka Tiered Storage](https://aiven.io/blog/kafka-tiered-storage)\n\n[Confluent's Iceberg Integration](https://www.confluent.io/blog/apache-iceberg-kafka-streams/)\n\nThe future of streaming data is here - start building with Iceberg Topics today!",
            "status": "published",
            "created_at": 1760907490696,
            "published_at": 1760907490696,
            "updated_at": 1760907490696,
            "author_id": 1
          },
          {
            "id": 29,
            "title": "Data from Smart Bird Feeder Project",
            "slug": "smart-bird-feeder-histogram",
            "markdown": "*Update: After [some feedback](https://www.linkedin.com/feed/update/urn:li:ugcPost:7381628783683854336?commentUrn=urn%3Ali%3Acomment%3A%28ugcPost%3A7381628783683854336%2C7381818839522619393%29&dashCommentUrn=urn%3Ali%3Afsd_comment%3A%287381818839522619393%2Curn%3Ali%3AugcPost%3A7381628783683854336%29) I manually relabeled my data and changed my approach slightly for better graphs*\n\nBy popular request here's all the data I collected from [my smart bird feeder](/load-cell-raspberry-pi) in histogram form:\n\n![](/content/images/bird-histogram-result.png)\n\nThis is my first time trying [Marimo](https://marimo.io/) and I really liked using it for visualising this data, I've embedded the Marimo notebook below so you can explore the data further should you wish:\n\n<div>\n  <iframe src=\"/projects/birds/bird-demo.html\" width=\"100%\" height=\"500px\"></iframe>\n</div>\n*Go fullscreen [here](/projects/birds/bird-demo.html).*",
            "status": "published",
            "created_at": 1759878000000,
            "published_at": 1759878000000,
            "updated_at": 1759878000000,
            "author_id": 1
          },
          {
            "id": 30,
            "title": "My ATO 2025 Highlights",
            "slug": "all-things-open-2025",
            "markdown": "I just wrapped up an incredible two days at the All Things Open Conference in Raleigh, and I'm still buzzing from the experience (though admittedly quite jet-lagged as I make my way back to London for the Dremio Subsurface conference).\n\n## **Day Two: Finding My Rhythm**\n\nI joined the conference on day two, starting with a wander around the exhibition hall. HeroDevs stole the show with their incredible Halloween display.\n\n![](/content/images/1760445577142.jpeg)\n\nThe morning sessions delivered some real gems. Moss Norman's talk on \"Solving Developer's Problems One Video At A Time\" was particularly insightful, and I'm definitely taking him up on his challenge to create a 6-10 minute video on a familiar topic to find my camera style when I get home.\n\n[Barkha Herman](https://www.linkedin.com/in/barkhaherman/) delivered an outstanding talk on \"The Collision of Real-Time Analytics and Observability,\" diving deep into why the open-source disaggregated-stack (a phrase I'll definitely be stealing) is optimal for observability at scale. She backed up her insights with a career's worth of case studies from some of the biggest organizations in the world.\n\n\"Scaling Advocacy Without Losing the Human Touch\" really resonated with me. It was fascinating hearing about [Budhaditya Bhattacharya](https://www.linkedin.com/in/budha-b/)'s approach to thoughtfully incorporating AI into his practice without losing the human element that makes developer advocacy special.\n\n![](/content/images/1760445577321.jpeg)\n\nThen came my own talk. Following one of my dev rel heroes, [Olena Kutsenko](https://www.linkedin.com/in/olenakutsenko/), is always a tough act (I'm really looking forward to seeing the recording of her talk\\!). I ran through the latest iteration of my flight radar talk, covering Kafka and ClickHouse for real-time analytics. The audience was wonderfully engaged, which I really appreciated so soon after lunch.\n\nThe afternoon brought a brilliant deep dive into the Apache Gluten project from Chengcheng Jin and [Binwei Yang](https://www.linkedin.com/in/binwei-yang-611462254/). These guys are building something amazing‚Äîa plugin that accelerates many Apache Spark workloads and adds a huge number of new integrations. It was a privilege to share a slot with them.\n\nThe day's final session was \"The Evolution of DevRel ‚Äì How we got here and what lies ahead\" with [John Coghlan](https://www.linkedin.com/in/johnwcoghlan/). He had a room full of Dev Rels laughing at his spot-on slide about the three things Dev Rels always talk about: What do we call ourselves? Where do we report? How do we measure what we do? Beyond the inside-baseball humor, John shared valuable insights from his work at GitLab, particularly his model for stages of dev rel as they relate to stages of organization growth and profitability.\n\nBetween sessions, I had some great conversations. I ran into [Kunal Kushwaha](https://www.linkedin.com/in/kunal-kushwaha/) and we caught up about events in London and the joys of conference travel. Over coffee with [Peter Zaitsev](https://www.linkedin.com/in/peterzaitsev/), we discussed databases, Kafka, and open source. The day rounded out perfectly with the social event and speakers dinner (massive thanks to CodeRabbit for hosting‚Äîit was wonderful to meet old friends and new).\n\n## **Day Three: Going Out Strong**\n\n![](/content/images/1760555496665.jpeg)\n\nDespite the accumulated fatigue from two full days of conferencing and a few thousand miles of air travel, I made it to the morning keynotes and was glad I did. The lineup featured some exceptional speakers: [Christian Heilmann](https://www.linkedin.com/in/christianheilmann/) delivered \"X-rAI Specs, Submarines and Juice,\" [Angie Jones](https://www.linkedin.com/in/angiejones/) provided an insightful deep dive into how Block operationalized MCP at scale, [taylor desseyn](https://www.linkedin.com/in/taylordesseyn/) brought incredible energy with his talk on \"Futureproofing Your Career With AI,\" and [Brian Douglas](https://www.linkedin.com/in/brianldouglas/) delivered a valuable session on \"Finding Future Maintainers for open-source projects.\"\n\nI particularly wish I'd seen Zoe Steinkamp's talk \"Unbundling of the Cloud Data Warehouse\" back when I was doing developer relations for Apache Druid. She did an amazing job breaking down the history of data warehousing and explaining key concepts like OLAP and data lake-houses.\n\n\"Fundamentals of DataOps: A Practitioner's Guide\" was a comprehensive journey into modern data ops. Lisa N. Cao's extremely deep knowledge was immediately apparent, and I learned a huge amount about both data ops and how to effectively communicate about it.\n\nMatthew Mullins walked us through the story of how he and his team built an impressive data product at Coginiti with DuckDB and Apache Iceberg in his talk \"Data Management with Apache Iceberg and Mixed Compute.\"\n\nThe conference ended on an unforgettable note with Paul Chin Jr.'s \"Data Gone in 60 Seconds: A Serverless ETL Heist\"‚Äîa Nicolas Cage-themed serverless talk that was absolutely incredible. He somehow had a room full of people shouting \"Praise Cage\" after three packed days of conference. Crazy stuff.\n\n## **Final Thoughts**\n\nThis was an awesome conference, and I can now see why so many of my friends were eager to recommend it. Todd Lewis and the organizing team are building the community they always wanted, and it's worth the transatlantic trip to experience it. While I'm not sure if I can keep Todd's pinky promise to come back next year given my current jet-lagged state, I'm definitely looking forward to being back in Raleigh with this wonderful community in the future.\n\nA huge thank you to the All Things Open Conference team for putting on such a fantastic event. Now, onwards to London for Dremio Subsurface\\!",
            "status": "published",
            "created_at": 1760482800000,
            "published_at": 1760482800000,
            "updated_at": 1760482800000,
            "author_id": 1
          },
          {
            "id": 31,
            "title": "Get Kafka-Nated (Episode 8) Realistic Synthetic Streaming Data with Michael Drogalis",
            "slug": "get-kafka-nated-ep-8",
            "markdown": "Check out the latest episode of Get Kafka-Nated! It's always a treat chatting with Michael Drogalis and I had a great time picking his brain about his career in streaming and his Synthetic Stream Data product ShadowTraffic.  You can watch the recording below.\n\n<div class=\"videoWrapper\"><iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/W5yVEDwc9ds?si=6J5mjrUY5khp35xk\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe></div>\n\n[Original release](https://www.linkedin.com/events/getkafka-natedep8-realisticsynt7381262670747873280/theater/)\n\nYou can find all the past episodes of Get Kafka-Nated as well as Kafka news and technical deep dives over at [getkafkanated.substack.com](https://getkafkanated.substack.com/)",
            "status": "published",
            "created_at": 1760482800000,
            "published_at": 1760482800000,
            "updated_at": 1760482800000,
            "author_id": 1
          }
        ],
        "tags": [
          {
            "id": 1,
            "name": "How to",
            "slug": "how-to"
          },
          {
            "id": 2,
            "name": "Prometheus",
            "slug": "prometheus"
          },
          {
            "id": 3,
            "name": "IaC",
            "slug": "iac"
          },
          {
            "id": 4,
            "name": "Kubernetes",
            "slug": "kubernetes"
          },
          {
            "id": 5,
            "name": "Article",
            "slug": "article"
          },
          {
            "id": 6,
            "name": "Long form",
            "slug": "long-form"
          },
          {
            "id": 7,
            "name": "AI",
            "slug": "ai"
          },
          {
            "id": 8,
            "name": "DALLE 2",
            "slug": "dalle-2"
          },
          {
            "id": 9,
            "name": "Art",
            "slug": "art"
          },
          {
            "id": 10,
            "name": "Raspberry Pi",
            "slug": "raspberry-pi"
          },
          {
            "id": 11,
            "name": "IoT",
            "slug": "iot"
          },
          {
            "id": 12,
            "name": "AWS",
            "slug": "aws"
          },
          {
            "id": 13,
            "name": "SQS",
            "slug": "sqs"
          },
          {
            "id": 14,
            "name": "Keycloak",
            "slug": "keycloak"
          },
          {
            "id": 15,
            "name": "SSO",
            "slug": "sso"
          },
          {
            "id": 16,
            "name": "OpenSearch",
            "slug": "opensearch"
          },
          {
            "id": 17,
            "name": "DevOps",
            "slug": "devops"
          },
          {
            "id": 18,
            "name": "Aerospike",
            "slug": "aerospike"
          },
          {
            "id": 19,
            "name": "Meetup",
            "slug": "meetup"
          },
          {
            "id": 20,
            "name": "Druid",
            "slug": "druid"
          },
          {
            "id": 21,
            "name": "Summit",
            "slug": "summit"
          },
          {
            "id": 22,
            "name": "POSSE",
            "slug": "posse"
          },
          {
            "id": 23,
            "name": "IndieWeb",
            "slug": "indieweb"
          },
          {
            "id": 24,
            "name": "Year in Review",
            "slug": "year-in-review"
          },
          {
            "id": 25,
            "name": "Documentation",
            "slug": "documentation"
          },
          {
            "id": 26,
            "name": "Swimm",
            "slug": "swimm"
          },
          {
            "id": 27,
            "name": "Codebar",
            "slug": "codebar"
          },
          {
            "id": 28,
            "name": "Festival Fringe",
            "slug": "festival-fringe"
          },
          {
            "id": 29,
            "name": "Project",
            "slug": "project"
          },
          {
            "id": 30,
            "name": "Stickers",
            "slug": "stickers"
          },
          {
            "id": 31,
            "name": "Cursor",
            "slug": "cursor"
          },
          {
            "id": 32,
            "name": "Tooling",
            "slug": "tooling"
          },
          {
            "id": 33,
            "name": "Community",
            "slug": "community"
          },
          {
            "id": 34,
            "name": "Events",
            "slug": "events"
          },
          {
            "id": 35,
            "name": "Organising",
            "slug": "organising"
          },
          {
            "id": 36,
            "name": "Data Engineering",
            "slug": "data-engineering"
          },
          {
            "id": 37,
            "name": "Architecture",
            "slug": "architecture"
          },
          {
            "id": 38,
            "name": "OSS",
            "slug": "oss"
          },
          {
            "id": 39,
            "name": "Kafka",
            "slug": "kafka"
          },
          {
            "id": 40,
            "name": "RaspberryPi",
            "slug": "raspberrypi"
          },
          {
            "id": 41,
            "name": "Python",
            "slug": "python"
          },
          {
            "id": 42,
            "name": "Hardware",
            "slug": "hardware"
          },
          {
            "id": 43,
            "name": "Iceberg",
            "slug": "iceberg"
          }
        ],
        "posts_tags": [
          {
            "post_id": 1,
            "tag_id": 1,
            "sort_order": 0
          },
          {
            "post_id": 1,
            "tag_id": 2,
            "sort_order": 1
          },
          {
            "post_id": 1,
            "tag_id": 3,
            "sort_order": 2
          },
          {
            "post_id": 1,
            "tag_id": 4,
            "sort_order": 3
          },
          {
            "post_id": 1,
            "tag_id": 5,
            "sort_order": 4
          },
          {
            "post_id": 2,
            "tag_id": 6,
            "sort_order": 0
          },
          {
            "post_id": 2,
            "tag_id": 7,
            "sort_order": 1
          },
          {
            "post_id": 2,
            "tag_id": 8,
            "sort_order": 2
          },
          {
            "post_id": 2,
            "tag_id": 9,
            "sort_order": 3
          },
          {
            "post_id": 2,
            "tag_id": 5,
            "sort_order": 4
          },
          {
            "post_id": 3,
            "tag_id": 1,
            "sort_order": 0
          },
          {
            "post_id": 3,
            "tag_id": 10,
            "sort_order": 1
          },
          {
            "post_id": 3,
            "tag_id": 11,
            "sort_order": 2
          },
          {
            "post_id": 3,
            "tag_id": 12,
            "sort_order": 3
          },
          {
            "post_id": 3,
            "tag_id": 13,
            "sort_order": 4
          },
          {
            "post_id": 3,
            "tag_id": 5,
            "sort_order": 5
          },
          {
            "post_id": 4,
            "tag_id": 1,
            "sort_order": 0
          },
          {
            "post_id": 4,
            "tag_id": 14,
            "sort_order": 1
          },
          {
            "post_id": 4,
            "tag_id": 12,
            "sort_order": 2
          },
          {
            "post_id": 4,
            "tag_id": 15,
            "sort_order": 3
          },
          {
            "post_id": 4,
            "tag_id": 5,
            "sort_order": 4
          },
          {
            "post_id": 5,
            "tag_id": 1,
            "sort_order": 0
          },
          {
            "post_id": 5,
            "tag_id": 12,
            "sort_order": 1
          },
          {
            "post_id": 5,
            "tag_id": 16,
            "sort_order": 2
          },
          {
            "post_id": 5,
            "tag_id": 17,
            "sort_order": 3
          },
          {
            "post_id": 5,
            "tag_id": 5,
            "sort_order": 4
          },
          {
            "post_id": 6,
            "tag_id": 5,
            "sort_order": 0
          },
          {
            "post_id": 6,
            "tag_id": 18,
            "sort_order": 1
          },
          {
            "post_id": 6,
            "tag_id": 19,
            "sort_order": 2
          },
          {
            "post_id": 7,
            "tag_id": 5,
            "sort_order": 0
          },
          {
            "post_id": 7,
            "tag_id": 20,
            "sort_order": 1
          },
          {
            "post_id": 7,
            "tag_id": 21,
            "sort_order": 2
          },
          {
            "post_id": 8,
            "tag_id": 5,
            "sort_order": 0
          },
          {
            "post_id": 8,
            "tag_id": 22,
            "sort_order": 1
          },
          {
            "post_id": 8,
            "tag_id": 23,
            "sort_order": 2
          },
          {
            "post_id": 9,
            "tag_id": 5,
            "sort_order": 0
          },
          {
            "post_id": 9,
            "tag_id": 24,
            "sort_order": 1
          },
          {
            "post_id": 10,
            "tag_id": 7,
            "sort_order": 0
          },
          {
            "post_id": 10,
            "tag_id": 25,
            "sort_order": 1
          },
          {
            "post_id": 10,
            "tag_id": 26,
            "sort_order": 2
          },
          {
            "post_id": 10,
            "tag_id": 5,
            "sort_order": 3
          },
          {
            "post_id": 11,
            "tag_id": 5,
            "sort_order": 0
          },
          {
            "post_id": 11,
            "tag_id": 27,
            "sort_order": 1
          },
          {
            "post_id": 11,
            "tag_id": 28,
            "sort_order": 2
          },
          {
            "post_id": 12,
            "tag_id": 29,
            "sort_order": 0
          },
          {
            "post_id": 12,
            "tag_id": 30,
            "sort_order": 1
          },
          {
            "post_id": 12,
            "tag_id": 27,
            "sort_order": 2
          },
          {
            "post_id": 12,
            "tag_id": 5,
            "sort_order": 3
          },
          {
            "post_id": 13,
            "tag_id": 7,
            "sort_order": 0
          },
          {
            "post_id": 13,
            "tag_id": 31,
            "sort_order": 1
          },
          {
            "post_id": 13,
            "tag_id": 32,
            "sort_order": 2
          },
          {
            "post_id": 13,
            "tag_id": 5,
            "sort_order": 3
          },
          {
            "post_id": 14,
            "tag_id": 33,
            "sort_order": 0
          },
          {
            "post_id": 14,
            "tag_id": 34,
            "sort_order": 1
          },
          {
            "post_id": 14,
            "tag_id": 35,
            "sort_order": 2
          },
          {
            "post_id": 14,
            "tag_id": 5,
            "sort_order": 3
          },
          {
            "post_id": 15,
            "tag_id": 36,
            "sort_order": 0
          },
          {
            "post_id": 15,
            "tag_id": 37,
            "sort_order": 1
          },
          {
            "post_id": 15,
            "tag_id": 38,
            "sort_order": 2
          },
          {
            "post_id": 15,
            "tag_id": 39,
            "sort_order": 3
          },
          {
            "post_id": 15,
            "tag_id": 5,
            "sort_order": 4
          },
          {
            "post_id": 16,
            "tag_id": 36,
            "sort_order": 0
          },
          {
            "post_id": 16,
            "tag_id": 37,
            "sort_order": 1
          },
          {
            "post_id": 16,
            "tag_id": 38,
            "sort_order": 2
          },
          {
            "post_id": 16,
            "tag_id": 39,
            "sort_order": 3
          },
          {
            "post_id": 16,
            "tag_id": 5,
            "sort_order": 4
          },
          {
            "post_id": 17,
            "tag_id": 36,
            "sort_order": 0
          },
          {
            "post_id": 17,
            "tag_id": 37,
            "sort_order": 1
          },
          {
            "post_id": 17,
            "tag_id": 38,
            "sort_order": 2
          },
          {
            "post_id": 17,
            "tag_id": 39,
            "sort_order": 3
          },
          {
            "post_id": 17,
            "tag_id": 5,
            "sort_order": 4
          },
          {
            "post_id": 18,
            "tag_id": 40,
            "sort_order": 0
          },
          {
            "post_id": 18,
            "tag_id": 41,
            "sort_order": 1
          },
          {
            "post_id": 18,
            "tag_id": 42,
            "sort_order": 2
          },
          {
            "post_id": 18,
            "tag_id": 39,
            "sort_order": 3
          },
          {
            "post_id": 18,
            "tag_id": 5,
            "sort_order": 4
          },
          {
            "post_id": 19,
            "tag_id": 36,
            "sort_order": 0
          },
          {
            "post_id": 19,
            "tag_id": 37,
            "sort_order": 1
          },
          {
            "post_id": 19,
            "tag_id": 38,
            "sort_order": 2
          },
          {
            "post_id": 19,
            "tag_id": 39,
            "sort_order": 3
          },
          {
            "post_id": 19,
            "tag_id": 5,
            "sort_order": 4
          },
          {
            "post_id": 20,
            "tag_id": 36,
            "sort_order": 0
          },
          {
            "post_id": 20,
            "tag_id": 37,
            "sort_order": 1
          },
          {
            "post_id": 20,
            "tag_id": 38,
            "sort_order": 2
          },
          {
            "post_id": 20,
            "tag_id": 39,
            "sort_order": 3
          },
          {
            "post_id": 20,
            "tag_id": 5,
            "sort_order": 4
          },
          {
            "post_id": 21,
            "tag_id": 40,
            "sort_order": 0
          },
          {
            "post_id": 21,
            "tag_id": 41,
            "sort_order": 1
          },
          {
            "post_id": 21,
            "tag_id": 42,
            "sort_order": 2
          },
          {
            "post_id": 21,
            "tag_id": 39,
            "sort_order": 3
          },
          {
            "post_id": 21,
            "tag_id": 5,
            "sort_order": 4
          },
          {
            "post_id": 22,
            "tag_id": 36,
            "sort_order": 0
          },
          {
            "post_id": 22,
            "tag_id": 37,
            "sort_order": 1
          },
          {
            "post_id": 22,
            "tag_id": 38,
            "sort_order": 2
          },
          {
            "post_id": 22,
            "tag_id": 39,
            "sort_order": 3
          },
          {
            "post_id": 22,
            "tag_id": 5,
            "sort_order": 4
          },
          {
            "post_id": 23,
            "tag_id": 36,
            "sort_order": 0
          },
          {
            "post_id": 23,
            "tag_id": 37,
            "sort_order": 1
          },
          {
            "post_id": 23,
            "tag_id": 38,
            "sort_order": 2
          },
          {
            "post_id": 23,
            "tag_id": 39,
            "sort_order": 3
          },
          {
            "post_id": 23,
            "tag_id": 5,
            "sort_order": 4
          },
          {
            "post_id": 24,
            "tag_id": 36,
            "sort_order": 0
          },
          {
            "post_id": 24,
            "tag_id": 37,
            "sort_order": 1
          },
          {
            "post_id": 24,
            "tag_id": 38,
            "sort_order": 2
          },
          {
            "post_id": 24,
            "tag_id": 39,
            "sort_order": 3
          },
          {
            "post_id": 24,
            "tag_id": 5,
            "sort_order": 4
          },
          {
            "post_id": 25,
            "tag_id": 41,
            "sort_order": 0
          },
          {
            "post_id": 25,
            "tag_id": 33,
            "sort_order": 1
          },
          {
            "post_id": 25,
            "tag_id": 38,
            "sort_order": 2
          },
          {
            "post_id": 25,
            "tag_id": 5,
            "sort_order": 3
          },
          {
            "post_id": 26,
            "tag_id": 36,
            "sort_order": 0
          },
          {
            "post_id": 26,
            "tag_id": 37,
            "sort_order": 1
          },
          {
            "post_id": 26,
            "tag_id": 38,
            "sort_order": 2
          },
          {
            "post_id": 26,
            "tag_id": 39,
            "sort_order": 3
          },
          {
            "post_id": 26,
            "tag_id": 5,
            "sort_order": 4
          },
          {
            "post_id": 27,
            "tag_id": 33,
            "sort_order": 0
          },
          {
            "post_id": 27,
            "tag_id": 19,
            "sort_order": 1
          },
          {
            "post_id": 27,
            "tag_id": 5,
            "sort_order": 2
          },
          {
            "post_id": 28,
            "tag_id": 36,
            "sort_order": 0
          },
          {
            "post_id": 28,
            "tag_id": 37,
            "sort_order": 1
          },
          {
            "post_id": 28,
            "tag_id": 38,
            "sort_order": 2
          },
          {
            "post_id": 28,
            "tag_id": 39,
            "sort_order": 3
          },
          {
            "post_id": 28,
            "tag_id": 43,
            "sort_order": 4
          },
          {
            "post_id": 28,
            "tag_id": 5,
            "sort_order": 5
          },
          {
            "post_id": 29,
            "tag_id": 40,
            "sort_order": 0
          },
          {
            "post_id": 29,
            "tag_id": 41,
            "sort_order": 1
          },
          {
            "post_id": 29,
            "tag_id": 42,
            "sort_order": 2
          },
          {
            "post_id": 29,
            "tag_id": 39,
            "sort_order": 3
          },
          {
            "post_id": 29,
            "tag_id": 5,
            "sort_order": 4
          },
          {
            "post_id": 30,
            "tag_id": 33,
            "sort_order": 0
          },
          {
            "post_id": 30,
            "tag_id": 38,
            "sort_order": 1
          },
          {
            "post_id": 30,
            "tag_id": 5,
            "sort_order": 2
          },
          {
            "post_id": 31,
            "tag_id": 36,
            "sort_order": 0
          },
          {
            "post_id": 31,
            "tag_id": 37,
            "sort_order": 1
          },
          {
            "post_id": 31,
            "tag_id": 38,
            "sort_order": 2
          },
          {
            "post_id": 31,
            "tag_id": 39,
            "sort_order": 3
          },
          {
            "post_id": 31,
            "tag_id": 5,
            "sort_order": 4
          }
        ]
      }
    }
  ]
}