---
title: DALLÂ·E 2 - what happens when machines make art?
tags: [Long form, AI, DALLE 2, Art, Article]
canonicalurl: https://medium.com/daemon-engineering/dall-e-2-what-happens-when-machines-make-art-ebd94b3f028b
status: published
---


![](/content/images/1S0r7QUhKx6R7K9jJxX9RRQ.png)
*3D render of DALL-E-2 making art in an open office on a red brick background, digitalÂ art*

#### What is DALLÂ·EÂ 2?


DALLÂ·E 2 is an AI created by [OpenAI](https://openai.com/about/) which produces original images in response to a text description. DALLÂ·E 2 can also make edits to existing images (which OpenAI describes as *in-painting*) and create different variations based on an existingÂ image.


Throughout this article you will find examples of images generated by DALLÂ·E 2 along with the descriptions used to createÂ them.


![](/content/images/16MjxnR8JBAuqoZvZnF1idA.png) 
![](/content/images/1OttgW5ELlucROHJMG0-Qkg.png)
*comfortable armchair in a library neon, pink, blue, geometric, futuristic, â€˜80s*

OpenAI recently added an *out-painting* feature, this allows an existing image to be expanded by adding new content around itsÂ edges.


#### How does DALLÂ·E 2Â work?


Under the hood, DALLÂ·E 2 is a series of machine learning models which have been trained to perform the operations required to generate a new image, the first of which is [CLIP](https://openai.com/blog/clip/). Developed by OpenAI and able to encode images and descriptions, CLIP can quantify how well matched text and image encodings are in a process called *contrastive learning*. DALLÂ·E 2 uses another model called a *prior* to select a relevant image encoding for a given text encoding of a description, this image encoding is used to create a gist of the image which will be generated.


![](/content/images/1OWoXzQlBD3KyTFiIvPPwLA.png)
![](/content/images/1fQ5-GvFiUhqTvXrVYKGGjw.png)
![](/content/images/1z-Vl3-0VJlL6zPvYzd04EQ.png)
![](/content/images/117o9JugflMmHVjhPKwUPpQ.png)
*salvador dali waving to the camera photo realistic*

The gist is then passed to a diffusion model. Diffusion models are trained to remove [noise](https://en.wikipedia.org/wiki/Image_noise) from images, but in this case DALLÂ·E 2 uses diffusion to take the gist image and gradually add more detail to it until it is complete, resulting in the generation of an original image corresponding to the entered description.


For a more in-depth explanation of how DALLÂ·E 2 works, please [see this article](http://adityaramesh.com/posts/dalle2/dalle2.html) by [Aditya Ramesh](https://twitter.com/model_mechanic), the creator of DALLÂ·E and one of the co-creators of DALLÂ·EÂ 2.


#### What are the potential applications for DALLÂ·EÂ 2?


[OpenAI permits the use of images generated by DALLÂ·E 2 commercially](https://www.technollama.co.uk/dall%C2%B7e-goes-commercial-but-what-about-copyright), which opens up a world of potential business applications for this technology. It is tempting to imagine that DALLÂ·E 2 can be used anywhere an original image is needed but it has several major limitations (see below) which significantly narrows the field of potential applications. Replacing stock image libraries may be a potential application, especially given the lower cost of accessing DALLÂ·E 2 when compared to licensing images and the similarities between querying libraries of stock photos and composing DALLÂ·E 2 descriptions. Generating assets for games also seems like a good candidate. Marketplaces like the [Unity asset store](https://assetstore.unity.com/) currently offer a variety of assets for use in game development and they can range in price dramatically, DALLÂ·E 2 can be used to quickly and cheaply create some types of assets without requiring the application of artisticÂ skill.


#### What are the limitations of DALLÂ·EÂ 2?


DALLÂ·E 2 does not have a good understanding of composition. Consider the following example: when asked to create an image containing a series of different coloured shapes, DALLÂ·E 2 succeeds in creating an image with the correct shapes and colours, however; it fails to assign the correct colours to each shape consistently.


This is as a result of how contrastive learning works: CLIP learns which features are sufficient for matching an image with the correct description from the training data it is provided, unless that data includes counter examples (in this case images with captions describing their shape, colour, and position in a variety of combinations), CLIP disregards information about composition.


![](/content/images/1Lc3poRm17aXz1xjmviLkqg.png)
![](/content/images/1NOm5Orif-bisQ5PbWbel6A.png)
![](/content/images/1KH31qEckCvu2t4XIcWmKIA.png)
![](/content/images/1lYUP6Y7-GhYKC8okGhBv0w.png)
*oil pastel sketch of a red triangle, a blue square, and a yellow circle onÂ canvas*

Struggles with composition also prevent DALLÂ·E 2 from being able to recreate coherent text. Given that DALLÂ·E 2 is an image generation AI, and not a specialised text generation AI such as [GPT-3](https://openai.com/api/) (also created by OpenAI), it is unsurprising that it struggles with text generation. It is clear that DALLÂ·E 2 understands the concept of text: images containing text often have readable lettering and the text itself often reflects information contained in the description, however; DALLÂ·E 2 struggles with rules on grammar and syntax. In generated images, words will usually be in the wrong order, and sometimes text generated will be gibberish, especially if the description doesnâ€™t specify the exact wording, resulting in some text that doesnâ€™t feel quiteÂ right.


![](/content/images/1gKUcvKybfn1xcLVehVpY8A.png)
![](/content/images/1ebzkbn0S93pZ-ENyHkJ8lw.png)
![](/content/images/1OfI740MRnRK69AUYd9laZw.png)
![](/content/images/1JGuhZa4Vyw8z9jixlVvVZw.png)
*square, polaroid a sign that says hello world photography*

In an effort to prevent DALLÂ·E 2 being used for harmful purposes OpenAI filters text prompts such that DALLÂ·E 2 will not generate images for prompts containing language which is violent, adult, or political. Whilst it is important to prevent the use of image generation AI for harmful purposes these text filters have been known to be overly restrictive at times, with some users reporting seemingly innocent words or phrases being filtered.


<blockquote class="twitter-tweet"><p lang="en" dir="ltr">I can&#39;t force dalle2 to create ğŸ¤¯ emoji, because of their content filter, I used up my last credits while trying ğŸ˜•<a href="https://twitter.com/hashtag/dalle2?src=hash&amp;ref_src=twsrc%5Etfw">#dalle2</a> <a href="https://twitter.com/hashtag/dalle?src=hash&amp;ref_src=twsrc%5Etfw">#dalle</a> <a href="https://twitter.com/hashtag/emojis?src=hash&amp;ref_src=twsrc%5Etfw">#emojis</a> <a href="https://twitter.com/hashtag/aiemoji?src=hash&amp;ref_src=twsrc%5Etfw">#aiemoji</a> <a href="https://twitter.com/hashtag/aiemojis?src=hash&amp;ref_src=twsrc%5Etfw">#aiemojis</a> <a href="https://t.co/z0blFHcVr7">pic.twitter.com/z0blFHcVr7</a></p>&mdash; AIXD (@AIRUNXD) <a href="https://twitter.com/AIRUNXD/status/1558308629505359872?ref_src=twsrc%5Etfw">August 13, 2022</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

In addition to filtering text prompts, OpenAI also uses â€œadvanced techniquesâ€ to prevent DALLÂ·E 2 from creating realistic images of real people (including celebrities and public figures). For example, you cannot upload a photo of yourself and modify it with DALLÂ·E 2, at least forÂ now.


#### Will DALLÂ·E 2 replace human artists in the nearÂ future?


Definitely not. DALLÂ·E 2 has some hard limits that ultimately prevent it from competing with human artists. Other image generation AI such as Craiyon (formerly DALLÂ·E mini), Midjourney, and Dream also show promise, but all such systems suffer from similar limitations to DALLÂ·E 2. Image generation AI are based on machine learning models and so are ultimately reliant on the creativity of the artists, photographers, and other creatives who contribute images to the data used to train theseÂ models.


![](/content/images/1qzXd69kvqrMzWx4w0qWmtQ.png)
![](/content/images/1cbp7RaB4802gChJPSY7rpQ.png)
![](/content/images/1Xqj-eO4Xex2rSDK9Fb3Zmw.png)
![](/content/images/1SBxRqixb1_8rx_s0LhdcNw.png)
*digital art of a doctor using a stethoscope, in the style of van gogh, white background*

AI are also reliant on the people who label training data. To function accurately and without bias machine learning models require data that is labeled accurately, consistently, and responsibly. Whilst OpenAI have [attempted to address bias in DALLÂ·E 2](https://openai.com/blog/reducing-bias-and-improving-safety-in-dall-e-2/) the effects of bias are still evident in generated images,Â forÂ example, descriptions including phrases like â€œdoctorâ€ typically resulting in images ofÂ men.


#### What does the future hold for DALLÂ·EÂ 2?


To answer this question, it can be helpful to look at an example of an older image generation AI technology. GANs ([Generative Adversarial Networks](https://en.wikipedia.org/wiki/Generative_adversarial_network)) are a class of image generation models trained by using a model that spots AI-generated images to train image generation AI to make betterÂ images.


GANs can create original images like DALLÂ·E but only of a specific subject. An example implementation of GANs such as [This Person Does Not Exist](https://thispersondoesnotexist.com/) are effective only because it focuses on a particular type of imageâ€Šâ€”â€Šin this case, faces. Despite the limitations associated with using GANs, they have seen wide use and study; rather than replacing human creativity GANs have become a tool used for creative expression.


There is already a dedicated online community experimenting with DALLÂ·E 2 (which has produced some great [resources on how to get the best results](https://dallery.gallery/the-dalle-2-prompt-book/) when writing descriptions) so it seems likely that DALLÂ·E 2 will, like GANs, become a tool used to support human creativity, not replaceÂ it.


It is possible to imagine then that 10 years from now, we may see tools such as DALLÂ·E 2 reach a level of sophistication such that using them is more like working in partnership with another person rather than simply using aÂ tool.


![](/content/images/1y2v3JyyIYQiu8WeTHvaysg.png)
*bright, vibrant, modern AI working with humans to create art 3dÂ render*

[DALLÂ·E 2: what happens when machines make art?](https://medium.com/daemon-engineering/dall-e-2-what-happens-when-machines-make-art-ebd94b3f028b) was originally published in [daemon-engineering](https://medium.com/daemon-engineering) on Medium.


